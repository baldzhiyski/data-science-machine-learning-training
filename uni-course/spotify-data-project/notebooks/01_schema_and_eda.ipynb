{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Exploration (EDA) für `spotify.sqlite`\n",
    "\n",
    "**Ziel:** Diese Notebook führt eine EDA für den bereitgestellten Spotify-Basisdatensatz (SQLite) durch."
   ],
   "id": "53d359a9de9840be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T09:46:06.130151Z",
     "start_time": "2026-01-04T09:46:06.120324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Grundlegendes Setup ---\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib: keine Styles/keine festen Farben setzen\n",
    "plt.rcParams.update({'figure.figsize': (8, 5), 'axes.grid': True})\n",
    "\n",
    "# Verbindung nur für Schema-Abfrage (UTF-8/Bytes egal, da keine Daten gelesen werden)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ],
   "id": "5b1f859779fa2996",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Global Configurations\n",
   "id": "d6125b8cea5e04eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T09:46:06.182432Z",
     "start_time": "2026-01-04T09:46:06.158582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# GLOBAL CONFIGURATION (CHANGE ONLY THE SLICE SECTION)\n",
    "# ================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) PROJECT PATHS\n",
    "# ------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "DATA_INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "DATA_REPORTS_DIR = PROJECT_ROOT / \"data\" / \"reports\"\n",
    "DATA_MODELS_DIR  = PROJECT_ROOT / \"models\"\n",
    "RAW_SPOTIFY_DB_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"spotify.sqlite\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) CURRENT SLICE (ONLY CHANGE THIS PART)\n",
    "# ------------------------------------------------\n",
    "SAMPLE_NAME = \"slice_001\"      # slice_000 | slice_001 | slice_002\n",
    "ROW_K_START = 7                 # slice_000 → 0 | slice_001 → 7 | slice_002 → 14\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) DATA & REPORT LOCATIONS FOR THIS SLICE\n",
    "# ------------------------------------------------\n",
    "EXPORT_DIR = DATA_INTERIM_DIR / \"converted_sqlite_samples\" / SAMPLE_NAME\n",
    "\n",
    "SCHEMA_REPORTS_DIR = DATA_REPORTS_DIR / \"01_schema_overview\" /  SAMPLE_NAME\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# SAMPLE SELECTOR (NEW)\n",
    "# ----------------------------------------------------------------\n",
    "CURRENT_SAMPLE_POINTER = DATA_INTERIM_DIR / \"converted_sqlite_samples\"\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Target sample size\n",
    "# ----------------------------------------------------------------\n",
    "TARGET_TRACKS = 300_000\n",
    "\n",
    "\n",
    "for d in [EXPORT_DIR,DATA_MODELS_DIR,DATA_REPORTS_DIR,DATA_MODELS_DIR,DATA_INTERIM_DIR ,SCHEMA_REPORTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) SAMPLING DESCRIPTION (DOCUMENTATION ONLY)\n",
    "# ------------------------------------------------\n",
    "# The actual sampling happens in Notebook 01.\n",
    "# These values are stored for transparency & reproducibility.\n",
    "\n",
    "    SAMPLING_CONFIG = {\n",
    "    \"method\": \"ROWID multi-bucket slicing\",\n",
    "    \"row_mod\": 200,\n",
    "    \"buckets_per_slice\": 7,\n",
    "    \"row_k_start\": ROW_K_START,\n",
    "    \"target_tracks\": 300_000\n",
    "}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) SAVE METADATA (ONCE PER SLICE)\n",
    "# ------------------------------------------------\n",
    "META_PATH = DATA_INTERIM_DIR / \"converted_sqlite_samples\" / \"current_sample.json\"\n",
    "\n",
    "\n",
    "META_PATH.write_text(json.dumps({\n",
    "        \"sample_name\": SAMPLE_NAME,\n",
    "        \"created_at\": datetime.datetime.now(datetime.UTC).isoformat(),\n",
    "        \"sampling_config\": SAMPLING_CONFIG,\n",
    "    }, indent=2))\n",
    "\n",
    "print(\"Active slice:\", SAMPLE_NAME)\n",
    "print(\"Buckets:\", list(range(ROW_K_START, ROW_K_START + 7)))\n",
    "print(\"Data dir:\", EXPORT_DIR)\n",
    "print(\"Reports dir:\", DATA_REPORTS_DIR / SAMPLE_NAME)\n"
   ],
   "id": "2d14980f620131af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active slice: slice_001\n",
      "Buckets: [7, 8, 9, 10, 11, 12, 13]\n",
      "Data dir: C:\\GitHub\\data-science\\uni-course\\spotify-data-project\\data\\interim\\converted_sqlite_samples\\slice_001\n",
      "Reports dir: C:\\GitHub\\data-science\\uni-course\\spotify-data-project\\data\\reports\\slice_001\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Datenbankstruktur und Tabellenübersicht\n",
    "\n",
    "Bevor ich mit der eigentlichen Analyse beginne, will ich ein klares Verständnis der Struktur des Spotify-Datenbestands gewinnen.\n",
    "Dazu prüfe ich:\n",
    "- Welche Tabellen existieren in der SQLite-Datenbank\n",
    "- Wie viele Zeilen jede Tabelle enthält\n",
    "- Welche Spalten und Datentypen sie besitzen\n",
    "- Welche Felder Primärschlüssel sind\n"
   ],
   "id": "ba9960315e243660"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-04T09:46:06.260656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "con_overview = sqlite3.connect(str(RAW_SPOTIFY_DB_PATH))\n",
    "\n",
    "# Gibt alle Benutzertabellen (ohne SQLite-Systemtabellen) zurück\n",
    "def list_user_tables(con):\n",
    "    q = \"\"\"\n",
    "    SELECT name\n",
    "    FROM sqlite_master\n",
    "    WHERE type='table' AND name NOT LIKE 'sqlite_%'\n",
    "    ORDER BY name;\n",
    "    \"\"\"\n",
    "    return pd.read_sql(q, con)['name'].tolist()\n",
    "\n",
    "# Liest Spalteninformationen einer Tabelle (Name, Typ, Primärschlüssel usw.)\n",
    "def get_table_info(con, table):\n",
    "    info = pd.read_sql(f\"PRAGMA table_info({table});\", con)\n",
    "    return info\n",
    "\n",
    "# Gibt die Anzahl der Zeilen einer Tabelle zurück\n",
    "def get_rowcount(con, table):\n",
    "    try:\n",
    "        return pd.read_sql(f\"SELECT COUNT(*) AS n FROM {table};\", con).iloc[0, 0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Baut eine Gesamtübersicht: Tabellenname, Zeilenanzahl, Spaltenanzahl, Spaltenliste\n",
    "def build_db_summary(con, max_preview_chars: int = 140):\n",
    "    tables = list_user_tables(con)\n",
    "    rows = []\n",
    "    column_details = {}\n",
    "\n",
    "    for t in tables:\n",
    "        info = get_table_info(con, t)\n",
    "        column_details[t] = info\n",
    "\n",
    "        # Spaltenliste mit Typ und PK-Markierung\n",
    "        col_display = [\n",
    "            f\"{r['name']} ({r['type']})\" + (\" [PK]\" if r['pk'] == 1 else \"\")\n",
    "            for _, r in info.iterrows()\n",
    "        ]\n",
    "        rowcount = get_rowcount(con, t)\n",
    "\n",
    "        rows.append({\n",
    "            \"table\": t,\n",
    "            \"rowcount\": rowcount,\n",
    "            \"n_columns\": len(col_display),\n",
    "            \"columns_full\": \", \".join(col_display)\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values(\"rowcount\", ascending=False)\n",
    "    # Kürze Spaltenliste für Übersicht (volle Liste bleibt in columns_full)\n",
    "    summary[\"columns_preview\"] = summary[\"columns_full\"].apply(\n",
    "        lambda s: s if len(s) <= max_preview_chars else s[:max_preview_chars] + \"…\"\n",
    "    )\n",
    "    summary = summary[[\"table\", \"rowcount\", \"n_columns\", \"columns_preview\", \"columns_full\"]]\n",
    "    return summary, column_details\n",
    "\n",
    "#  Erstelle die Übersicht und zeige sie an\n",
    "db_summary, db_column_details = build_db_summary(con_overview)\n",
    "\n",
    "print(\"Tabellen-Übersicht:\")\n",
    "display(db_summary[[\"table\", \"rowcount\", \"n_columns\", \"columns_preview\"]])\n",
    "\n",
    "#  Verbindung schließen\n",
    "con_overview.close()\n",
    "\n"
   ],
   "id": "7bb549de60163ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  1.2 Überblick über Tabelleninhalte\n",
    "\n",
    "#### Ziel\n",
    "\n",
    "Nachdem wir das Schema und die Zeilenzahlen kennen, wollen wir uns nun die Inhalte der wichtigsten Tabellen ansehen.\n",
    "Wir prüfen einige Beispieleinträge und Datentypen, um zu verstehen, welche Informationen für Analysen relevant sind.\n",
    "\n",
    "#### Vorgehen\n",
    "\n",
    "Wir laden jeweils die erste Zeile der Tabellen nur zur Überblick."
   ],
   "id": "3a8385e622d0d079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "key_tables = ['artists', 'tracks', 'audio_features', 'albums', 'genres']\n",
    "con = sqlite3.connect(str(RAW_SPOTIFY_DB_PATH))\n",
    "for t in key_tables:\n",
    "    print(f\"\\n{'='*70}\\n Tabelle: {t.upper()}\\n{'='*70}\")\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {t} LIMIT 5;\", con)\n",
    "        display(df.head())\n",
    "        print(\"\\n Spaltentypen:\")\n",
    "        display(df.dtypes.to_frame(name=\"dtype\"))\n",
    "    except Exception as e:\n",
    "        print(f\" Fehler beim Lesen von {t}: {e}\")"
   ],
   "id": "34aa96659f4a85be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Export der Tabellen als CSV-Dateien\n",
    "\n",
    "**Ziel**\n",
    "Um effizient mit den Daten arbeiten zu können, exportiere ich ausgewählte Tabellen aus der SQLite-Datenbank als CSV-Dateien.\n",
    "Dadurch können die weiteren Analysen und Modellierungen mit Pandas durchgeführt werden, ohne die komplette Datenbank bei jedem Schritt erneut laden zu müssen.\n",
    "\n",
    "**Vorgehen**\n",
    "- Es werden die zentralen Entitäten (`tracks`, `audio_features`, `artists`, `albums`) sowie die relevanten Beziehungstabellen exportiert.\n",
    "- Aufgrund der Größe der Datenbank wird **keine Vollkopie**, sondern eine **repräsentative Stichprobe** erzeugt.\n",
    "- Die Stichprobe wird direkt in der Datenbank mittels SQL erstellt, um Speicherverbrauch und Laufzeit gering zu halten.\n",
    "- Die Auswahl stellt sicher, dass:\n",
    "  - das gesamte Popularitätsspektrum abgedeckt ist,\n",
    "  - seltene, aber relevante Fälle (z. B. sehr populäre Tracks oder explizite Inhalte) enthalten sind,\n",
    "  - alle exportierten Tabellen zueinander konsistent bleiben.\n",
    "- Die erzeugten CSV-Dateien werden in einem sample-spezifischen Verzeichnis gespeichert, sodass unterschiedliche Stichproben vergleichbar analysiert werden können.\n",
    "\n",
    "**Ergebnis**\n",
    "- Mehrere zusammenhängende CSV-Dateien liegen unter `../data/interim/converted_sqlite_samples/<sample_name>/`.\n",
    "- Diese Dateien bilden die Grundlage für die explorative Datenanalyse, Feature-Engineering-Schritte und die Modellierung in den folgenden Notebooks.\n"
   ],
   "id": "3f0846a87030e9e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# Exportiere eine verbundene, ML-freundliche Stichprobe (<= 300k Tracks)\n",
    "# aus einer großen Spotify-ähnlichen SQLite-DB, ohne komplette Tabellen\n",
    "# in pandas zu laden.\n",
    "#\n",
    "# UPDATE: ROWID_MOD jetzt so angepasst, dass jede \"Slice\" ~300k Tracks hat:\n",
    "#   -> Statt (rowid % ROW_MOD) = k wird jetzt IN (k..k+ROW_BUCKETS_PER_SLICE-1) genutzt\n",
    "# ================================================================\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# Choose sampling mode:\n",
    "# - \"HYBRID_STRATIFIED\" (your original logic)\n",
    "# - \"ROWID_MOD\"         (systematic slicing; now supports ~300k per slice)\n",
    "# - \"FAST_APPROX\"       (fast random-ish)\n",
    "SAMPLING_MODE = \"ROWID_MOD\"\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Repro/quality knobs\n",
    "# ----------------------------------------------------------------\n",
    "REQUIRE_AUDIO_FEATURES = True\n",
    "MIN_EXPLICIT_SHARE = 0.25\n",
    "HIT_MIN_POP = 70\n",
    "HIT_SHARE_IN_SAMPLE = 0.20\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# For FAST_APPROX randomness\n",
    "# ----------------------------------------------------------------\n",
    "APPROX_MOD = 200\n",
    "APPROX_THRESH = 5\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# ROWID slicing knobs (IMPORTANT)\n",
    "# ----------------------------------------------------------------\n",
    "# ROW_MOD controls how many partitions exist in total. With ~8.7M tracks:\n",
    "# - ROW_MOD=200 => each bucket ~44k -> need ~7 buckets per slice to reach 300k\n",
    "ROW_MOD = 200\n",
    "\n",
    "# how many buckets to UNION per slice to hit ~300k\n",
    "ROW_BUCKETS_PER_SLICE = 7\n",
    "\n",
    "# Optional: add randomization to HYBRID_STRATIFIED to get different reruns\n",
    "HYBRID_RANDOMIZE = True\n",
    "HYBRID_RANDOM_MOD = 200\n",
    "HYBRID_RANDOM_THRESH = 10\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Connect + PRAGMAs (performance)\n",
    "# ----------------------------------------------------------------\n",
    "con = sqlite3.connect(str(RAW_SPOTIFY_DB_PATH))\n",
    "con.text_factory = lambda b: b.decode(\"utf-8\", \"replace\")\n",
    "\n",
    "con.execute(\"PRAGMA journal_mode=OFF;\")\n",
    "con.execute(\"PRAGMA synchronous=OFF;\")\n",
    "con.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "con.execute(\"PRAGMA cache_size=-200000;\")\n",
    "con.execute(\"PRAGMA mmap_size=30000000000;\")\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Cleanup previous TEMP tables (safe reruns)\n",
    "# ----------------------------------------------------------------\n",
    "cur.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS temp.sel_tracks;\n",
    "DROP TABLE IF EXISTS temp.sel_r_albums_tracks;\n",
    "DROP TABLE IF EXISTS temp.sel_r_track_artist;\n",
    "DROP TABLE IF EXISTS temp.sel_albums;\n",
    "DROP TABLE IF EXISTS temp.sel_artists;\n",
    "DROP TABLE IF EXISTS temp.sel_audio_features;\n",
    "DROP TABLE IF EXISTS temp.sel_r_artist_genre;\n",
    "DROP TABLE IF EXISTS temp.sel_genres;\n",
    "DROP TABLE IF EXISTS temp.sel_r_albums_artists;\n",
    "\"\"\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1) Track selection\n",
    "# ----------------------------------------------------------------\n",
    "audio_filter = \"AND audio_feature_id IS NOT NULL\" if REQUIRE_AUDIO_FEATURES else \"\"\n",
    "\n",
    "hybrid_rand_clause = \"\"\n",
    "if HYBRID_RANDOMIZE:\n",
    "    hybrid_rand_clause = f\"AND (abs(random()) % {HYBRID_RANDOM_MOD}) < {HYBRID_RANDOM_THRESH}\"\n",
    "\n",
    "if SAMPLING_MODE == \"HYBRID_STRATIFIED\":\n",
    "\n",
    "    # Compute quotas\n",
    "    n_explicit = int(TARGET_TRACKS * MIN_EXPLICIT_SHARE)\n",
    "    n_hit = int(TARGET_TRACKS * HIT_SHARE_IN_SAMPLE)\n",
    "\n",
    "    remainder = max(TARGET_TRACKS - n_explicit - n_hit, 0)\n",
    "\n",
    "    n_b1 = remainder // 4\n",
    "    n_b2 = remainder // 4\n",
    "    n_b3 = remainder // 4\n",
    "    n_b4 = remainder - (n_b1 + n_b2 + n_b3)\n",
    "\n",
    "    cur.executescript(\"\"\"\n",
    "    CREATE TEMP TABLE sel_tracks (\n",
    "        track_id TEXT PRIMARY KEY,\n",
    "        disc_number INTEGER,\n",
    "        duration INTEGER,\n",
    "        explicit INTEGER,\n",
    "        audio_feature_id TEXT,\n",
    "        name TEXT,\n",
    "        preview_url TEXT,\n",
    "        track_number INTEGER,\n",
    "        popularity INTEGER,\n",
    "        is_playable INTEGER\n",
    "    );\n",
    "    CREATE INDEX IF NOT EXISTS idx_sel_tracks_id ON sel_tracks(track_id);\n",
    "    CREATE INDEX IF NOT EXISTS idx_sel_tracks_af ON sel_tracks(audio_feature_id);\n",
    "    \"\"\")\n",
    "\n",
    "    # (A) Explicit slice\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE explicit = 1\n",
    "          AND popularity IS NOT NULL\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_explicit};\n",
    "    \"\"\")\n",
    "\n",
    "    # (B) High-popularity slice\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE popularity >= {HIT_MIN_POP}\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_hit};\n",
    "    \"\"\")\n",
    "\n",
    "    # (C) Balanced buckets\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE explicit = 0\n",
    "          AND popularity BETWEEN 0 AND 20\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_b1};\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE explicit = 0\n",
    "          AND popularity BETWEEN 21 AND 40\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_b2};\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE explicit = 0\n",
    "          AND popularity BETWEEN 41 AND 60\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_b3};\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "        INSERT OR IGNORE INTO sel_tracks\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE explicit = 0\n",
    "          AND popularity BETWEEN 61 AND {HIT_MIN_POP - 1}\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "          {hybrid_rand_clause}\n",
    "        LIMIT {n_b4};\n",
    "    \"\"\")\n",
    "\n",
    "    # (D) Filler\n",
    "    n_now = cur.execute(\"SELECT COUNT(*) FROM sel_tracks;\").fetchone()[0]\n",
    "    missing = TARGET_TRACKS - n_now\n",
    "\n",
    "    if missing > 0:\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT OR IGNORE INTO sel_tracks\n",
    "            SELECT id AS track_id,\n",
    "                   disc_number, duration, explicit, audio_feature_id,\n",
    "                   name, preview_url, track_number, popularity, is_playable\n",
    "            FROM tracks\n",
    "            WHERE (abs(random()) % {APPROX_MOD}) < {APPROX_THRESH}\n",
    "              {audio_filter}\n",
    "              AND duration IS NOT NULL AND duration > 0\n",
    "              AND popularity IS NOT NULL\n",
    "            LIMIT {missing};\n",
    "        \"\"\")\n",
    "\n",
    "elif SAMPLING_MODE == \"ROWID_MOD\":\n",
    "    # NEW: multiple buckets per slice => ~300k rows per slice\n",
    "    row_ks = [(ROW_K_START + i) % ROW_MOD for i in range(ROW_BUCKETS_PER_SLICE)]\n",
    "    row_k_sql = \",\".join(map(str, row_ks))\n",
    "    print(f\"[{SAMPLE_NAME}] Using ROWID buckets: {row_ks}\")\n",
    "\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TEMP TABLE sel_tracks AS\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE (rowid % {ROW_MOD}) IN ({row_k_sql})\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "        LIMIT {TARGET_TRACKS};\n",
    "    \"\"\")\n",
    "\n",
    "elif SAMPLING_MODE == \"FAST_APPROX\":\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TEMP TABLE sel_tracks AS\n",
    "        SELECT id AS track_id,\n",
    "               disc_number, duration, explicit, audio_feature_id,\n",
    "               name, preview_url, track_number, popularity, is_playable\n",
    "        FROM tracks\n",
    "        WHERE (abs(random()) % {APPROX_MOD}) < {APPROX_THRESH}\n",
    "          {audio_filter}\n",
    "          AND duration IS NOT NULL AND duration > 0\n",
    "          AND popularity IS NOT NULL\n",
    "        LIMIT {TARGET_TRACKS};\n",
    "    \"\"\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SAMPLING_MODE: {SAMPLING_MODE}\")\n",
    "\n",
    "# Ensure indexes exist (safe for all modes)\n",
    "cur.executescript(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_tracks_id ON sel_tracks(track_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_tracks_af ON sel_tracks(audio_feature_id);\n",
    "\"\"\")\n",
    "\n",
    "# Info print\n",
    "n_tracks = cur.execute(\"SELECT COUNT(*) FROM sel_tracks;\").fetchone()[0]\n",
    "print(f\"[{SAMPLE_NAME}] Selected tracks: {n_tracks:,} (target {TARGET_TRACKS:,})\")\n",
    "\n",
    "stats = cur.execute(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN explicit=1 THEN 1 ELSE 0 END) AS n_explicit,\n",
    "  SUM(CASE WHEN popularity>=70 THEN 1 ELSE 0 END) AS n_hit_like,\n",
    "  AVG(popularity) AS avg_pop\n",
    "FROM sel_tracks;\n",
    "\"\"\").fetchone()\n",
    "print(f\" explicit: {stats[0]:,} | hit-like(pop>=70): {stats[1]:,} | avg popularity: {stats[2]:.2f}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2) Filter relation tables (DB-side joins)\n",
    "# ----------------------------------------------------------------\n",
    "cur.executescript(\"\"\"\n",
    "CREATE TEMP TABLE sel_r_albums_tracks AS\n",
    "SELECT r.album_id, r.track_id\n",
    "FROM r_albums_tracks r\n",
    "JOIN sel_tracks t ON t.track_id = r.track_id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rat_track ON sel_r_albums_tracks(track_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rat_album ON sel_r_albums_tracks(album_id);\n",
    "\n",
    "CREATE TEMP TABLE sel_r_track_artist AS\n",
    "SELECT r.track_id, r.artist_id\n",
    "FROM r_track_artist r\n",
    "JOIN sel_tracks t ON t.track_id = r.track_id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rta_track ON sel_r_track_artist(track_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rta_artist ON sel_r_track_artist(artist_id);\n",
    "\"\"\")\n",
    "\n",
    "n_rat = cur.execute(\"SELECT COUNT(*) FROM sel_r_albums_tracks;\").fetchone()[0]\n",
    "n_rta = cur.execute(\"SELECT COUNT(*) FROM sel_r_track_artist;\").fetchone()[0]\n",
    "print(f\" sel_r_albums_tracks: {n_rat:,}\")\n",
    "print(f\" sel_r_track_artist:  {n_rta:,}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3) Derive linked entity sets (albums, artists, audio_features, genres)\n",
    "# ----------------------------------------------------------------\n",
    "cur.executescript(\"\"\"\n",
    "CREATE TEMP TABLE sel_albums AS\n",
    "SELECT a.id, a.name, a.album_group, a.album_type, a.release_date, a.popularity\n",
    "FROM albums a\n",
    "JOIN sel_r_albums_tracks rat ON rat.album_id = a.id\n",
    "GROUP BY a.id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_albums_id ON sel_albums(id);\n",
    "\n",
    "CREATE TEMP TABLE sel_artists AS\n",
    "SELECT ar.id, ar.name, ar.popularity, ar.followers\n",
    "FROM artists ar\n",
    "JOIN sel_r_track_artist rta ON rta.artist_id = ar.id\n",
    "GROUP BY ar.id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_artists_id ON sel_artists(id);\n",
    "\n",
    "CREATE TEMP TABLE sel_audio_features AS\n",
    "SELECT af.id, af.acousticness, af.analysis_url, af.danceability, af.duration,\n",
    "       af.energy, af.instrumentalness, af.key, af.liveness, af.loudness,\n",
    "       af.mode, af.speechiness, af.tempo, af.time_signature, af.valence\n",
    "FROM audio_features af\n",
    "JOIN sel_tracks t ON t.audio_feature_id = af.id\n",
    "GROUP BY af.id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_af_id ON sel_audio_features(id);\n",
    "\"\"\")\n",
    "\n",
    "cur.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS temp.sel_r_artist_genre;\n",
    "CREATE TEMP TABLE sel_r_artist_genre AS\n",
    "SELECT rag.genre_id, rag.artist_id\n",
    "FROM r_artist_genre rag\n",
    "JOIN sel_artists a ON a.id = rag.artist_id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rag_artist ON sel_r_artist_genre(artist_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_rag_genre  ON sel_r_artist_genre(genre_id);\n",
    "\n",
    "DROP TABLE IF EXISTS temp.sel_genres;\n",
    "CREATE TEMP TABLE sel_genres AS\n",
    "SELECT g.id\n",
    "FROM genres g\n",
    "JOIN sel_r_artist_genre rag ON rag.genre_id = g.id\n",
    "GROUP BY g.id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_genres_id ON sel_genres(id);\n",
    "\"\"\")\n",
    "\n",
    "cur.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS temp.sel_r_albums_artists;\n",
    "CREATE TEMP TABLE sel_r_albums_artists AS\n",
    "SELECT r.album_id, r.artist_id\n",
    "FROM r_albums_artists r\n",
    "JOIN sel_albums a ON a.id = r.album_id;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_raa_album ON sel_r_albums_artists(album_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_sel_raa_artist ON sel_r_albums_artists(artist_id);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4) Export TEMP tables to CSV\n",
    "# ----------------------------------------------------------------\n",
    "def dump(name: str, sql: str):\n",
    "    df = pd.read_sql(sql, con)\n",
    "    out = EXPORT_DIR / f\"{name}.csv\"\n",
    "    df.to_csv(out, index=False, header=True, encoding=\"utf-8-sig\")\n",
    "    print(f\" {name:<20} → {out.name} ({len(df):,} rows)\")\n",
    "\n",
    "\n",
    "dump(\"tracks\", \"SELECT * FROM sel_tracks;\")\n",
    "dump(\"r_albums_tracks\", \"SELECT * FROM sel_r_albums_tracks;\")\n",
    "dump(\"r_track_artist\", \"SELECT * FROM sel_r_track_artist;\")\n",
    "dump(\"albums\", \"SELECT * FROM sel_albums;\")\n",
    "dump(\"artists\", \"SELECT * FROM sel_artists;\")\n",
    "dump(\"audio_features\", \"SELECT * FROM sel_audio_features;\")\n",
    "dump(\"r_artist_genre\", \"SELECT * FROM sel_r_artist_genre;\")\n",
    "dump(\"genres\", \"SELECT * FROM sel_genres;\")\n",
    "dump(\"r_albums_artists\", \"SELECT * FROM sel_r_albums_artists;\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5) Save selection metadata (repro + audit)\n",
    "# ----------------------------------------------------------------\n",
    "df_ids = pd.read_sql(\"SELECT track_id FROM sel_tracks;\", con)\n",
    "df_ids.to_csv(EXPORT_DIR / \"selected_track_ids.csv\", index=False)\n",
    "\n",
    "print(\"Export complete\")\n"
   ],
   "id": "1caf8ec450c69ee1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 Deskriptive Statistiken & Verteilungen\n",
    "Ich untersuche numerische Spalten (z. B. Popularity, Energy, Danceability) auf ihre Verteilungen, Ausreißer und Mittelwerte."
   ],
   "id": "44ca3b5ba25d83af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BASE_EXPORT_DIR = Path(\"../data/interim/converted_sqlite_samples\")\n",
    "cfg = json.loads((BASE_EXPORT_DIR / \"current_sample.json\").read_text())\n",
    "DATA_DIR = Path(cfg[\"EXPORT_DIR\"])\n",
    "SCHEMA_REPORTS_DIR = Path(f\"../data/reports/01_schema_overview/{SAMPLE_NAME}\")\n",
    "SCHEMA_REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Using sample:\", cfg[\"SAMPLE_NAME\"], \"| mode:\", cfg[\"SAMPLING_MODE\"])\n",
    "\n",
    "\n",
    "# Lade mehrere Tabellen\n",
    "tracks = pd.read_csv(DATA_DIR / \"tracks.csv\")\n",
    "audio = pd.read_csv(DATA_DIR / \"audio_features.csv\")\n",
    "artists = pd.read_csv(DATA_DIR / \"artists.csv\")\n",
    "albums = pd.read_csv(DATA_DIR / \"albums.csv\")\n",
    "\n",
    "# -------- Übersicht: Anzahl numerischer Spalten pro Tabelle --------\n",
    "for name, df in {\"tracks\": tracks, \"audio_features\": audio, \"artists\": artists, \"albums\": albums}.items():\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    print(f\"{name:<15} -> numerische Spalten: {len(num_cols)} | {', '.join(num_cols[:8])}{'...' if len(num_cols)>8 else ''}\")"
   ],
   "id": "b183377adaac11e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fast Overview der Tabellen mit describe()",
   "id": "c31ca414efe2a216"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tables = {\"tracks\": tracks, \"audio_features\": audio, \"artists\": artists, \"albums\": albums}\n",
    "\n",
    "out_dir = Path(SCHEMA_REPORTS_DIR) / \"descriptions\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name, df in tables.items():\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    if len(num_cols) == 0:\n",
    "        print(f\"skipped {name}: no numeric columns\")\n",
    "        continue\n",
    "\n",
    "    desc = (\n",
    "        df[num_cols]\n",
    "        .describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "        .T.round(2)\n",
    "    )\n",
    "\n",
    "    out_file = out_dir / f\"{name}_describe.html\"\n",
    "\n",
    "    # Safety: if an old mistake created a DIRECTORY where the file should be, fail clearly\n",
    "    if out_file.exists() and out_file.is_dir():\n",
    "        raise RuntimeError(f\"{out_file} is a directory. Delete it and rerun.\")\n",
    "\n",
    "    desc.to_html(out_file)  # pathlib Path is fine\n",
    "    print(\"saved:\", out_file)\n",
    "\n",
    "\n"
   ],
   "id": "3aae086a89b7d4b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Histogramme für ausgewählte numerische Spalten",
   "id": "df98e329f2a21b4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HIST_DIR = SCHEMA_REPORTS_DIR / \"histograms\"\n",
    "HIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def numeric_profile_grid(\n",
    "    df,\n",
    "    cols,\n",
    "    title,\n",
    "    bins=40,\n",
    "    clip_q=(0.01, 0.99),\n",
    "    log_mode=\"auto\",      # \"auto\", True, False\n",
    "    show_box=True,\n",
    "    save_path=None,       # NEW\n",
    "):\n",
    "    \"\"\"\n",
    "    Pro EDA Grid pro Feature:\n",
    "      - links: Histogram (geclippt auf Quantile)\n",
    "      - rechts: log1p-Hist (bei heavy tails) ODER Boxplot\n",
    "    \"\"\"\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        print(\"Keine gültigen Spalten gefunden.\")\n",
    "        return\n",
    "\n",
    "    n = len(cols)\n",
    "    nrows = n\n",
    "    ncols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 3.2 * nrows))\n",
    "    if nrows == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        s = df[col]\n",
    "        s_nonnull = s.dropna()\n",
    "\n",
    "        missing_rate = 1.0 - (len(s_nonnull) / max(len(s), 1))\n",
    "        zero_rate = float((s_nonnull == 0).mean()) if len(s_nonnull) else np.nan\n",
    "\n",
    "        if len(s_nonnull) == 0:\n",
    "            axes[i, 0].set_title(f\"{col} (all missing)\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        q_low, q_high = s_nonnull.quantile(clip_q[0]), s_nonnull.quantile(clip_q[1])\n",
    "        s_clip = s_nonnull.clip(q_low, q_high)\n",
    "\n",
    "        use_log = False\n",
    "        if log_mode is True:\n",
    "            use_log = True\n",
    "        elif log_mode == \"auto\" and s_nonnull.min() >= 0:\n",
    "            p50 = s_nonnull.quantile(0.50)\n",
    "            p99 = s_nonnull.quantile(0.99)\n",
    "            if p50 > 0 and (p99 / p50) >= 10:\n",
    "                use_log = True\n",
    "\n",
    "        # LEFT\n",
    "        axL = axes[i, 0]\n",
    "        axL.hist(s_clip.values, bins=bins)\n",
    "        axL.set_ylabel(\"count\")\n",
    "        axL.set_title(\n",
    "            f\"{col} | miss={missing_rate:.1%} | zero={zero_rate:.1%} | clip=[{q_low:.3g},{q_high:.3g}]\"\n",
    "        )\n",
    "\n",
    "        # RIGHT\n",
    "        axR = axes[i, 1]\n",
    "        if use_log:\n",
    "            s_log = np.log1p(s_nonnull.clip(lower=0))\n",
    "            axR.hist(s_log.values, bins=bins)\n",
    "            axR.set_title(f\"{col} (log1p view)\")\n",
    "        else:\n",
    "            if show_box:\n",
    "                axR.boxplot(s_nonnull.values, vert=False, showfliers=True)\n",
    "                axR.set_title(f\"{col} (boxplot)\")\n",
    "            else:\n",
    "                axR.axis(\"off\")\n",
    "\n",
    "        for ax in (axL, axR):\n",
    "            ax.grid(alpha=0.2)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"saved:\", save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Columns\n",
    "track_cols = [c for c in [\"popularity\", \"duration\", \"track_number\", \"disc_number\", \"explicit\"] if c in tracks.columns]\n",
    "audio_cols = [c for c in [\"danceability\", \"energy\", \"loudness\", \"valence\", \"tempo\",\n",
    "                          \"acousticness\", \"instrumentalness\", \"speechiness\", \"time_signature\"] if c in audio.columns]\n",
    "artist_cols = [c for c in [\"popularity\", \"followers\"] if c in artists.columns]\n",
    "album_cols = [c for c in [\"popularity\", \"release_date\"] if c in albums.columns]\n",
    "\n",
    "numeric_profile_grid(\n",
    "    tracks,\n",
    "    track_cols,\n",
    "    \"Tracks – Verteilungen (clipped + log/box)\",\n",
    "    bins=40,\n",
    "    save_path=HIST_DIR / \"tracks_numeric_profiles.png\"\n",
    ")\n",
    "\n",
    "numeric_profile_grid(\n",
    "    audio,\n",
    "    audio_cols,\n",
    "    \"Audio Features – Verteilungen (clipped + log/box)\",\n",
    "    bins=40,\n",
    "    save_path=HIST_DIR / \"audio_features_numeric_profiles.png\"\n",
    ")\n",
    "\n",
    "numeric_profile_grid(\n",
    "    artists,\n",
    "    artist_cols,\n",
    "    \"Artists – Popularität & Follower (clipped + log/box)\",\n",
    "    bins=40,\n",
    "    save_path=HIST_DIR / \"artists_numeric_profiles.png\"\n",
    ")\n",
    "\n",
    "numeric_profile_grid(\n",
    "    albums,\n",
    "    album_cols,\n",
    "    \"Albums – Popularität & Release Date (clipped + log/box)\",\n",
    "    bins=40,\n",
    "    save_path=HIST_DIR / \"albums_numeric_profiles.png\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ab0fdaa213cd7858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ausreißer-Erkennung\n",
   "id": "dcca5a60b438ea3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "OUTLIER_DIR = Path(SCHEMA_REPORTS_DIR) / \"outliers\"\n",
    "OUTLIER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Rule-based validators\n",
    "# ---------------------------\n",
    "def default_rules():\n",
    "    \"\"\"\n",
    "    Domain-Rules für Spotify-ähnliche Daten.\n",
    "    Passe sie bei Bedarf an.\n",
    "    Jede Rule liefert eine Bool-Maske (True = invalid).\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Tracks\n",
    "        \"duration\": lambda s: (s <= 0) | (s > 30 * 60 * 1000),          # >30min in ms\n",
    "        \"track_number\": lambda s: (s <= 0) | (s > 200),                # >200 sehr verdächtig\n",
    "        \"disc_number\": lambda s: (s <= 0) | (s > 10),                  # >10 sehr selten / suspicious\n",
    "        \"popularity\": lambda s: (s < 0) | (s > 100),                   # Spotify popularity bounds\n",
    "        \"explicit\": lambda s: ~s.isin([0, 1]),\n",
    "\n",
    "        # Audio\n",
    "        \"tempo\": lambda s: (s <= 0) | (s > 300),                       # tempo 0 invalid, >300 rare\n",
    "        \"time_signature\": lambda s: ~s.isin([3, 4, 5]),                # 4/4 dominant, 3/4 & 5/4 ok\n",
    "        \"loudness\": lambda s: (s < -40) | (s > 5),                     # -60 suspicious, >5 suspicious\n",
    "\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Quantile outliers\n",
    "# ---------------------------\n",
    "def quantile_outliers(s: pd.Series, low_q=0.005, high_q=0.995):\n",
    "    s = s.dropna()\n",
    "    if len(s) == 0:\n",
    "        return pd.Series([], dtype=bool), (np.nan, np.nan)\n",
    "    lo, hi = s.quantile(low_q), s.quantile(high_q)\n",
    "    mask = (s < lo) | (s > hi)\n",
    "    return mask, (lo, hi)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) IQR outliers (classic)\n",
    "# ---------------------------\n",
    "def iqr_outliers(s: pd.Series, k=1.5):\n",
    "    s = s.dropna()\n",
    "    if len(s) == 0:\n",
    "        return pd.Series([], dtype=bool), (np.nan, np.nan)\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - k * iqr, q3 + k * iqr\n",
    "    mask = (s < lo) | (s > hi)\n",
    "    return mask, (lo, hi)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Release-date validation helper\n",
    "# ---------------------------\n",
    "def validate_release_date_epoch_ms(series, min_year=1900, max_year=None):\n",
    "    \"\"\"\n",
    "    Erwartet epoch in ms (wie Spotify häufig).\n",
    "    Gibt invalid mask zurück (True=invalid) + year series.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if max_year is None:\n",
    "        max_year = pd.Timestamp.today().year + 1\n",
    "\n",
    "    # convert epoch ms -> datetime\n",
    "    dt = pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "    year = dt.dt.year\n",
    "\n",
    "    invalid = dt.isna() | (year < min_year) | (year > max_year)\n",
    "    return invalid, year\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Main analyzer\n",
    "# ---------------------------\n",
    "def robust_outlier_report(\n",
    "    df: pd.DataFrame,\n",
    "    cols=None,\n",
    "    q_low=0.005,\n",
    "    q_high=0.995,\n",
    "    use_iqr=True,\n",
    "    iqr_k=1.5,\n",
    "    rules=None,\n",
    "    include_indices=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Erzeugt einen Report mit:\n",
    "      - missing%, zero%\n",
    "      - invalid_count (rules)\n",
    "      - quantile_outliers_count (+ quantile bounds)\n",
    "      - iqr_outliers_count (+ iqr bounds)\n",
    "\n",
    "    include_indices=True -> liefert zusätzlich dict mit indices der auffälligen rows pro col.\n",
    "    \"\"\"\n",
    "    if cols is None:\n",
    "        cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    if rules is None:\n",
    "        rules = default_rules()\n",
    "\n",
    "    rows = []\n",
    "    idx_dict = {} if include_indices else None\n",
    "\n",
    "    n_total = len(df)\n",
    "\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        s_full = df[col]\n",
    "        if not pd.api.types.is_numeric_dtype(s_full):\n",
    "            continue\n",
    "\n",
    "        s = s_full.dropna()\n",
    "        n_nonnull = len(s)\n",
    "        miss_rate = 1 - (n_nonnull / max(n_total, 1))\n",
    "\n",
    "        # zero-rate (only meaningful for numeric)\n",
    "        try:\n",
    "            zero_rate = float((s == 0).mean()) if n_nonnull > 0 else np.nan\n",
    "        except Exception:\n",
    "            zero_rate = np.nan\n",
    "\n",
    "        # ---- rule-based invalids ----\n",
    "        invalid_count = 0\n",
    "        invalid_rate = 0.0\n",
    "        invalid_bounds = None\n",
    "\n",
    "        if col == \"release_date\":\n",
    "            inv_mask, year = validate_release_date_epoch_ms(s_full)\n",
    "            invalid_count = int(inv_mask.sum())\n",
    "            invalid_rate = invalid_count / max(n_total, 1)\n",
    "            invalid_bounds = f\"year∉[1900,{pd.Timestamp.today().year+1}] or parse fail\"\n",
    "            if include_indices:\n",
    "                idx_dict.setdefault(col, {})\n",
    "                idx_dict[col][\"invalid_idx\"] = df.index[inv_mask].tolist()\n",
    "        elif col in rules:\n",
    "            inv_mask = rules[col](s_full)\n",
    "            inv_mask = inv_mask.fillna(False)\n",
    "            invalid_count = int(inv_mask.sum())\n",
    "            invalid_rate = invalid_count / max(n_total, 1)\n",
    "            if include_indices:\n",
    "                idx_dict.setdefault(col, {})\n",
    "                idx_dict[col][\"invalid_idx\"] = df.index[inv_mask].tolist()\n",
    "\n",
    "        # ---- quantile outliers ----\n",
    "        q_mask, (q_lo, q_hi) = quantile_outliers(s_full, q_low, q_high)\n",
    "        # q_mask is on dropna() series index; align back:\n",
    "        q_idx = q_mask[q_mask].index if len(q_mask) else []\n",
    "        q_count = len(q_idx)\n",
    "        q_rate = q_count / max(n_total, 1)\n",
    "\n",
    "        if include_indices:\n",
    "            idx_dict.setdefault(col, {})\n",
    "            idx_dict[col][\"quantile_idx\"] = list(q_idx)\n",
    "\n",
    "        # ---- iqr outliers ----\n",
    "        iqr_count = np.nan\n",
    "        iqr_rate = np.nan\n",
    "        iqr_lo = np.nan\n",
    "        iqr_hi = np.nan\n",
    "\n",
    "        if use_iqr:\n",
    "            i_mask, (i_lo, i_hi) = iqr_outliers(s_full, iqr_k)\n",
    "            i_idx = i_mask[i_mask].index if len(i_mask) else []\n",
    "            iqr_count = len(i_idx)\n",
    "            iqr_rate = iqr_count / max(n_total, 1)\n",
    "            iqr_lo, iqr_hi = i_lo, i_hi\n",
    "            if include_indices:\n",
    "                idx_dict.setdefault(col, {})\n",
    "                idx_dict[col][\"iqr_idx\"] = list(i_idx)\n",
    "\n",
    "        rows.append({\n",
    "            \"col\": col,\n",
    "            \"missing_%\": round(miss_rate * 100, 2),\n",
    "            \"zero_%\": round(zero_rate * 100, 2) if pd.notna(zero_rate) else np.nan,\n",
    "\n",
    "            \"invalid_n\": invalid_count,\n",
    "            \"invalid_%\": round(invalid_rate * 100, 3),\n",
    "            \"invalid_rule\": invalid_bounds if invalid_bounds else (\"custom_rule\" if col in rules else \"\"),\n",
    "\n",
    "            \"q_outliers_n\": q_count,\n",
    "            \"q_outliers_%\": round(q_rate * 100, 3),\n",
    "            \"q_bounds\": f\"[{q_lo:.3g}, {q_hi:.3g}]\" if pd.notna(q_lo) else \"\",\n",
    "\n",
    "            \"iqr_outliers_n\": iqr_count,\n",
    "            \"iqr_outliers_%\": round(iqr_rate * 100, 3) if pd.notna(iqr_rate) else np.nan,\n",
    "            \"iqr_bounds\": f\"[{iqr_lo:.3g}, {iqr_hi:.3g}]\" if pd.notna(iqr_lo) else \"\",\n",
    "        })\n",
    "\n",
    "    report = pd.DataFrame(rows).sort_values(\n",
    "        by=[\"invalid_n\", \"q_outliers_n\"],\n",
    "        ascending=False\n",
    "    )\n",
    "\n",
    "    return (report, idx_dict) if include_indices else report\n"
   ],
   "id": "eedc1f1b1996a615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Anwendung auf die Tabellen",
   "id": "d3d3d135a195f919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tables = {\n",
    "    \"tracks\": tracks,\n",
    "    \"audio_features\": audio,\n",
    "    \"artists\": artists,\n",
    "    \"albums\": albums\n",
    "}\n",
    "\n",
    "reports = {}\n",
    "\n",
    "for name, df in tables.items():\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        print(\"skip (no numeric cols):\", name)\n",
    "        continue\n",
    "\n",
    "    rep = robust_outlier_report(\n",
    "        df,\n",
    "        cols=num_cols,\n",
    "        q_low=0.005,\n",
    "        q_high=0.995,\n",
    "        use_iqr=True,\n",
    "        iqr_k=1.5,\n",
    "        rules=default_rules(),\n",
    "        include_indices=False\n",
    "    )\n",
    "\n",
    "    reports[name] = rep\n",
    "\n",
    "    # -------------------------\n",
    "    # (A) Save FULL report as CSV\n",
    "    # -------------------------\n",
    "    csv_path = OUTLIER_DIR / f\"{name}_robust_outlier_report.csv\"\n",
    "    rep.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"saved:\", csv_path)\n",
    "\n",
    "    # -------------------------\n",
    "    # (B) Save TOP-15 as PNG table\n",
    "    # -------------------------\n",
    "    top = rep.head(15).copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 0.6 * len(top) + 2))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    tbl = ax.table(\n",
    "        cellText=top.values,\n",
    "        colLabels=top.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\"\n",
    "    )\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(8)\n",
    "    tbl.scale(1, 1.2)\n",
    "\n",
    "    png_path = OUTLIER_DIR / f\"{name}_robust_outlier_report_top15.png\"\n",
    "    plt.savefig(png_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"saved:\", png_path)\n",
    "\n",
    "    # -------------------------\n",
    "    # (C) Optional: simple bar chart (invalid_% + q_outliers_%)\n",
    "    # -------------------------\n",
    "    # Keep it readable: show top 12 by invalid_n\n",
    "    plot_df = rep.sort_values(\"invalid_n\", ascending=False).head(12).copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(plot_df[\"col\"], plot_df[\"invalid_%\"])\n",
    "    ax.set_title(f\"{name}: invalid_% (Top 12)\")\n",
    "    ax.set_ylabel(\"invalid_%\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    bar_path = OUTLIER_DIR / f\"{name}_invalid_percent_top12.png\"\n",
    "    fig.savefig(bar_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"saved:\", bar_path)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(plot_df[\"col\"], plot_df[\"q_outliers_%\"])\n",
    "    ax.set_title(f\"{name}: quantile outliers % (Top 12)\")\n",
    "    ax.set_ylabel(\"q_outliers_%\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    bar_path2 = OUTLIER_DIR / f\"{name}_q_outliers_percent_top12.png\"\n",
    "    fig.savefig(bar_path2, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"saved:\", bar_path2)\n",
    "\n"
   ],
   "id": "ebf60a80ca14e7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vergleichende Verteilungen",
   "id": "bab33d70cee0eb0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "POPULARITY_DIR = SCHEMA_REPORTS_DIR / \"popularity_distributions\"\n",
    "POPULARITY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.kdeplot(tracks[\"popularity\"], label=\"Tracks\", fill=True)\n",
    "sns.kdeplot(artists[\"popularity\"], label=\"Artists\", fill=True)\n",
    "plt.title(\"Popularität – Vergleich: Tracks vs. Artists\")\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.legend()\n",
    "plt.savefig(POPULARITY_DIR / \"popularity_tracks_vs_artists.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n"
   ],
   "id": "545eb67842a69f54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Zeitbasierte Analyse",
   "id": "af1082b87990f1d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRACKS_RELEASE_DIR = SCHEMA_REPORTS_DIR / \"time_based_analysis\"\n",
    "TRACKS_RELEASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if \"release_date\" in albums.columns:\n",
    "    albums[\"release_date\"] = pd.to_datetime(albums[\"release_date\"], errors=\"coerce\")\n",
    "    albums[\"year\"] = albums[\"release_date\"].dt.year\n",
    "    year_counts = albums[\"year\"].value_counts().sort_index()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=year_counts.index, y=year_counts.values)\n",
    "    plt.title(\"Anzahl veröffentlichter Alben pro Jahr\")\n",
    "    plt.xlabel(\"Jahr\")\n",
    "    plt.ylabel(\"Anzahl Alben\")\n",
    "    plt.savefig(TRACKS_RELEASE_DIR / \"albums_per_year.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ],
   "id": "82cd2f0e70e73f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5 Feature-Korrelationen\n",
    "\n",
    "####  Ziel\n",
    "Ich analysiere die linearen Zusammenhänge zwischen numerischen Merkmalen\n",
    "(z. B. Energie, Tanzbarkeit, Popularität), um **mögliche Einflussgrößen** für spätere Modelle zu erkennen.\n",
    "\n",
    "####  Vorgehen\n",
    "- Berechne **Korrelationsmatrizen** (Pearson) für mehrere Tabellen.\n",
    "- Visualisiere sie mit **kompakten Heatmaps**.\n",
    "- Untersuche exemplarisch **wichtige Beziehungen** in Scatterplots.\n",
    "- Berechne zusätzlich **Korrelations-Rankings** für Popularität.\n"
   ],
   "id": "ea510879f336807a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "HEAT_DIR = Path(SCHEMA_REPORTS_DIR) / \"correlation_heatmaps\"\n",
    "HEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tables = {\n",
    "    \"Tracks\": tracks,\n",
    "    \"Audio Features\": audio,\n",
    "    \"Artists\": artists,\n",
    "    \"Albums\": albums\n",
    "}\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "for name, df in tables.items():\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    if len(num_cols) < 2:\n",
    "        print(\"skip (not enough numeric cols):\", name)\n",
    "        continue\n",
    "\n",
    "    corr = df[num_cols].corr(numeric_only=True).round(2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        annot=False,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        cbar_kws={\"shrink\": 0.7},\n",
    "        square=True,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"Korrelationsmatrix – {name}\", fontsize=12, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_path = HEAT_DIR / f\"corr_heatmap_{safe_name(name)}.png\"\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"saved:\", out_path)\n"
   ],
   "id": "5eb7926966bf0fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.5.1 Wichtigste Korrelationen mit Popularität\n",
    "Compute and rank correlations vs. popularity (when present)."
   ],
   "id": "6c6fe026091e35c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# slice-aware folder (adjust base if you prefer another location)\n",
    "CORR_DIR = Path(SCHEMA_REPORTS_DIR) / \"correlations\"\n",
    "CORR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def corr_with_popularity(df, name):\n",
    "    if \"popularity\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    if \"popularity\" not in num_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    corr = df[num_cols].corr(numeric_only=True)[\"popularity\"].sort_values(ascending=False)\n",
    "    corr_df = corr.to_frame(name=\"corr_with_popularity\").reset_index()\n",
    "    corr_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "    corr_df[\"table\"] = name\n",
    "    return corr_df\n",
    "\n",
    "# build full table\n",
    "pop_corrs = []\n",
    "for n, d in tables.items():\n",
    "    c = corr_with_popularity(d, n)\n",
    "    if not c.empty:\n",
    "        pop_corrs.append(c)\n",
    "\n",
    "pop_corrs = (\n",
    "    pd.concat(pop_corrs, ignore_index=True)\n",
    "      .dropna(subset=[\"corr_with_popularity\"])\n",
    "      .sort_values(\"corr_with_popularity\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- Save full table to CSV\n",
    "csv_path = CORR_DIR / \"corr_with_popularity_all_tables.csv\"\n",
    "pop_corrs.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"saved:\", csv_path)\n",
    "\n",
    "# --- Save top 10 as PNG table (nice for report)\n",
    "top10 = pop_corrs.head(10).copy()\n",
    "top10[\"corr_with_popularity\"] = top10[\"corr_with_popularity\"].round(3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 0.6 * len(top10) + 1.5))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "tbl = ax.table(\n",
    "    cellText=top10.values,\n",
    "    colLabels=top10.columns,\n",
    "    cellLoc=\"center\",\n",
    "    loc=\"center\"\n",
    ")\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(9)\n",
    "tbl.scale(1, 1.2)\n",
    "\n",
    "png_path = CORR_DIR / \"corr_with_popularity_top10.png\"\n",
    "plt.savefig(png_path, dpi=200, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"saved:\", png_path)\n",
    "\n"
   ],
   "id": "6b0843f3d3275b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5.2 Visualisierung wichtiger Beziehungen",
   "id": "18d8ebc73c4c5b92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SCATTER_DIR = Path(SCHEMA_REPORTS_DIR) / \"scatter_small\"\n",
    "SCATTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def small_scatter(df, x, y, title, out_dir: Path, filename: str = None):\n",
    "    # skip if columns missing\n",
    "    if x not in df.columns or y not in df.columns:\n",
    "        print(f\"skip (missing col): {title} [{x}, {y}]\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    sns.scatterplot(data=df, x=x, y=y, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # safe filename\n",
    "    if filename is None:\n",
    "        safe = title.lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\".\", \"\")\n",
    "        filename = f\"{safe}.png\"\n",
    "\n",
    "    out_path = out_dir / filename\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"saved:\", out_path)\n",
    "\n",
    "\n",
    "\n",
    "# Audio scatters (these will auto-skip if a column doesn't exist)\n",
    "small_scatter(audio, \"energy\", \"danceability\", \"Energy vs. Danceability\", SCATTER_DIR)\n",
    "small_scatter(audio, \"valence\", \"energy\", \"Valence vs. Energy\", SCATTER_DIR)\n",
    "small_scatter(audio, \"tempo\", \"energy\", \"Tempo vs. Energy\", SCATTER_DIR)\n",
    "small_scatter(audio, \"danceability\", \"tempo\", \"Danceability vs. Tempo\", SCATTER_DIR)\n",
    "small_scatter(audio, \"energy\", \"loudness\", \"Energy vs. Loudness\", SCATTER_DIR)\n",
    "small_scatter(audio, \"valence\", \"danceability\", \"Valence vs. Danceability\", SCATTER_DIR)\n",
    "small_scatter(audio, \"instrumentalness\", \"speechiness\", \"Instrumentalness vs. Speechiness\", SCATTER_DIR)\n",
    "small_scatter(audio, \"acousticness\", \"energy\", \"Acousticness vs. Energy\", SCATTER_DIR)\n",
    "\n",
    "# Track scatter\n",
    "small_scatter(tracks, \"duration\", \"popularity\", \"Duration vs. Popularity\", SCATTER_DIR)"
   ],
   "id": "89531ad8e19d7dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5.3 Scatter-Matrix",
   "id": "6ae88293016443bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "SCATTER_DIR = Path(SCHEMA_REPORTS_DIR) / \"scatter\"\n",
    "SCATTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "selected_cols = [c for c in [\"danceability\", \"energy\", \"valence\", \"loudness\", \"tempo\"] if c in audio.columns]\n",
    "\n",
    "if len(selected_cols) >= 3:\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    scatter_matrix(\n",
    "        audio[selected_cols].sample(2000, random_state=42),\n",
    "        figsize=(8, 6),\n",
    "        alpha=0.3,\n",
    "        diagonal=\"kde\"\n",
    "    )\n",
    "\n",
    "    plt.suptitle(\"Scatter-Matrix ausgewählter Audio-Features\", y=1.02, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = SCATTER_DIR / \"audio_features_scatter_matrix.png\"\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"saved:\", out_path)\n"
   ],
   "id": "aed8fd92e2a971c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.6 Missing Values & Data Completeness\n",
    "\n",
    "**Ziel**\n",
    "Ich ermittle die Datenvollständigkeit je Tabelle:\n",
    "- Anteil fehlender Werte (`NaN`) pro Spalte\n",
    "- Heatmap der Missing-Pattern\n",
    "- Export als CSV je Tabelle\n",
    "\n",
    "**Ergebnis**\n",
    "- `missing_<table>.csv` pro Tabelle\n",
    "- `missing_heatmap_<table>.png` (optional)\n",
    "- Interpretation: Wo fehlen Daten signifikant?\n"
   ],
   "id": "264fa7234465221d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "tables = {}\n",
    "for name, fname in [\n",
    "    (\"tracks\", \"tracks.csv\"),\n",
    "    (\"audio_features\", \"audio_features.csv\"),\n",
    "    (\"artists\", \"artists.csv\"),\n",
    "    (\"albums\", \"albums.csv\"),\n",
    "]:\n",
    "    p = DATA_DIR / fname\n",
    "    if p.exists():\n",
    "        tables[name] = pd.read_csv(p)\n",
    "\n",
    "\n",
    "def missing_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ms = (df.isna().mean() * 100).sort_values(ascending=False).round(2)\n",
    "    return ms.to_frame(\"missing_pct\")\n",
    "\n",
    "\n",
    "outMissing = Path(SCHEMA_REPORTS_DIR) / \"missing\"\n",
    "outMissing.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name, df in tables.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    ms = missing_summary(df)\n",
    "    display(ms.head(20))\n",
    "    ms.to_csv(SCHEMA_REPORTS_DIR / \"missing\"  / f\"missing_{name}.csv\", encoding=\"utf-8\")\n",
    "    # Heatmap (nur bei breiten Tabellen sinnvoll)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(df.sample(min(len(df), 1000), random_state=42).isna(), cbar=False)\n",
    "    plt.title(f\"Missing-Heatmap (Sample) – {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SCHEMA_REPORTS_DIR / \"missing\" / f\"missing_heatmap_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n"
   ],
   "id": "145566ee19e9be9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.7 Eindeutigkeit, Duplikate & Fremdschlüssel-Integrität\n",
    "\n",
    "**Ziel**\n",
    "Ich prüfe die Datenintegrität:\n",
    "- Eindeutigkeit der Primärschlüssel (z. B. `id`)\n",
    "- Duplikate pro Tabelle\n",
    "- Fremdschlüssel-Konsistenz (z. B. `r_track_artist.track_id` existiert in `tracks.id`)\n",
    "\n",
    "**Ergebnis**\n",
    "- `integrity_report.csv` mit Kennzahlen\n",
    "- ggf. Listen der verletzten Referenzen\n"
   ],
   "id": "468fcff0853d9dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INTEG_DIR = Path(SCHEMA_REPORTS_DIR) / \"integrity\"\n",
    "INTEG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load(name):\n",
    "    p = DATA_DIR / f\"{name}.csv\"\n",
    "    return pd.read_csv(p) if p.exists() else None\n",
    "\n",
    "# 2) Laden\n",
    "tracks = load(\"tracks\")\n",
    "audio   = load(\"audio_features\")\n",
    "artists = load(\"artists\")\n",
    "albums  = load(\"albums\")\n",
    "rta     = load(\"r_track_artist\")\n",
    "rat     = load(\"r_albums_tracks\")\n",
    "rag     = load(\"r_artist_genre\")\n",
    "genres  = load(\"genres\")\n",
    "\n",
    "# 3) Utils\n",
    "def key_col_for(table_name: str) -> str:\n",
    "    # in deinem Export hat nur tracks die Spalte \"track_id\", alle anderen \"id\"\n",
    "    return \"track_id\" if table_name == \"tracks\" else \"id\"\n",
    "\n",
    "def pct(part, whole):\n",
    "    return 0.0 if whole == 0 else round(100.0 * part / whole, 3)\n",
    "\n",
    "rows = []\n",
    "\n",
    "def report_row(table, check, status, n_bad, n_total):\n",
    "    rows.append({\n",
    "        \"table\": table,\n",
    "        \"check\": check,\n",
    "        \"status\": status,\n",
    "        \"n_bad\": int(n_bad) if n_bad is not None else None,\n",
    "        \"n_total\": int(n_total) if n_total is not None else None,\n",
    "        \"pct_bad\": pct(n_bad, n_total) if (n_bad is not None and n_total) else None\n",
    "    })\n",
    "\n",
    "# 4) Duplikate + (optional) ID-Eindeutigkeit pro Tabelle\n",
    "def check_table(table_name: str, df: pd.DataFrame):\n",
    "    if df is None:\n",
    "        report_row(table_name, \"duplicates_all_cols\", \"skip\", None, None)\n",
    "        report_row(table_name, f\"unique({key_col_for(table_name)})\", \"skip\", None, None)\n",
    "        return\n",
    "    # Duplikate über alle Spalten\n",
    "    dups = df.duplicated().sum()\n",
    "    report_row(table_name, \"duplicates_all_cols\", \"ok\" if dups == 0 else \"warn\", dups, len(df))\n",
    "\n",
    "    # ID-Eindeutigkeit wenn Key existiert\n",
    "    key = key_col_for(table_name)\n",
    "    if key in df.columns:\n",
    "        dup_ids = df[key].duplicated().sum()\n",
    "        report_row(table_name, f\"unique({key})\", \"ok\" if dup_ids == 0 else \"warn\", dup_ids, len(df))\n",
    "    else:\n",
    "        report_row(table_name, f\"unique({key})\", \"skip\", None, None)\n",
    "\n",
    "for name, df in [(\"tracks\", tracks), (\"audio_features\", audio), (\"artists\", artists), (\"albums\", albums),(\"genres\", genres),]:\n",
    "    check_table(name, df)\n",
    "\n",
    "# 5) Fremdschlüssel-Checks (Schema-bewusst)\n",
    "def fk_check(child_df, child_key, parent_df, parent_key, table_label):\n",
    "    if child_df is None or parent_df is None or child_key not in child_df.columns or parent_key not in parent_df.columns:\n",
    "        report_row(table_label, f\"fk({child_key}->{table_label.split(':')[-1]})\", \"skip\", None, None)\n",
    "        return\n",
    "    total = len(child_df)\n",
    "    missing = (~child_df[child_key].isin(parent_df[parent_key])).sum()\n",
    "    report_row(table_label, f\"fk({child_key}->{parent_key})\", \"ok\" if missing == 0 else \"warn\", missing, total)\n",
    "\n",
    "# r_track_artist: track_id -> tracks.track_id ; artist_id -> artists.id\n",
    "if rta is not None:\n",
    "    fk_check(rta, \"track_id\", tracks, key_col_for(\"tracks\"), \"r_track_artist:tracks\")\n",
    "    fk_check(rta, \"artist_id\", artists, key_col_for(\"artists\"), \"r_track_artist:artists\")\n",
    "\n",
    "# r_albums_tracks: track_id -> tracks.track_id ; album_id -> albums.id\n",
    "if rat is not None:\n",
    "    fk_check(rat, \"track_id\", tracks, key_col_for(\"tracks\"), \"r_albums_tracks:tracks\")\n",
    "    fk_check(rat, \"album_id\", albums, key_col_for(\"albums\"), \"r_albums_tracks:albums\")\n",
    "\n",
    "# tracks.audio_feature_id -> audio_features.id\n",
    "if tracks is not None and audio is not None and \"audio_feature_id\" in tracks.columns and \"id\" in audio.columns:\n",
    "    total = tracks[\"audio_feature_id\"].notna().sum()\n",
    "    missing = (~tracks.loc[tracks[\"audio_feature_id\"].notna(), \"audio_feature_id\"].isin(audio[\"id\"])).sum()\n",
    "    report_row(\"tracks\", \"fk(audio_feature_id->audio_features.id)\", \"ok\" if missing == 0 else \"warn\", missing, total)\n",
    "else:\n",
    "    report_row(\"tracks\", \"fk(audio_feature_id->audio_features.id)\", \"skip\", None, None)\n",
    "\n",
    "\n",
    "fk_check(rag, \"artist_id\", artists, key_col_for(\"artists\"), \"r_artist_genre:artists\")\n",
    "\n",
    "fk_check(rag, \"genre_id\", genres, key_col_for(\"genres\"), \"r_artist_genre:genres\")\n",
    "\n",
    "\n",
    "# 6) Ausgabe\n",
    "integ = pd.DataFrame(rows).sort_values([\"table\",\"check\"]).reset_index(drop=True)\n",
    "display(integ)\n",
    "\n",
    "# Save\n",
    "out_csv = INTEG_DIR / \"integrity_report.csv\"\n",
    "integ.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\" Integrity-Report gespeichert unter:\", out_csv.resolve())\n"
   ],
   "id": "1f2b731816da588e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.8 Feature-Verteilungen nach Kategorien (Genre/Albumtyp)\n",
    "\n",
    "**Ziel**\n",
    "Ich vergleiche Audio-Merkmale (z. B. `energy`, `valence`, `tempo`) über **Kategorien** hinweg:\n",
    "- Standard: `album_type` (z. B. album/single/compilation)\n",
    "- Optional: `genre` (falls `genres` + `r_artist_genre` vorliegen)\n"
   ],
   "id": "1d620bf24e988fe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CAT_DIR = Path(SCHEMA_REPORTS_DIR) / \"categories\"\n",
    "CAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Basisdaten\n",
    "tracks = pd.read_csv(DATA_DIR / \"tracks.csv\")\n",
    "audio = pd.read_csv(DATA_DIR / \"audio_features.csv\")\n",
    "albums = pd.read_csv(DATA_DIR / \"albums.csv\")\n",
    "\n",
    "# Merge: tracks + audio + albums (für album_type + release_date)\n",
    "tracks = tracks.rename(columns={\"id\": \"track_id\"})\n",
    "audio = audio.rename(columns={\"id\": \"audio_feature_id\"})\n",
    "df = tracks.merge(audio, left_on=\"audio_feature_id\", right_on=\"audio_feature_id\", how=\"left\")\n",
    "df = df.merge(albums[[\"id\", \"album_type\", \"release_date\"]].rename(columns={\"id\": \"album_id\"}), how=\"left\",\n",
    "              left_on=\"track_id\", right_on=\"album_id\")\n",
    "\n",
    "# Saubere Kategorien\n",
    "cat_col = \"album_type\"\n",
    "num_cols = [c for c in [\"danceability\", \"energy\", \"valence\", \"loudness\", \"tempo\"] if c in df.columns]\n",
    "\n",
    "for y in num_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=df, x=cat_col, y=y)\n",
    "    plt.title(f\"{y} nach {cat_col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CAT_DIR / f\"box_{y}_by_{cat_col}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Gruppierte Statistiken\n",
    "group_stats = (\n",
    "    df.groupby(cat_col)[num_cols]\n",
    "    .agg([\"mean\", \"std\", \"median\", \"count\"])\n",
    "    .round(2)\n",
    ")\n",
    "display(group_stats.head(10))\n",
    "group_stats.to_csv(CAT_DIR / f\"stats_{cat_col}.csv\", encoding=\"utf-8\")\n",
    "print(f\"Kategorien-Auswertung gespeichert unter: {CAT_DIR.resolve()}\")\n",
    "\n",
    "# Optional: nach Genre (nur wenn verfügbar)\n",
    "genres_path = DATA_DIR / \"genres.csv\"\n",
    "rag_path = DATA_DIR / \"r_artist_genre.csv\"\n",
    "rta_path = DATA_DIR / \"r_track_artist.csv\"\n",
    "artists_path = DATA_DIR / \"artists.csv\"\n",
    "\n",
    "if genres_path.exists() and rag_path.exists() and rta_path.exists() and artists_path.exists():\n",
    "    genres = pd.read_csv(genres_path)\n",
    "    rag = pd.read_csv(rag_path)\n",
    "    rta = pd.read_csv(rta_path)\n",
    "    artists = pd.read_csv(artists_path)\n",
    "    # Map track -> artist -> genre (First genre per artist)\n",
    "    first_genre = rag.merge(genres, left_on=\"genre_id\", right_on=\"id\", how=\"left\").dropna(subset=[\"id\"])\n",
    "    first_genre = first_genre.drop_duplicates(subset=[\"artist_id\"]).rename(columns={\"id\": \"genre_name\"})\n",
    "    track_genre = rta.merge(first_genre[[\"artist_id\", \"genre_name\"]], on=\"artist_id\", how=\"left\")\n",
    "    df_g = tracks.merge(track_genre[[\"track_id\", \"genre_name\"]], on=\"track_id\", how=\"left\").merge(audio,\n",
    "                                                                                                  on=\"audio_feature_id\",\n",
    "                                                                                                  how=\"left\")\n",
    "    # Top 15 Genres\n",
    "    top_genres = df_g[\"genre_name\"].value_counts().head(15).index\n",
    "    df_g = df_g[df_g[\"genre_name\"].isin(top_genres)]\n",
    "    for y in num_cols:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(data=df_g, x=\"genre_name\", y=y)\n",
    "        plt.title(f\"{y} nach Genre (Top 15)\")\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(CAT_DIR / f\"box_{y}_by_genre_top15.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n"
   ],
   "id": "9ae210ba5d21067",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.9 Zeitliche Trends (Release-Jahre)\n",
    "\n",
    "**Ziel**\n",
    "Ich analysiere die Entwicklung musikalischer Merkmale über die Zeit (pro Jahr):\n",
    "- Jahresdurchschnitt von `tempo`, `energy`, `valence`, `loudness`\n",
    "- Gleitender Durchschnitt (k=3) zur Glättung\n",
    "\n",
    "**Ergebnis**\n",
    "- `yearly_trends.csv`\n",
    "- Trendplots als PNG\n"
   ],
   "id": "62fb12f65cf91de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "TREND_DIR = Path(SCHEMA_REPORTS_DIR) / \"trends\"\n",
    "TREND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load\n",
    "tracks = pd.read_csv(DATA_DIR / \"tracks.csv\").rename(columns={\"id\": \"track_id\"})\n",
    "audio  = pd.read_csv(DATA_DIR / \"audio_features.csv\").rename(columns={\"id\": \"audio_feature_id\"})\n",
    "albums = pd.read_csv(DATA_DIR / \"albums.csv\")[[\"id\", \"release_date\"]].rename(columns={\"id\": \"album_id\"})\n",
    "\n",
    "# Optional relation (best way to map tracks -> albums)\n",
    "rat_path = DATA_DIR / \"r_albums_tracks.csv\"\n",
    "r_at = pd.read_csv(rat_path) if rat_path.exists() else None\n",
    "\n",
    "# Base merge (tracks + audio)\n",
    "df = tracks.merge(audio, on=\"audio_feature_id\", how=\"left\")\n",
    "\n",
    "# Attach album_id properly\n",
    "if r_at is not None:\n",
    "    df = df.merge(r_at[[\"track_id\", \"album_id\"]], on=\"track_id\", how=\"left\")\n",
    "elif \"album_id\" in tracks.columns:\n",
    "    df = df.merge(tracks[[\"track_id\", \"album_id\"]], on=\"track_id\", how=\"left\")\n",
    "else:\n",
    "    df[\"album_id\"] = pd.NA\n",
    "\n",
    "# Join albums to get release_date\n",
    "df = df.merge(albums, on=\"album_id\", how=\"left\")\n",
    "\n",
    "# Parse release_date: it's ms timestamps (per your sample)\n",
    "# (works whether dtype is str, int, or float)\n",
    "s = pd.to_numeric(df[\"release_date\"], errors=\"coerce\")\n",
    "df[\"release_dt\"] = pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "df[\"year\"] = df[\"release_dt\"].dt.year\n",
    "\n",
    "# Features & yearly aggregation\n",
    "feat_cols = [c for c in [\"tempo\", \"energy\", \"valence\", \"loudness\", \"danceability\"] if c in df.columns]\n",
    "yearly = (\n",
    "    df.dropna(subset=[\"year\"])\n",
    "      .groupby(\"year\")[feat_cols]\n",
    "      .mean()\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "# Add counts to judge stability; optionally filter sparse years\n",
    "yearly[\"count\"] = df.groupby(\"year\")[\"track_id\"].count()\n",
    "MIN_YEAR_COUNT = 50\n",
    "yearly_filtered = yearly[yearly[\"count\"] >= MIN_YEAR_COUNT]\n",
    "\n",
    "# Save table\n",
    "yearly_filtered.to_csv(TREND_DIR / \"yearly_trends.csv\", encoding=\"utf-8\", index=True)\n",
    "\n",
    "# Plots: per-feature trend + 3Y rolling mean (dashed)\n",
    "for c in feat_cols:\n",
    "    s = yearly_filtered[c].dropna()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x=s.index, y=s.values, marker=\"o\")\n",
    "    if len(s) >= 3:\n",
    "        s_roll = s.rolling(3, min_periods=1).mean()\n",
    "        sns.lineplot(x=s_roll.index, y=s_roll.values, linestyle=\"--\")\n",
    "    plt.title(f\"Trend über Jahre: {c} (Ø, 3Y-Rolling gestrichelt)\")\n",
    "    plt.xlabel(\"Jahr\"); plt.ylabel(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TREND_DIR / f\"trend_{c}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\" Trend-Reports gespeichert unter: {TREND_DIR.resolve()}\")\n",
    "\n"
   ],
   "id": "74d9ab2d434d7081",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.10 Cross-Table Relationship: Artist Influence\n",
    "\n",
    "**Ziel**\n",
    "Ich untersuche, ob **Künstler-Reichweite** (Followers) mit **Erfolg ihrer Songs** (Ø-Track-Popularität) zusammenhängt.\n",
    "\n",
    "**Ergebnis**\n",
    "- Scatter + Regressionslinie (log-Followers)\n",
    "- `artist_influence.csv`\n"
   ],
   "id": "273c9f2620b949a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "INF_DIR = Path(SCHEMA_REPORTS_DIR) / \"influence\"\n",
    "INF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Helpers to resolve column names across schema variants\n",
    "def pick(dframe, candidates, required=True):\n",
    "    for c in candidates:\n",
    "        if c in dframe.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of {candidates} found in columns: {list(dframe.columns)}\")\n",
    "    return None\n",
    "\n",
    "# --- Load CSVs\n",
    "tracks = pd.read_csv(DATA_DIR / \"tracks.csv\", encoding=\"utf-8\")  # ensure headers load\n",
    "artists = pd.read_csv(DATA_DIR / \"artists.csv\", encoding=\"utf-8\")\n",
    "rta = pd.read_csv(DATA_DIR / \"r_track_artist.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# --- Resolve column names\n",
    "track_id_col = pick(tracks, [\"track_id\", \"id\"])\n",
    "track_pop_col = pick(tracks, [\"popularity\"])\n",
    "\n",
    "artist_id_col = pick(artists, [\"artist_id\", \"id\"])\n",
    "followers_col  = pick(artists, [\"followers\", \"followers_count\"])\n",
    "artist_pop_col = pick(artists, [\"artist_popularity\", \"popularity\"], required=False)  # optional\n",
    "\n",
    "rta_track_col  = pick(rta, [\"track_id\", \"trackId\", \"trackID\"])\n",
    "rta_artist_col = pick(rta, [\"artist_id\", \"artistId\", \"artistID\"])\n",
    "\n",
    "# --- Minimal frames with normalized names\n",
    "tracks_min = tracks[[track_id_col, track_pop_col]].rename(\n",
    "    columns={track_id_col: \"track_id\", track_pop_col: \"track_popularity\"}\n",
    ")\n",
    "\n",
    "artists_min_cols = [artist_id_col, followers_col]\n",
    "rename_map = {artist_id_col: \"artist_id\", followers_col: \"followers\"}\n",
    "if artist_pop_col:\n",
    "    artists_min_cols.append(artist_pop_col)\n",
    "    rename_map[artist_pop_col] = \"artist_popularity\"\n",
    "\n",
    "artists_min = artists[artists_min_cols].rename(columns=rename_map)\n",
    "\n",
    "rta_min = rta[[rta_track_col, rta_artist_col]].rename(\n",
    "    columns={rta_track_col: \"track_id\", rta_artist_col: \"artist_id\"}\n",
    ")\n",
    "\n",
    "# --- Avg track popularity per artist\n",
    "track_pop_by_artist = (\n",
    "    rta_min.merge(tracks_min, on=\"track_id\", how=\"left\")\n",
    "           .groupby(\"artist_id\", as_index=False)[\"track_popularity\"]\n",
    "           .mean()\n",
    "           .rename(columns={\"track_popularity\": \"avg_track_popularity\"})\n",
    ")\n",
    "\n",
    "# --- Combine with artist followers (and optional artist_popularity)\n",
    "df = artists_min.merge(track_pop_by_artist, on=\"artist_id\", how=\"left\")\n",
    "df = df.dropna(subset=[\"avg_track_popularity\"])\n",
    "\n",
    "# Followers cleaning: ensure non-negative numbers\n",
    "df[\"followers\"] = pd.to_numeric(df[\"followers\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "# Log-transform followers (log1p handles zeros)\n",
    "df[\"followers_log1p\"] = np.log1p(df[\"followers\"])\n",
    "\n",
    "# --- Plot: Scatter + regression line\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.regplot(\n",
    "    data=df,\n",
    "    x=\"followers_log1p\",\n",
    "    y=\"avg_track_popularity\",\n",
    "    scatter_kws={\"alpha\": 0.4, \"s\": 18},\n",
    "    line_kws={\"linewidth\": 2}\n",
    ")\n",
    "plt.title(\"Artist-Followers (log) vs. Ø Track-Popularität\")\n",
    "plt.xlabel(\"Followers (log1p)\")\n",
    "plt.ylabel(\"Ø Track-Popularität\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(INF_DIR / \"artist_followers_vs_avg_track_popularity.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# --- Save CSV\n",
    "cols_out = [\"artist_id\", \"followers\", \"avg_track_popularity\", \"followers_log1p\"]\n",
    "if \"artist_popularity\" in df.columns:\n",
    "    cols_out.insert(2, \"artist_popularity\")\n",
    "\n",
    "df[cols_out].to_csv(INF_DIR / \"artist_influence.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\" Artist-Influence gespeichert unter: {INF_DIR.resolve()}\")\n",
    "print(\"Columns used:\",\n",
    "      {\"tracks\": list(tracks_min.columns),\n",
    "       \"artists\": list(artists_min.columns),\n",
    "       \"r_track_artist\": list(rta_min.columns)})\n",
    "\n"
   ],
   "id": "b62ec071da64905e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
