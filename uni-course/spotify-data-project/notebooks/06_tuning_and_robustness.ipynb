{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04 — Hypertuning (Baseline laden → Tuning → Hypertuned speichern)\n",
    "\n",
    "Dieses Notebook:\n",
    "1) lädt die Baseline-Modelle aus `models/baseline/`\n",
    "2) lädt die Modell-Datasets (Features/Targets)\n",
    "3) evaluiert Baseline-Modelle als Referenz\n",
    "4) führt Hypertuning mit RandomizedSearchCV durch\n",
    "5) evaluiert die besten Modelle auf dem Testset\n",
    "6) speichert die hypertuned Modelle nach `models/hypertuned/`\n",
    "7) schreibt Reports (`json`) für spätere Notebooks\n"
   ],
   "id": "ef817af335343aad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "b22586766eee8ccc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:12.983569Z",
     "start_time": "2026-01-04T11:47:12.965095Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import load, dump\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"default\")\n"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konfiguration",
   "id": "c25f913ba933ccdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:13.066627Z",
     "start_time": "2026-01-04T11:47:13.046720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = globals().get(\"RANDOM_SEED\", 42)\n",
    "\n",
    "BASE_EXPORT_DIR = Path(\"../data/interim/converted_sqlite_samples\")\n",
    "CURRENT_SAMPLE_PATH = BASE_EXPORT_DIR / \"current_sample.json\"\n",
    "cfg = json.loads(CURRENT_SAMPLE_PATH.read_text())\n",
    "SAMPLE_NAME = cfg[\"SAMPLE_NAME\"]\n",
    "\n",
    "# Ordnerstruktur (wie du es beschrieben hast)\n",
    "MODELS_BASELINE_DIR = Path(\"../data/models/baseline\") / SAMPLE_NAME\n",
    "MODELS_HYPER_DIR = Path(\"../data/models/hypertuned\") / SAMPLE_NAME\n",
    "MODELS_HYPER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reports\n",
    "REPORT_PATH = Path(\"../data/reports/04_hypertuning\") / SAMPLE_NAME\n",
    "BEST_PARAMS_PATH = Path(\"../data/reports/04_hypertuning\") / SAMPLE_NAME\n",
    "\n",
    "REPORT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PARAMS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "TUNE_ITER_REG = 18\n",
    "TUNE_ITER_CLS = 22\n",
    "CV_FOLDS = 3\n",
    "\n",
    "\n",
    "# Ausgabe-Container\n",
    "report: Dict[str, Any] = {}\n",
    "best_params: Dict[str, Any] = {}\n"
   ],
   "id": "256001d23af8e629",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "eb274d63d8a17e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:13.109822Z",
     "start_time": "2026-01-04T11:47:13.082042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _identity(X):\n",
    "    return X\n",
    "\n",
    "def build_preprocessor_tree(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für XGBoost/Tree-Modelle (ohne Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def build_preprocessor_linear(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für lineare Modelle (mit Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def regression_report(y_true, y_pred) -> Dict[str, float]:\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def classification_report_binary(y_true, proba, threshold=0.5) -> Dict[str, Any]:\n",
    "    pred = (proba >= threshold).astype(int)\n",
    "    roc = float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    pr = float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    f1 = float(f1_score(y_true, pred))\n",
    "    cm = confusion_matrix(y_true, pred).tolist()\n",
    "    return {\"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"confusion_matrix\": cm}\n",
    "\n",
    "def best_f1_threshold(y_true, proba, thresholds=np.linspace(0.05, 0.95, 19)):\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in thresholds:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "\n",
    "from utils.helper_functions import  sklearn_sanitize_df\n",
    "\n",
    "\n",
    "sanitize_tf = FunctionTransformer(sklearn_sanitize_df, feature_names_out=\"one-to-one\")"
   ],
   "id": "1aacd242493bd3d4",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten Laden",
   "id": "1adca59ee8b67d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:15.095846Z",
     "start_time": "2026-01-04T11:47:13.205639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = Path(\"../data/datasets\") / SAMPLE_NAME\n",
    "\n",
    "def _load_parquet(p: Path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {p}\")\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "files = {\n",
    "    \"X_track_pop\": DATA_DIR / \"X_track_pop.parquet\",\n",
    "    \"y_track_pop\": DATA_DIR / \"y_track_pop.parquet\",\n",
    "    \"X_album_pop\": DATA_DIR / \"X_album_pop.parquet\",\n",
    "    \"y_album_pop\": DATA_DIR / \"y_album_pop.parquet\",\n",
    "    \"X_track_hit\": DATA_DIR / \"X_track_hit.parquet\",\n",
    "    \"y_hit\": DATA_DIR / \"y_hit.parquet\",\n",
    "    \"X_track_explicit\": DATA_DIR / \"X_track_explicit.parquet\",\n",
    "    \"y_explicit\": DATA_DIR / \"y_explicit.parquet\",\n",
    "    \"X_track_mood\": DATA_DIR / \"X_track_mood.parquet\",\n",
    "    \"Y_mood\": DATA_DIR / \"Y_mood.parquet\",\n",
    "}\n",
    "\n",
    "X_track_pop = _load_parquet(files[\"X_track_pop\"])\n",
    "y_track_pop = _load_parquet(files[\"y_track_pop\"]).squeeze()\n",
    "\n",
    "X_album_pop = _load_parquet(files[\"X_album_pop\"])\n",
    "y_album_pop = _load_parquet(files[\"y_album_pop\"]).squeeze()\n",
    "\n",
    "X_track_hit = _load_parquet(files[\"X_track_hit\"])\n",
    "y_hit = _load_parquet(files[\"y_hit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_explicit = _load_parquet(files[\"X_track_explicit\"])\n",
    "y_explicit = _load_parquet(files[\"y_explicit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_mood = _load_parquet(files[\"X_track_mood\"])\n",
    "y_mood = _load_parquet(files[\"Y_mood\"])"
   ],
   "id": "fe46e569ed6f7e83",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:15.117080Z",
     "start_time": "2026-01-04T11:47:15.110280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X_track_pop:\", X_track_mood.shape)\n",
    "print(\"y_track_pop:\", y_mood.shape)\n",
    "print(\"X index unique:\", X_track_pop.index.is_unique)\n",
    "print(\"y index unique:\", y_track_pop.index.is_unique)\n",
    "print(\"Index equal:\", X_track_pop.index.equals(y_track_pop.index))\n"
   ],
   "id": "7c9e4a565e2e9e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_track_pop: (299981, 70)\n",
      "y_track_pop: (299981, 7)\n",
      "X index unique: True\n",
      "y index unique: True\n",
      "Index equal: True\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline-Modelle laden",
   "id": "92edf052f9efe44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:16.989937Z",
     "start_time": "2026-01-04T11:47:15.135224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_paths = {\n",
    "    \"track_popularity\": MODELS_BASELINE_DIR / \"03_track_popularity_pipeline_xgb.joblib\",\n",
    "    \"album_popularity\": MODELS_BASELINE_DIR / \"03_album_popularity_pipeline_xgb.joblib\",\n",
    "    \"hit\": MODELS_BASELINE_DIR / \"03_hit_pipeline_xgb.joblib\",\n",
    "    \"explicit\": MODELS_BASELINE_DIR / \"03_explicit_pipeline_xgb.joblib\",\n",
    "    \"mood\": MODELS_BASELINE_DIR / \"03_mood_pipeline.joblib\",\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "for k, p in baseline_paths.items():\n",
    "    if p.exists():\n",
    "        baseline_models[k] = load(p)\n",
    "    else:\n",
    "        print(f\"Baseline nicht gefunden (übersprungen): {p}\")\n",
    "\n",
    "list(baseline_models.keys())\n"
   ],
   "id": "2666e1a630658fe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_popularity', 'album_popularity', 'hit', 'explicit', 'mood']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gemeinsame Splits (Baseline & Hypertuned müssen gleich evaluieren)",
   "id": "d1b7190b41b089a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:22.283400Z",
     "start_time": "2026-01-04T11:47:17.034839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = {}\n",
    "\n",
    "# Regression Splits (kein stratify)\n",
    "splits[\"track_pop\"] = train_test_split(\n",
    "    X_track_pop, y_track_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "splits[\"album_pop\"] = train_test_split(\n",
    "    X_album_pop, y_album_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Classification Splits (stratifiziert)\n",
    "splits[\"hit\"] = train_test_split(\n",
    "    X_track_hit, y_hit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_hit\n",
    ")\n",
    "splits[\"explicit\"] = train_test_split(\n",
    "    X_track_explicit, y_explicit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_explicit\n",
    ")\n",
    "\n",
    "# Mood optional\n",
    "splits[\"mood\"] = train_test_split(\n",
    "        X_track_mood, y_mood, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n"
   ],
   "id": "71799f0884756e07",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tuning-Funktion",
   "id": "133f49c1885a134c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:47:22.468578Z",
     "start_time": "2026-01-04T11:47:22.344940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class TuneCFG:\n",
    "    random_seed: int = 42\n",
    "    valid_size: float = 0.15\n",
    "    early_stopping_rounds: int = 50\n",
    "    n_iter_reg: int = 18\n",
    "    n_iter_cls: int = 18\n",
    "    n_estimators_reg: int = 6000\n",
    "    n_estimators_cls: int = 8000\n",
    "    prefer_gpu: bool = True\n",
    "    # start stable: single GPU + ES => keep parallel low\n",
    "    n_jobs_search: int = 1\n",
    "\n",
    "\n",
    "CFG = TuneCFG(\n",
    "    random_seed=globals().get(\"RANDOM_SEED\", 42),\n",
    "    n_iter_reg=globals().get(\"TUNE_ITER_REG\", 18),\n",
    "    n_iter_cls=globals().get(\"TUNE_ITER_CLS\", 18),\n",
    ")\n",
    "\n",
    "\n",
    "def _detect_xgb_device(prefer_gpu=True) -> str:\n",
    "    if not prefer_gpu:\n",
    "        return \"cpu\"\n",
    "    X = np.random.rand(4000, 20).astype(np.float32)\n",
    "    y = np.random.rand(4000).astype(np.float32)\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        try:\n",
    "            m = xgb.XGBRegressor(n_estimators=30, tree_method=\"hist\", device=\"cuda\", n_jobs=1)\n",
    "            m.fit(X, y, verbose=False)\n",
    "        except Exception:\n",
    "            return \"cpu\"\n",
    "        msgs = \"\\n\".join(str(x.message) for x in w)\n",
    "        if \"No visible GPU is found\" in msgs or \"Device is changed from GPU to CPU\" in msgs:\n",
    "            return \"cpu\"\n",
    "    return \"cuda\"\n",
    "\n",
    "\n",
    "def _make_ps(n_train: int, n_val: int) -> PredefinedSplit:\n",
    "    return PredefinedSplit(test_fold=np.array([-1] * n_train + [0] * n_val, dtype=int))\n",
    "\n",
    "\n",
    "def _stack(a, b):\n",
    "    # works for numpy arrays and scipy sparse\n",
    "    try:\n",
    "        import scipy.sparse as sp\n",
    "        if sp.issparse(a) or sp.issparse(b):\n",
    "            return sp.vstack([a, b])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.vstack([a, b])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Wrapper so your downstream code still works: best_model.predict(X_df)\n",
    "# -----------------------------\n",
    "class PreprocessThenModel:\n",
    "    def __init__(self, preprocess_pipe: Pipeline, model):\n",
    "        self.preprocess_pipe = preprocess_pipe\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xt = self.preprocess_pipe.transform(X)\n",
    "        return self.model.predict(Xt)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xt = self.preprocess_pipe.transform(X)\n",
    "        return self.model.predict_proba(Xt)\n",
    "\n",
    "    # so joblib dump works nicely\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"preprocess_pipe\": self.preprocess_pipe, \"model\": self.model}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DROP-IN API (same call style)\n",
    "# -----------------------------\n",
    "def tune_xgb_regression(Xtr, ytr, preprocessor):\n",
    "    device = _detect_xgb_device(CFG.prefer_gpu)\n",
    "    print(f\"[tune_xgb_regression FIXED] xgboost={xgb.__version__} | device={device} | n_iter={CFG.n_iter_reg}\")\n",
    "\n",
    "    # split once (production-style)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        Xtr, ytr, test_size=CFG.valid_size, random_state=CFG.random_seed\n",
    "    )\n",
    "\n",
    "    # fit preprocessing ONLY on train, then transform train+val\n",
    "    preprocess_pipe = Pipeline([(\"sanitize\", sanitize_tf), (\"pre\", preprocessor)])\n",
    "    preprocess_pipe.fit(X_train, y_train)\n",
    "\n",
    "    Xt_train = preprocess_pipe.transform(X_train)\n",
    "    Xt_val = preprocess_pipe.transform(X_val)\n",
    "\n",
    "    X_all = _stack(Xt_train, Xt_val)\n",
    "    y_all = np.concatenate([np.asarray(y_train), np.asarray(y_val)], axis=0)\n",
    "    ps = _make_ps(Xt_train.shape[0], Xt_val.shape[0])\n",
    "\n",
    "    base = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        device=device,\n",
    "        random_state=CFG.random_seed,\n",
    "        n_jobs=1,\n",
    "        n_estimators=CFG.n_estimators_reg,\n",
    "        eval_metric=\"mae\",\n",
    "        early_stopping_rounds=CFG.early_stopping_rounds,  # XGB 3.1.x: set here\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        \"learning_rate\": loguniform(0.03, 0.15),\n",
    "        \"max_depth\": randint(3, 8),\n",
    "        \"min_child_weight\": loguniform(1.0, 20.0),\n",
    "        \"subsample\": uniform(0.75, 0.25),\n",
    "        \"colsample_bytree\": uniform(0.75, 0.25),\n",
    "        \"gamma\": loguniform(1e-8, 2.0),\n",
    "        \"reg_lambda\": loguniform(1e-2, 50.0),\n",
    "        \"reg_alpha\": loguniform(1e-8, 5.0),\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=min(CFG.n_iter_reg, 40),\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=ps,\n",
    "        random_state=CFG.random_seed,\n",
    "        n_jobs=CFG.n_jobs_search,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "        refit=False,  # we will refit manually with proper ES\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: eval_set is ALREADY transformed numeric matrix -> no string dtypes possible\n",
    "    search.fit(X_all, y_all, eval_set=[(Xt_val, np.asarray(y_val))], verbose=False)\n",
    "\n",
    "    # refit final model with best params on transformed train, ES on transformed val\n",
    "    best = XGBRegressor(**base.get_params())\n",
    "    best.set_params(**search.best_params_)\n",
    "    best.fit(Xt_train, np.asarray(y_train), eval_set=[(Xt_val, np.asarray(y_val))], verbose=False)\n",
    "\n",
    "    # return a “production-like” object with predict on raw DF\n",
    "    search.best_estimator_ = PreprocessThenModel(preprocess_pipe, best)\n",
    "    return search\n",
    "\n",
    "\n",
    "def tune_xgb_classification(Xtr, ytr, preprocessor, scale_pos_weight: float):\n",
    "    device = _detect_xgb_device(CFG.prefer_gpu)\n",
    "    print(f\"[tune_xgb_classification FIXED] xgboost={xgb.__version__} | device={device} | n_iter={CFG.n_iter_cls}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        Xtr, ytr, test_size=CFG.valid_size, random_state=CFG.random_seed, stratify=ytr\n",
    "    )\n",
    "\n",
    "    preprocess_pipe = Pipeline([(\"sanitize\", sanitize_tf), (\"pre\", preprocessor)])\n",
    "    preprocess_pipe.fit(X_train, y_train)\n",
    "\n",
    "    Xt_train = preprocess_pipe.transform(X_train)\n",
    "    Xt_val = preprocess_pipe.transform(X_val)\n",
    "\n",
    "    X_all = _stack(Xt_train, Xt_val)\n",
    "    y_all = np.concatenate([np.asarray(y_train), np.asarray(y_val)], axis=0)\n",
    "    ps = _make_ps(Xt_train.shape[0], Xt_val.shape[0])\n",
    "\n",
    "    base = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        tree_method=\"hist\",\n",
    "        device=device,\n",
    "        random_state=CFG.random_seed,\n",
    "        n_jobs=1,\n",
    "        n_estimators=CFG.n_estimators_cls,\n",
    "        eval_metric=\"aucpr\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        early_stopping_rounds=CFG.early_stopping_rounds,\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        \"learning_rate\": loguniform(0.03, 0.15),\n",
    "        \"max_depth\": randint(3, 7),\n",
    "        \"min_child_weight\": loguniform(1.0, 20.0),\n",
    "        \"subsample\": uniform(0.75, 0.25),\n",
    "        \"colsample_bytree\": uniform(0.75, 0.25),\n",
    "        \"gamma\": loguniform(1e-8, 2.0),\n",
    "        \"reg_lambda\": loguniform(1e-2, 50.0),\n",
    "        \"reg_alpha\": loguniform(1e-8, 5.0),\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=min(CFG.n_iter_cls, 40),\n",
    "        scoring=\"average_precision\",\n",
    "        cv=ps,\n",
    "        random_state=CFG.random_seed,\n",
    "        n_jobs=CFG.n_jobs_search,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "        refit=False,\n",
    "    )\n",
    "\n",
    "    search.fit(X_all, y_all, eval_set=[(Xt_val, np.asarray(y_val))], verbose=False)\n",
    "\n",
    "    best = XGBClassifier(**base.get_params())\n",
    "    best.set_params(**search.best_params_)\n",
    "    best.fit(Xt_train, np.asarray(y_train), eval_set=[(Xt_val, np.asarray(y_val))], verbose=False)\n",
    "\n",
    "    search.best_estimator_ = PreprocessThenModel(preprocess_pipe, best)\n",
    "    return search\n"
   ],
   "id": "8f9cc8a4ef38468",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Track Popularity Tuning",
   "id": "bcfdbd56221c220a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:50:43.707736Z",
     "start_time": "2026-01-04T11:47:22.538179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"track_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(Xtr)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"track_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"track_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_track_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"track_popularity\"]"
   ],
   "id": "fdf3cb7380a9052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tune_xgb_regression FIXED] xgboost=3.1.2 | device=cuda | n_iter=18\n",
      "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'MAE': 2.77017913132906,\n",
       "  'RMSE': 5.191235544050068,\n",
       "  'R2': 0.7617520606845933},\n",
       " 'cv_best_mae': 2.7912476561665533}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Album Popularity Tuning",
   "id": "5f7d9f3811ae8f85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:50:59.778962Z",
     "start_time": "2026-01-04T11:50:43.760952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"album_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(Xtr)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"album_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"album_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_album_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"album_popularity\"]\n"
   ],
   "id": "95e07df45a9f421f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tune_xgb_regression FIXED] xgboost=3.1.2 | device=cuda | n_iter=18\n",
      "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hit Tuning + Threshhold\n",
   "id": "1e036896b3a25ca4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"hit\"]\n",
    "pre, _, _ = build_preprocessor_tree(Xtr)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"hit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"hit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_hit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"hit_prediction\"]\n"
   ],
   "id": "67fda4b58ececf58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Explicit Tuning + Threshold",
   "id": "e68005067d391d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"explicit\"]\n",
    "pre, _, _ = build_preprocessor_tree(Xtr)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"explicit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"explicit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_explicit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"explicit_prediction\"]\n"
   ],
   "id": "5b3c93a70e7af2e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mood Tuning",
   "id": "6e229830d3bfb21c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tune_logreg_multilabel_mood(Xtr, Ytr, preprocessor):\n",
    "    # Cache preprocessing across CV folds & candidates\n",
    "    cache_dir = os.path.join(tempfile.gettempdir(), \"sklearn_linear_cache\")\n",
    "\n",
    "\n",
    "    base = OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            max_iter=300,           # will be controlled by halving (resource)\n",
    "            tol=1e-3,               # practical default; can be tuned\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=1                # keep single-thread; CV parallel outside\n",
    "        ),\n",
    "        n_jobs=1                   # avoid oversubscription with CV n_jobs=-1\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"sanitize\", sanitize_tf), (\"pre\", preprocessor), (\"model\", base)]\n",
    "    )\n",
    "\n",
    "    # Conditional distributions: l1_ratio only when elasticnet\n",
    "    param_dist = [\n",
    "        {\n",
    "            \"model__estimator__penalty\": [\"l2\"],\n",
    "            \"model__estimator__C\": loguniform(1e-2, 20.0),\n",
    "            \"model__estimator__tol\": loguniform(1e-4, 1e-2),\n",
    "        },\n",
    "        {\n",
    "            \"model__estimator__penalty\": [\"elasticnet\"],\n",
    "            \"model__estimator__C\": loguniform(1e-2, 20.0),\n",
    "            \"model__estimator__l1_ratio\": uniform(0.0, 1.0),\n",
    "            \"model__estimator__tol\": loguniform(1e-4, 1e-2),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Fast path: HalvingRandomSearchCV (massive speedup vs full 2000 iters always)\n",
    "    try:\n",
    "        from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "        from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "        search = HalvingRandomSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            scoring=\"f1_micro\",\n",
    "            cv=CV_FOLDS,                 # use 3 if you can, 2 if speed is critical\n",
    "            n_candidates=max(12, TUNE_ITER_CLS),\n",
    "            factor=3,\n",
    "            resource=\"model__estimator__max_iter\",\n",
    "            min_resources=100,\n",
    "            max_resources=2000,\n",
    "            aggressive_elimination=True,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score=\"raise\"\n",
    "        )\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback: faster randomized search with capped max_iter range\n",
    "        param_dist_fallback = [\n",
    "            {\n",
    "                **d,\n",
    "                \"model__estimator__max_iter\": [200, 400, 800, 1200, 2000],\n",
    "            }\n",
    "            for d in param_dist\n",
    "        ]\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_dist_fallback,\n",
    "            n_iter=12,\n",
    "            scoring=\"f1_micro\",\n",
    "            cv=2,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            error_score=\"raise\"\n",
    "        )\n",
    "\n",
    "    search.fit(Xtr, Ytr)\n",
    "    return search\n"
   ],
   "id": "cc6986cb141b726b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Xtr, Xte, Ytr, Yte = splits[\"mood\"]\n",
    "pre, _, _ = build_preprocessor_linear(Xtr)\n",
    "search = tune_logreg_multilabel_mood(Xtr, Ytr, pre)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "Ypred = best_model.predict(Xte)\n",
    "\n",
    "micro_f1 = float(f1_score(Yte, Ypred, average=\"micro\"))\n",
    "macro_f1 = float(f1_score(Yte, Ypred, average=\"macro\"))\n",
    "\n",
    "report[\"mood_multilabel\"] = {\n",
    "    \"hypertuned\": {\"micro_f1\": micro_f1, \"macro_f1\": macro_f1},\n",
    "    \"cv_best_f1_micro\": float(search.best_score_)\n",
    "}\n",
    "best_params[\"mood_multilabel\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_mood_multilabel_logreg_hypertuned.joblib\")\n"
   ],
   "id": "bbbfd4813f088078",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reports Saving",
   "id": "64c200f35d048540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write hypertuning report\n",
    "with open(REPORT_PATH / \"04_hypertuning_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Write best parameters\n",
    "with open(BEST_PARAMS_PATH / \"04_hypertuning_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(\"Hypertuning fertig. Gespeichert unter:\")\n",
    "print(\" -\", MODELS_HYPER_DIR)\n",
    "print(\" -\", REPORT_PATH)\n",
    "print(\" -\", BEST_PARAMS_PATH)\n"
   ],
   "id": "1f02a85343f7a96d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
