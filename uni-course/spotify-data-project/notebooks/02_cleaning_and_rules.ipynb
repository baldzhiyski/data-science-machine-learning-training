{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02 — Cleaning & Rules\n",
    "\n",
    "## Zweck\n",
    "Dieses Notebook bereinigt den exportierten **verbundenen Subgraphen** (bis zu **300k Tracks**), erzwingt einen **stabilen Datenvertrag (Data Contract)** und erzeugt eine **ML-fertige Clean-Layer**.\n",
    "\n",
    "## Input\n",
    "- `../data/interim/converted_sqlite_samples/<sample_name>/*.csv`\n",
    "\n",
    "## Output\n",
    "- `../data/processed/<sample_name>/clean_csv/*.csv`\n",
    "- `../data/processed/<sample_name>/parquet/*.parquet`\n",
    "- `../data/reports/<sample_name>/cleaning_and_rules/cleaning_report.json`\n",
    "\n",
    "## Erwartete Tabellen (aus dem Exporter)\n",
    "| Tabelle | Beschreibung | Schlüssel / Beziehung |\n",
    "|---|---|---|\n",
    "| `tracks` | Track-Stammdaten | PK: `track_id` |\n",
    "| `audio_features` | Audio-Features pro Track | PK: `id` |\n",
    "| `albums` | Album-Stammdaten | PK: `id` |\n",
    "| `artists` | Artist-Stammdaten | PK: `id` |\n",
    "| `genres` | Genre-Stammdaten | PK: `id` |\n",
    "| `r_albums_tracks` | Zuordnung Album ↔ Track | (`album_id`, `track_id`) |\n",
    "| `r_track_artist` | Zuordnung Track ↔ Artist | (`track_id`, `artist_id`) |\n",
    "| `r_artist_genre` | Zuordnung Artist ↔ Genre | (`genre_id`, `artist_id`) |\n",
    "| `r_albums_artists` | Zuordnung Album ↔ Artist | (`album_id`, `artist_id`)|\n",
    "\n",
    "## Ergebnis\n",
    "Am Ende steht eine **konsistente, validierte und reproduzierbare** Datenbasis:\n",
    "- bereinigte CSVs (für schnelle Inspektion),\n",
    "- Parquet (für effizientes Training/Batching),\n",
    "- ein JSON-Report mit Regeln, Checks und Statistiken.\n"
   ],
   "id": "98a8d858ee79f6ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "55db29e3093df49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:30.310747Z",
     "start_time": "2026-01-05T19:18:30.298208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproductibility\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ],
   "id": "d410767135122672",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config and Paths",
   "id": "e264bcee0e932d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:30.353674Z",
     "start_time": "2026-01-05T19:18:30.334357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_EXPORT_DIR = Path(\"../data/interim/converted_sqlite_samples\")\n",
    "CURRENT_SAMPLE_PATH = BASE_EXPORT_DIR / \"current_sample.json\"\n",
    "cfg = json.loads(CURRENT_SAMPLE_PATH.read_text())\n",
    "SAMPLE_NAME = cfg[\"SAMPLE_NAME\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PipelinePaths:\n",
    "    raw_dir: Path = BASE_EXPORT_DIR / SAMPLE_NAME\n",
    "    clean_dir: Path = Path(\"../data/processed/clean_csv\") / SAMPLE_NAME\n",
    "    parquet_dir: Path = Path(\"../data/processed/parquet\") / SAMPLE_NAME\n",
    "    report_path: Path = Path(\"../data/reports/cleaning_and_rules\") / SAMPLE_NAME\n",
    "\n",
    "PATHS = PipelinePaths()\n",
    "\n",
    "for p in [PATHS.clean_dir,PATHS.report_path,PATHS.raw_dir,PATHS.parquet_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CleaningPolicy:\n",
    "    drop_orphan_bridge_rows:bool = True\n",
    "    clip_popularity:bool = True\n",
    "    popularity_min:int = 0\n",
    "    popularity_max:int = 100\n",
    "    duration_cap_quantile:float = 0.999\n",
    "    tempo_cap_quantile:float = 0.999\n",
    "    audio_01_cols:Tuple[str,...] = (\n",
    "        \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n",
    "        \"liveness\", \"speechiness\", \"valence\"\n",
    "    )\n",
    "    loudness_range:Tuple[float,float] = (-60.0, 5.0)\n",
    "    key_range:Tuple[int,int] = (0,11)\n",
    "    mode_range: Tuple[int, int] = (0, 1)\n",
    "\n",
    "\n",
    "POLICY = CleaningPolicy()\n",
    "\n",
    "RUN_META = {\n",
    "    \"run_ts_unix\": int(time.time()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {k: str(v) for k, v in asdict(PATHS).items()},\n",
    "    \"policy\": asdict(POLICY),\n",
    "}"
   ],
   "id": "6f9868238199006c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Utilities",
   "id": "18b5febcdf6e2067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:30.395765Z",
     "start_time": "2026-01-05T19:18:30.366183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def snake_case(s:str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w]+\",\"_\",s)\n",
    "    s = re.sub(r\"__+\",\"_\",s)\n",
    "    return s.strip(\"_\").lower()\n",
    "\n",
    "def norm_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize whitespace and empty strings to NA.\"\"\"\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    s = s.replace(\"\", pd.NA)\n",
    "    return s\n",
    "def to_int(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def to_float(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "def to_bool(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust boolean parser to pandas BooleanDtype.\"\"\"\n",
    "    x = s.astype(\"string\").str.lower().str.strip()\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "    out[x.isin([\"1\", \"true\", \"t\", \"yes\", \"y\"])] = True\n",
    "    out[x.isin([\"0\", \"false\", \"f\", \"no\", \"n\"])] = False\n",
    "    return out\n",
    "\n",
    "def memory_mb(df: pd.DataFrame) -> float:\n",
    "    return float(df.memory_usage(deep=True).sum()) / (1024 ** 2)\n",
    "\n",
    "def keep_most_complete_row(df: pd.DataFrame, key_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Resolve duplicates by keeping the row with most non-null values.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"_nonnulls\"] = df.notna().sum(axis=1)\n",
    "    df = df.sort_values(\"_nonnulls\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    df = df.drop(columns=[\"_nonnulls\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def clip_series(s: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def parse_release_date_universal(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Universal release_date parser that parses EVERYTHING it reasonably can.\n",
    "\n",
    "    Supports:\n",
    "      - Spotify date strings: \"YYYY\", \"YYYY-MM\", \"YYYY-MM-DD\"\n",
    "      - Epoch timestamps (string or numeric), incl. negative and old:\n",
    "          * >= 11 digits  -> milliseconds\n",
    "          * 9-10 digits   -> seconds\n",
    "      - 0 treated as missing (NaT) to avoid fake 1970-01-01\n",
    "\n",
    "    Returns:\n",
    "      datetime64[ns] Series with NaT for unparseable values.\n",
    "    \"\"\"\n",
    "    x = s.astype(\"string\").str.strip()\n",
    "    x = x.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA, \"null\": pd.NA})\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=x.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # ---------- numeric epoch parsing ----------\n",
    "    num = pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "    # treat 0 as missing (placeholder -> avoids 1970-01-01 pollution)\n",
    "    num = num.mask(num == 0)\n",
    "\n",
    "    # integer coercion\n",
    "    num_int = pd.Series(pd.NA, index=x.index, dtype=\"Int64\")\n",
    "    mask_num = num.notna()\n",
    "    if mask_num.any():\n",
    "        num_int.loc[mask_num] = np.floor(num.loc[mask_num].astype(\"float64\")).astype(\"int64\")\n",
    "\n",
    "    # digit-length heuristic (works for old/negative ms values too)\n",
    "    digits = num_int.abs().astype(\"string\").str.len()\n",
    "    ms_mask = num_int.notna() & (digits >= 11)          # milliseconds\n",
    "    s_mask  = num_int.notna() & digits.between(9, 10)   # seconds\n",
    "\n",
    "    out.loc[ms_mask] = pd.to_datetime(num_int.loc[ms_mask].astype(\"int64\"), unit=\"ms\", errors=\"coerce\")\n",
    "    out.loc[s_mask]  = pd.to_datetime(num_int.loc[s_mask].astype(\"int64\"), unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "    # ---------- spotify-like string parsing ----------\n",
    "    rest = out.isna() & x.notna()\n",
    "\n",
    "    txt = x.copy()\n",
    "    txt = txt.where(~txt.str.fullmatch(r\"\\d{4}\"), txt + \"-01-01\")\n",
    "    txt = txt.where(~txt.str.fullmatch(r\"\\d{4}-\\d{2}\"), txt + \"-01\")\n",
    "\n",
    "    out.loc[rest] = pd.to_datetime(txt.loc[rest], errors=\"coerce\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def assert_gate(condition: bool, msg: str):\n",
    "    if not condition:\n",
    "        raise AssertionError(f\"QUALITY GATE FAILED: {msg}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TableProfile:\n",
    "    name: str\n",
    "    rows: int\n",
    "    cols: int\n",
    "    memory_mb: float\n",
    "    missing_by_col: Dict[str, int]\n",
    "    duplicate_rows_full: Optional[int] = None\n",
    "    duplicate_rows_on_keys: Optional[int] = None\n",
    "\n",
    "def profile_table(df: pd.DataFrame, name: str, key_cols: Optional[List[str]] = None) -> TableProfile:\n",
    "    missing = {c: int(df[c].isna().sum()) for c in df.columns}\n",
    "    prof = TableProfile(\n",
    "        name=name,\n",
    "        rows=int(len(df)),\n",
    "        cols=int(df.shape[1]),\n",
    "        memory_mb=round(memory_mb(df), 2),\n",
    "        missing_by_col=missing,\n",
    "    )\n",
    "    if key_cols:\n",
    "        prof.duplicate_rows_on_keys = int(df.duplicated(subset=key_cols).sum())\n",
    "    else:\n",
    "        prof.duplicate_rows_full = int(df.duplicated().sum())\n",
    "    return prof"
   ],
   "id": "ae056591dfba8a81",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load CSV Exports",
   "id": "b688d7f79e78003c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:37.745959Z",
     "start_time": "2026-01-05T19:18:30.409622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "EXPECTED_FILES = {\n",
    "    \"tracks\": \"tracks.csv\",\n",
    "    \"audio_features\": \"audio_features.csv\",\n",
    "    \"albums\": \"albums.csv\",\n",
    "    \"artists\": \"artists.csv\",\n",
    "    \"genres\": \"genres.csv\",\n",
    "    \"r_albums_tracks\": \"r_albums_tracks.csv\",\n",
    "    \"r_track_artist\": \"r_track_artist.csv\",\n",
    "    \"r_artist_genre\": \"r_artist_genre.csv\",\n",
    "    \"r_albums_artists\": \"r_albums_artists.csv\",\n",
    "}\n",
    "\n",
    "missing_files = [k for k, f in EXPECTED_FILES.items() if not (PATHS.raw_dir / f).exists()]\n",
    "if missing_files:\n",
    "    print(\" Missing CSV files: \", missing_files)\n",
    "else:\n",
    "    print(\"All expected CSV exports found !\")\n",
    "\n",
    "def load_csv(name: str) -> pd.DataFrame:\n",
    "    fp = PATHS.raw_dir / EXPECTED_FILES[name]\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    df.columns = [snake_case(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "raw: Dict[str, pd.DataFrame] = {}\n",
    "for table in EXPECTED_FILES:\n",
    "    fp = PATHS.raw_dir / EXPECTED_FILES[table]\n",
    "    if fp.exists():\n",
    "        raw[table] = load_csv(table)\n",
    "\n",
    "{k: v.shape for k, v in raw.items()}"
   ],
   "id": "2c0b0f9031f9d812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All expected CSV exports found !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tracks': (300000, 10),\n",
       " 'audio_features': (299954, 15),\n",
       " 'albums': (195938, 6),\n",
       " 'artists': (187440, 4),\n",
       " 'genres': (5455, 1),\n",
       " 'r_albums_tracks': (340898, 2),\n",
       " 'r_track_artist': (407296, 2),\n",
       " 'r_artist_genre': (194023, 2),\n",
       " 'r_albums_artists': (224955, 2)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drop useless / high-missing columns",
   "id": "4b72e739ba9a886d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:37.844323Z",
     "start_time": "2026-01-05T19:18:37.786808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"tracks\" in raw:\n",
    "    raw[\"tracks\"] = raw[\"tracks\"].drop(columns=[\"is_playable\"], errors=\"ignore\")\n",
    "\n",
    "# albums: drop column \"album_group\" (100% missing)\n",
    "if \"albums\" in raw:\n",
    "    raw[\"albums\"] = raw[\"albums\"].drop(columns=[\"album_group\"], errors=\"ignore\")\n",
    "\n",
    "# OPTIONAL (recommended): keep only an indicator and drop URL column\n",
    "# (URLs are high-cardinality and not useful directly as a text feature in sklearn tabular models)\n",
    "if \"tracks\" in raw and \"preview_url\" in raw[\"tracks\"].columns:\n",
    "    raw[\"tracks\"][\"has_preview\"] = raw[\"tracks\"][\"preview_url\"].notna().astype(\"int8\")\n",
    "    raw[\"tracks\"] = raw[\"tracks\"].drop(columns=[\"preview_url\"], errors=\"ignore\")"
   ],
   "id": "a6fa3c60ca82f7d3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre - profiles",
   "id": "b5f2e92ce5947814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:39.586952Z",
     "start_time": "2026-01-05T19:18:37.859638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "profiles_before: Dict[str, Any] = {}\n",
    "\n",
    "if \"tracks\" in raw:\n",
    "    profiles_before[\"tracks\"] = asdict(profile_table(raw[\"tracks\"], \"tracks\", key_cols=[\"track_id\"] if \"track_id\" in raw[\"tracks\"].columns else [\"id\"]))\n",
    "if \"audio_features\" in raw:\n",
    "    profiles_before[\"audio_features\"] = asdict(profile_table(raw[\"audio_features\"], \"audio_features\", key_cols=[\"id\"]))\n",
    "if \"albums\" in raw:\n",
    "    profiles_before[\"albums\"] = asdict(profile_table(raw[\"albums\"], \"albums\", key_cols=[\"id\"]))\n",
    "if \"artists\" in raw:\n",
    "    profiles_before[\"artists\"] = asdict(profile_table(raw[\"artists\"], \"artists\", key_cols=[\"id\"]))\n",
    "if \"genres\" in raw:\n",
    "    profiles_before[\"genres\"] = asdict(profile_table(raw[\"genres\"], \"genres\", key_cols=[\"id\"]))\n",
    "\n",
    "# bridges\n",
    "if \"r_albums_tracks\" in raw:\n",
    "    profiles_before[\"r_albums_tracks\"] = asdict(profile_table(raw[\"r_albums_tracks\"], \"r_albums_tracks\", key_cols=[\"album_id\",\"track_id\"]))\n",
    "if \"r_track_artist\" in raw:\n",
    "    profiles_before[\"r_track_artist\"] = asdict(profile_table(raw[\"r_track_artist\"], \"r_track_artist\", key_cols=[\"track_id\",\"artist_id\"]))\n",
    "if \"r_artist_genre\" in raw:\n",
    "    profiles_before[\"r_artist_genre\"] = asdict(profile_table(raw[\"r_artist_genre\"], \"r_artist_genre\", key_cols=[\"genre_id\",\"artist_id\"]))\n",
    "if \"r_albums_artists\" in raw:\n",
    "    profiles_before[\"r_albums_artists\"] = asdict(profile_table(raw[\"r_albums_artists\"], \"r_albums_artists\", key_cols=[\"album_id\",\"artist_id\"]))\n",
    "\n",
    "pd.DataFrame.from_dict(profiles_before, orient=\"index\")[[\"rows\",\"cols\",\"memory_mb\",\"duplicate_rows_on_keys\"]]"
   ],
   "id": "9d6d31ca41654348",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    rows  cols  memory_mb  duplicate_rows_on_keys\n",
       "tracks            300000     9      73.11                       0\n",
       "audio_features    299954    15      82.38                       0\n",
       "albums            195938     5      40.45                       0\n",
       "artists           187440     4      26.94                       0\n",
       "genres              5455     1       0.33                       0\n",
       "r_albums_tracks   340898     2      46.17                       0\n",
       "r_track_artist    407296     2      55.16                       0\n",
       "r_artist_genre    194023     2      24.58                       0\n",
       "r_albums_artists  224955     2      30.46                       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>cols</th>\n",
       "      <th>memory_mb</th>\n",
       "      <th>duplicate_rows_on_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tracks</th>\n",
       "      <td>300000</td>\n",
       "      <td>9</td>\n",
       "      <td>73.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_features</th>\n",
       "      <td>299954</td>\n",
       "      <td>15</td>\n",
       "      <td>82.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albums</th>\n",
       "      <td>195938</td>\n",
       "      <td>5</td>\n",
       "      <td>40.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artists</th>\n",
       "      <td>187440</td>\n",
       "      <td>4</td>\n",
       "      <td>26.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genres</th>\n",
       "      <td>5455</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_tracks</th>\n",
       "      <td>340898</td>\n",
       "      <td>2</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_track_artist</th>\n",
       "      <td>407296</td>\n",
       "      <td>2</td>\n",
       "      <td>55.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_artist_genre</th>\n",
       "      <td>194023</td>\n",
       "      <td>2</td>\n",
       "      <td>24.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_artists</th>\n",
       "      <td>224955</td>\n",
       "      <td>2</td>\n",
       "      <td>30.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaning Classes",
   "id": "7bd691c8c257c336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:39.645492Z",
     "start_time": "2026-01-05T19:18:39.619298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseCleaner:\n",
    "    name:str\n",
    "\n",
    "    def clean(self,df : pd.DataFrame) -> pd.DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TracksCleaner(BaseCleaner):\n",
    "    name = \"tracks\"\n",
    "\n",
    "    def clean(self,df : pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        # Ensure PK column name: track_id\n",
    "        if \"track_id\" not in df.columns and \"id\" in df.columns:\n",
    "            df = df.rename(columns={\"id\": \"track_id\"})\n",
    "\n",
    "        # Normalize strings\n",
    "        df[\"track_id\"] = norm_str(df[\"track_id\"])\n",
    "        df = df[df[\"track_id\"].notna()]\n",
    "\n",
    "        for c in [\"name\", \"preview_url\", \"audio_feature_id\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        # Numerics\n",
    "        for c in [\"disc_number\", \"track_number\", \"duration\", \"popularity\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = to_int(df[c])\n",
    "\n",
    "        # Booleans\n",
    "        if \"explicit\" in df.columns:\n",
    "            df[\"explicit\"] = to_bool(df[\"explicit\"])\n",
    "        if \"is_playable\" in df.columns:\n",
    "            df[\"is_playable\"] = to_bool(df[\"is_playable\"])\n",
    "\n",
    "        # Rules: duration > 0, cap extremes\n",
    "        if \"duration\" in df.columns:\n",
    "            df.loc[df[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "            cap = df[\"duration\"].dropna().quantile(POLICY.duration_cap_quantile) if df[\"duration\"].notna().any() else None\n",
    "            if cap and not math.isnan(cap):\n",
    "                df[\"duration\"] = df[\"duration\"].clip(upper=int(cap))\n",
    "\n",
    "        # Rules: popularity in [0, 100]\n",
    "        if \"popularity\" in df.columns and POLICY.clip_popularity:\n",
    "            df[\"popularity\"] = clip_series(df[\"popularity\"], POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        # Dedupe by track_id keeping the most complete row\n",
    "        df = keep_most_complete_row(df, [\"track_id\"])\n",
    "        return df\n",
    "\n",
    "class AudioFeaturesCleaner(BaseCleaner):\n",
    "    name = \"audio_features\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        # normalize URL-like or text columns\n",
    "        for c in [\"analysis_url\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        # floats\n",
    "        for c in POLICY.audio_01_cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = clip_series(to_float(df[c]), 0.0, 1.0)\n",
    "\n",
    "        # tempo\n",
    "        if \"tempo\" in df.columns:\n",
    "            df[\"tempo\"] = to_float(df[\"tempo\"])\n",
    "            df.loc[df[\"tempo\"] <= 0, \"tempo\"] = pd.NA\n",
    "            cap = df[\"tempo\"].dropna().quantile(POLICY.tempo_cap_quantile) if df[\"tempo\"].notna().any() else None\n",
    "            if cap and not math.isnan(cap):\n",
    "                df[\"tempo\"] = df[\"tempo\"].clip(upper=float(cap))\n",
    "\n",
    "        # loudness\n",
    "        if \"loudness\" in df.columns:\n",
    "            df[\"loudness\"] = to_float(df[\"loudness\"])\n",
    "            lo, hi = POLICY.loudness_range\n",
    "            df.loc[~df[\"loudness\"].between(lo, hi), \"loudness\"] = pd.NA\n",
    "\n",
    "        # ints\n",
    "        for c in [\"key\", \"mode\", \"time_signature\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = to_int(df[c])\n",
    "\n",
    "        if \"key\" in df.columns:\n",
    "            lo, hi = POLICY.key_range\n",
    "            df.loc[~df[\"key\"].between(lo, hi), \"key\"] = pd.NA\n",
    "\n",
    "        if \"mode\" in df.columns:\n",
    "            lo, hi = POLICY.mode_range\n",
    "            df.loc[~df[\"mode\"].between(lo, hi), \"mode\"] = pd.NA\n",
    "\n",
    "        # duration (audio_features.duration is float in your export)\n",
    "        if \"duration\" in df.columns:\n",
    "            df[\"duration\"] = to_float(df[\"duration\"])\n",
    "            df.loc[df[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class AlbumsCleaner(BaseCleaner):\n",
    "    name = \"albums\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        for c in [\"name\", \"album_group\", \"album_type\", \"release_date\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        if \"album_group\" in df.columns:\n",
    "            df[\"album_group\"] = df[\"album_group\"].str.lower()\n",
    "        if \"album_type\" in df.columns:\n",
    "            df[\"album_type\"] = df[\"album_type\"].str.lower()\n",
    "\n",
    "        if \"popularity\" in df.columns:\n",
    "            df[\"popularity\"] = clip_series(to_int(df[\"popularity\"]), POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        # parsed date (keep both)\n",
    "        if \"release_date\" in df.columns:\n",
    "            df[\"release_date_parsed\"] = parse_release_date_universal(df[\"release_date\"])\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class ArtistsCleaner(BaseCleaner):\n",
    "    name = \"artists\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        if \"name\" in df.columns:\n",
    "            df[\"name\"] = norm_str(df[\"name\"])\n",
    "\n",
    "        if \"popularity\" in df.columns:\n",
    "            df[\"popularity\"] = clip_series(to_int(df[\"popularity\"]), POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        if \"followers\" in df.columns:\n",
    "            df[\"followers\"] = to_int(df[\"followers\"])\n",
    "            df[\"followers\"] = df[\"followers\"].clip(lower=0)\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class GenresCleaner(BaseCleaner):\n",
    "    name = \"genres\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        # your genres export only has \"id\"\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()].drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "class BridgeCleaner(BaseCleaner):\n",
    "    \"\"\"Generic bridge cleaner for composite keys + string normalization + dedupe.\"\"\"\n",
    "    def __init__(self, name: str, key_cols: List[str]):\n",
    "        self.name = name\n",
    "        self.key_cols = key_cols\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for c in self.key_cols:\n",
    "            df[c] = norm_str(df[c])\n",
    "        df = df.dropna(subset=self.key_cols)\n",
    "        df = df.drop_duplicates(subset=self.key_cols, keep=\"first\").reset_index(drop=True)\n",
    "        return df\n",
    "\n"
   ],
   "id": "9a5235ac4d94105",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying the cleaning",
   "id": "eac88fafd74fd286"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:47.999636Z",
     "start_time": "2026-01-05T19:18:39.662086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleaned: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "# Entities\n",
    "if \"tracks\" in raw:\n",
    "    cleaned[\"tracks\"] = TracksCleaner().clean(raw[\"tracks\"])\n",
    "if \"audio_features\" in raw:\n",
    "    cleaned[\"audio_features\"] = AudioFeaturesCleaner().clean(raw[\"audio_features\"])\n",
    "if \"albums\" in raw:\n",
    "    cleaned[\"albums\"] = AlbumsCleaner().clean(raw[\"albums\"])\n",
    "if \"artists\" in raw:\n",
    "    cleaned[\"artists\"] = ArtistsCleaner().clean(raw[\"artists\"])\n",
    "if \"genres\" in raw:\n",
    "    cleaned[\"genres\"] = GenresCleaner().clean(raw[\"genres\"])\n",
    "\n",
    "# Bridges\n",
    "if \"r_albums_tracks\" in raw:\n",
    "    cleaned[\"r_albums_tracks\"] = BridgeCleaner(\"r_albums_tracks\", [\"album_id\",\"track_id\"]).clean(raw[\"r_albums_tracks\"])\n",
    "if \"r_track_artist\" in raw:\n",
    "    cleaned[\"r_track_artist\"] = BridgeCleaner(\"r_track_artist\", [\"track_id\",\"artist_id\"]).clean(raw[\"r_track_artist\"])\n",
    "if \"r_artist_genre\" in raw:\n",
    "    cleaned[\"r_artist_genre\"] = BridgeCleaner(\"r_artist_genre\", [\"genre_id\",\"artist_id\"]).clean(raw[\"r_artist_genre\"])\n",
    "if \"r_albums_artists\" in raw:\n",
    "    cleaned[\"r_albums_artists\"] = BridgeCleaner(\"r_albums_artists\", [\"album_id\",\"artist_id\"]).clean(raw[\"r_albums_artists\"])\n",
    "\n",
    "{k: v.shape for k, v in cleaned.items()}\n"
   ],
   "id": "a2b1922c65d18af3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16240\\2907112459.py:88: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  out.loc[rest] = pd.to_datetime(txt.loc[rest], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tracks': (300000, 9),\n",
       " 'audio_features': (299954, 15),\n",
       " 'albums': (195938, 6),\n",
       " 'artists': (187440, 4),\n",
       " 'genres': (5455, 1),\n",
       " 'r_albums_tracks': (340898, 2),\n",
       " 'r_track_artist': (407296, 2),\n",
       " 'r_artist_genre': (194023, 2),\n",
       " 'r_albums_artists': (224955, 2)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Referential Integrity Enforcement"
   ],
   "id": "b8c3148e6d0e3959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:50.115911Z",
     "start_time": "2026-01-05T19:18:48.010452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build ID sets\n",
    "track_ids = set(cleaned[\"tracks\"][\"track_id\"].unique()) if \"tracks\" in cleaned else set()\n",
    "album_ids = set(cleaned[\"albums\"][\"id\"].unique()) if \"albums\" in cleaned else set()\n",
    "artist_ids = set(cleaned[\"artists\"][\"id\"].unique()) if \"artists\" in cleaned else set()\n",
    "genre_ids = set(cleaned[\"genres\"][\"id\"].unique()) if \"genres\" in cleaned else set()\n",
    "af_ids = set(cleaned[\"audio_features\"][\"id\"].unique()) if \"audio_features\" in cleaned else set()\n",
    "\n",
    "def filter_fk(df: pd.DataFrame, col: str, allowed: set) -> pd.DataFrame:\n",
    "    return df[df[col].isin(allowed)].copy()\n",
    "\n",
    "# Bridge FK cleanup\n",
    "if POLICY.drop_orphan_bridge_rows:\n",
    "    if \"r_albums_tracks\" in cleaned:\n",
    "        rat = cleaned[\"r_albums_tracks\"]\n",
    "        rat = filter_fk(filter_fk(rat, \"album_id\", album_ids), \"track_id\", track_ids)\n",
    "        cleaned[\"r_albums_tracks\"] = rat.reset_index(drop=True)\n",
    "\n",
    "    if \"r_track_artist\" in cleaned:\n",
    "        rta = cleaned[\"r_track_artist\"]\n",
    "        rta = filter_fk(filter_fk(rta, \"track_id\", track_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_track_artist\"] = rta.reset_index(drop=True)\n",
    "\n",
    "    if \"r_artist_genre\" in cleaned:\n",
    "        rag = cleaned[\"r_artist_genre\"]\n",
    "        rag = filter_fk(filter_fk(rag, \"genre_id\", genre_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_artist_genre\"] = rag.reset_index(drop=True)\n",
    "\n",
    "    if \"r_albums_artists\" in cleaned:\n",
    "        raa = cleaned[\"r_albums_artists\"]\n",
    "        raa = filter_fk(filter_fk(raa, \"album_id\", album_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_albums_artists\"] = raa.reset_index(drop=True)\n",
    "\n",
    "# Track -> audio_feature_id FK policy: set invalid to NA (do NOT drop track)\n",
    "if \"tracks\" in cleaned and \"audio_feature_id\" in cleaned[\"tracks\"].columns and af_ids:\n",
    "    bad = cleaned[\"tracks\"][\"audio_feature_id\"].notna() & ~cleaned[\"tracks\"][\"audio_feature_id\"].isin(af_ids)\n",
    "    cleaned[\"tracks\"].loc[bad, \"audio_feature_id\"] = pd.NA\n",
    "\n",
    "{k: v.shape for k, v in cleaned.items()}"
   ],
   "id": "fc15949062984921",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tracks': (300000, 9),\n",
       " 'audio_features': (299954, 15),\n",
       " 'albums': (195938, 6),\n",
       " 'artists': (187440, 4),\n",
       " 'genres': (5455, 1),\n",
       " 'r_albums_tracks': (340898, 2),\n",
       " 'r_track_artist': (407296, 2),\n",
       " 'r_artist_genre': (194023, 2),\n",
       " 'r_albums_artists': (218032, 2)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Post Cleaning",
   "id": "13a8fe12bcebad24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:53.183493Z",
     "start_time": "2026-01-05T19:18:50.141527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiles_after: Dict[str, Any] = {}\n",
    "\n",
    "profiles_after[\"tracks\"] = asdict(profile_table(cleaned[\"tracks\"], \"tracks\", key_cols=[\"track_id\"]))\n",
    "profiles_after[\"audio_features\"] = asdict(profile_table(cleaned[\"audio_features\"], \"audio_features\", key_cols=[\"id\"]))\n",
    "profiles_after[\"albums\"] = asdict(profile_table(cleaned[\"albums\"], \"albums\", key_cols=[\"id\"]))\n",
    "profiles_after[\"artists\"] = asdict(profile_table(cleaned[\"artists\"], \"artists\", key_cols=[\"id\"]))\n",
    "profiles_after[\"genres\"] = asdict(profile_table(cleaned[\"genres\"], \"genres\", key_cols=[\"id\"]))\n",
    "\n",
    "if \"r_albums_tracks\" in cleaned:\n",
    "    profiles_after[\"r_albums_tracks\"] = asdict(profile_table(cleaned[\"r_albums_tracks\"], \"r_albums_tracks\", key_cols=[\"album_id\",\"track_id\"]))\n",
    "if \"r_track_artist\" in cleaned:\n",
    "    profiles_after[\"r_track_artist\"] = asdict(profile_table(cleaned[\"r_track_artist\"], \"r_track_artist\", key_cols=[\"track_id\",\"artist_id\"]))\n",
    "if \"r_artist_genre\" in cleaned:\n",
    "    profiles_after[\"r_artist_genre\"] = asdict(profile_table(cleaned[\"r_artist_genre\"], \"r_artist_genre\", key_cols=[\"genre_id\",\"artist_id\"]))\n",
    "if \"r_albums_artists\" in cleaned:\n",
    "    profiles_after[\"r_albums_artists\"] = asdict(profile_table(cleaned[\"r_albums_artists\"], \"r_albums_artists\", key_cols=[\"album_id\",\"artist_id\"]))\n",
    "\n",
    "pd.DataFrame.from_dict(profiles_after, orient=\"index\")[[\"rows\",\"cols\",\"memory_mb\",\"duplicate_rows_on_keys\"]]\n"
   ],
   "id": "dfdb828144d91e3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    rows  cols  memory_mb  duplicate_rows_on_keys\n",
       "tracks            300000     9      72.54                       0\n",
       "audio_features    299954    15      83.24                       0\n",
       "albums            195938     6      52.19                       0\n",
       "artists           187440     4      27.30                       0\n",
       "genres              5455     1       0.33                       0\n",
       "r_albums_tracks   340898     2      46.17                       0\n",
       "r_track_artist    407296     2      55.16                       0\n",
       "r_artist_genre    194023     2      24.58                       0\n",
       "r_albums_artists  218032     2      29.53                       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>cols</th>\n",
       "      <th>memory_mb</th>\n",
       "      <th>duplicate_rows_on_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tracks</th>\n",
       "      <td>300000</td>\n",
       "      <td>9</td>\n",
       "      <td>72.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_features</th>\n",
       "      <td>299954</td>\n",
       "      <td>15</td>\n",
       "      <td>83.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albums</th>\n",
       "      <td>195938</td>\n",
       "      <td>6</td>\n",
       "      <td>52.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artists</th>\n",
       "      <td>187440</td>\n",
       "      <td>4</td>\n",
       "      <td>27.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genres</th>\n",
       "      <td>5455</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_tracks</th>\n",
       "      <td>340898</td>\n",
       "      <td>2</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_track_artist</th>\n",
       "      <td>407296</td>\n",
       "      <td>2</td>\n",
       "      <td>55.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_artist_genre</th>\n",
       "      <td>194023</td>\n",
       "      <td>2</td>\n",
       "      <td>24.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_artists</th>\n",
       "      <td>218032</td>\n",
       "      <td>2</td>\n",
       "      <td>29.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outlier & rule-based validation + flags\n",
    "\n",
    "\n",
    "In diesem Schritt wurden **keine Outlier pauschal gelöscht** (z. B. via IQR), weil viele Spalten in den Spotify-Daten **diskret**, **zero-inflated** oder **heavy-tail** verteilt sind. IQR wäre hier oft **zu aggressiv** und würde viele **gültige Spezialfälle** (z. B. Multi-Disc, Live-Tracks, Rap/Speech) fälschlich als Outlier markieren.\n",
    "Stattdessen nutzen wir ein **Contract-/Rule-based Cleaning**:\n",
    "\n",
    "1. **Ungültige Werte (Domain-Verletzung)** → auf `NaN` setzen oder clippen\n",
    "2. **Extreme, aber plausible Werte (Long-Tail)** → **Quantile-Capping** (z. B. 0.1% / 99.9%) statt Dropping\n",
    "3. **Signal behalten** → zusätzliche **Flag-Features** (`is_*`) für Extremfälle\n",
    "\n",
    "---\n",
    "\n",
    "#### Tracks – Regeln & Outlier-Handling\n",
    "\n",
    "- **popularity**: auf **[0,100]** clippen (Spotify-Definition), keine Zeilen löschen\n",
    "- **duration**: `<= 0` → `NaN`, außerdem **99.9%-Quantile-Cap** (Upper Tail); Flag **`is_long_track`**\n",
    "- **track_number**: diskret/albumabhängig → **Rule-based**: `<=0` und `>200` → `NaN`; Flag **`is_tracknum_extreme`**\n",
    "- **disc_number**: meist 1 (IQR wäre falsch) → Flag **`is_multidisc`** (`>1`), extreme Werte `>10` → `NaN`; Flag **`is_disc_extreme`**\n",
    "\n",
    "**Warum?**\n",
    "IQR markiert bei fast-konstanten/discreten Spalten (disc_number, track_number) zu viele gültige Fälle. Quantile-Cap ist robuster für Long-Tail-Variablen (duration).\n",
    "\n",
    "---\n",
    "\n",
    "#### Audio Features – Regeln & Outlier-Handling\n",
    "\n",
    "- **time_signature**: als **kategorial** behandeln; nur `{3,4,5}` gültig, sonst `NaN`; Flag **`is_time_signature_rare`**\n",
    "- **tempo**: `<=0` → `NaN`; **99.9%-Quantile-Cap**; Flag **`is_tempo_extreme`**\n",
    "- **loudness**: außerhalb **[-60, 5]** → `NaN` (Domain); Flag **`is_loudness_very_low`** (`<-40`)\n",
    "- **speechiness**: [0,1] clippen; Flag **`is_high_speech`** (>= 90%-Quantil)\n",
    "- **instrumentalness**: [0,1] clippen; Flag **`is_instrumental`** (>= 0.5)\n",
    "- **key/mode**: streng validieren (`key` 0–11, `mode` 0–1), sonst `NaN`\n",
    "\n",
    "**Warum?**\n",
    "Viele Audio-Features sind **bounded** (0..1) oder **kategorial** (time_signature, key/mode). Bei skew/zero-inflation (speechiness, instrumentalness) ist IQR ungeeignet → Flags + ggf. log/bins später.\n",
    "\n",
    "---\n",
    "\n",
    "#### Artists – Regeln & Outlier-Handling\n",
    "\n",
    "- **followers**: negative → `NaN`; **log1p-Feature** (`followers_log1p`) + Flag **`is_followers_extreme`** (99.9%-Quantil)\n",
    "- **artist popularity**: auf **[0,100]** clippen\n",
    "\n",
    "**Warum?**\n",
    "Followers sind stark **heavy-tail** (Superstars vs. Long Tail). Log-Transform ist Standard und stabilisiert Modelle.\n",
    "\n",
    "---\n",
    "\n",
    "#### Albums – Regeln & Outlier-Handling\n",
    "\n",
    "- **release_date**: in `datetime` konvertieren und **Year-Range** validieren (z. B. 1900–2035); invalid → `NaT`; Flag **`is_release_year_invalid`**\n",
    "- **album popularity**: auf **[0,100]** clippen\n",
    "\n",
    "**Warum?**\n",
    "Release-Daten können vereinzelt kaputt sein; wir **löschen keine Alben**, sondern korrigieren/flaggen.\n",
    "\n",
    "---\n",
    "\n",
    "### Ergebnis / Vorteil\n",
    "- **Stabile Features** für ML (weniger extreme Hebelwerte)\n",
    "- **Keine Join-/Graph-Schäden** durch massives Dropping\n",
    "- **Wichtige Rare-Cases bleiben erhalten** (über `is_*` Flags)\n",
    "- Die anschließenden **Quality Gates** prüfen dann die finalen, regel-konformen Daten.\n"
   ],
   "id": "5540c1b9d3d7a840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:18:53.621474Z",
     "start_time": "2026-01-05T19:18:53.204153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# OUTLIERS & RULE-BASED VALIDATION (based on your observations)\n",
    "# - NO IQR dropping (too aggressive for skew/discrete/zero-inflated cols)\n",
    "# - Use rule-based invalid -> NA\n",
    "# - Use quantile caps for heavy tails\n",
    "# - Add flags to preserve signal (rare cases)\n",
    "# ============================================================\n",
    "\n",
    "def quantile_cap(series: pd.Series, q_low: float = 0.001, q_high: float = 0.999) -> tuple[float, float]:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(s.quantile(q_low)), float(s.quantile(q_high)))\n",
    "\n",
    "# -------------------------\n",
    "# TRACKS rules\n",
    "# -------------------------\n",
    "if \"tracks\" in cleaned:\n",
    "    t = cleaned[\"tracks\"].copy()\n",
    "\n",
    "    # popularity: domain range [0,100]\n",
    "    if \"popularity\" in t.columns:\n",
    "        t[\"popularity\"] = pd.to_numeric(t[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "        # optional: binning helper (nice for Notebook 3)\n",
    "        # t[\"popularity_bin\"] = pd.cut(t[\"popularity\"], bins=[-1,0,20,40,60,80,100],\n",
    "        #                              labels=[\"0\",\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\"])\n",
    "\n",
    "    # duration: invalid -> NA, quantile cap, long-track flag\n",
    "    if \"duration\" in t.columns:\n",
    "        t[\"duration\"] = pd.to_numeric(t[\"duration\"], errors=\"coerce\")\n",
    "        t.loc[t[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "\n",
    "        lo, hi = quantile_cap(t[\"duration\"], q_low=0.001, q_high=0.999)\n",
    "        # cap only upper tail; lower tail usually valid if >0\n",
    "        t[\"is_long_track\"] = (t[\"duration\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            t[\"duration\"] = t[\"duration\"].clip(upper=int(hi))\n",
    "\n",
    "    # track_number: discrete + album-structural -> rule-based, NOT IQR\n",
    "    if \"track_number\" in t.columns:\n",
    "        t[\"track_number\"] = pd.to_numeric(t[\"track_number\"], errors=\"coerce\")\n",
    "        # rule: <=0 invalid\n",
    "        t.loc[t[\"track_number\"] <= 0, \"track_number\"] = pd.NA\n",
    "        # rule: extremely high track numbers probably broken\n",
    "        t[\"is_tracknum_extreme\"] = (t[\"track_number\"] > 200).astype(\"int8\")\n",
    "        t.loc[t[\"track_number\"] > 200, \"track_number\"] = pd.NA\n",
    "\n",
    "    # disc_number: mostly 1; create is_multidisc, rule extreme\n",
    "    if \"disc_number\" in t.columns:\n",
    "        t[\"disc_number\"] = pd.to_numeric(t[\"disc_number\"], errors=\"coerce\")\n",
    "        t.loc[t[\"disc_number\"] <= 0, \"disc_number\"] = pd.NA\n",
    "        t[\"is_multidisc\"] = (t[\"disc_number\"] > 1).astype(\"int8\")\n",
    "        t[\"is_disc_extreme\"] = (t[\"disc_number\"] > 10).astype(\"int8\")\n",
    "        t.loc[t[\"disc_number\"] > 10, \"disc_number\"] = pd.NA\n",
    "\n",
    "    cleaned[\"tracks\"] = t\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AUDIO FEATURES rules\n",
    "# -------------------------\n",
    "if \"audio_features\" in cleaned:\n",
    "    a = cleaned[\"audio_features\"].copy()\n",
    "\n",
    "    # time_signature: treat as categorical; rule-based set invalid to NA\n",
    "    if \"time_signature\" in a.columns:\n",
    "        a[\"time_signature\"] = pd.to_numeric(a[\"time_signature\"], errors=\"coerce\")\n",
    "        valid_ts = {3, 4, 5}\n",
    "        a[\"is_time_signature_rare\"] = (~a[\"time_signature\"].isin(list(valid_ts))).astype(\"int8\")\n",
    "        a.loc[~a[\"time_signature\"].isin(list(valid_ts)), \"time_signature\"] = pd.NA\n",
    "\n",
    "    # tempo: invalid -> NA, quantile cap, keep rare fast/slow\n",
    "    if \"tempo\" in a.columns:\n",
    "        a[\"tempo\"] = pd.to_numeric(a[\"tempo\"], errors=\"coerce\")\n",
    "        a.loc[a[\"tempo\"] <= 0, \"tempo\"] = pd.NA\n",
    "        lo, hi = quantile_cap(a[\"tempo\"], q_low=0.001, q_high=0.999)\n",
    "        a[\"is_tempo_extreme\"] = ((a[\"tempo\"] < lo) | (a[\"tempo\"] > hi)).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            a[\"tempo\"] = a[\"tempo\"].clip(upper=hi)\n",
    "\n",
    "    # loudness: domain sanity; very low values likely bad\n",
    "    if \"loudness\" in a.columns:\n",
    "        a[\"loudness\"] = pd.to_numeric(a[\"loudness\"], errors=\"coerce\")\n",
    "        # \"hard\" domain guard\n",
    "        a.loc[~a[\"loudness\"].between(-60, 5), \"loudness\"] = pd.NA\n",
    "        # additional flag: suspiciously low (often data issue)\n",
    "        a[\"is_loudness_very_low\"] = (a[\"loudness\"] < -40).astype(\"int8\")\n",
    "\n",
    "    # duration in audio_features\n",
    "    if \"duration\" in a.columns:\n",
    "        a[\"duration\"] = pd.to_numeric(a[\"duration\"], errors=\"coerce\")\n",
    "        a.loc[a[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "        _, hi = quantile_cap(a[\"duration\"], q_low=0.001, q_high=0.999)\n",
    "        a[\"is_af_long\"] = (a[\"duration\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            a[\"duration\"] = a[\"duration\"].clip(upper=hi)\n",
    "\n",
    "    # speechiness + instrumentalness: skew / zero inflation -> flags + (optional) log transform later\n",
    "    if \"speechiness\" in a.columns:\n",
    "        a[\"speechiness\"] = pd.to_numeric(a[\"speechiness\"], errors=\"coerce\").clip(0, 1)\n",
    "        a[\"is_high_speech\"] = (a[\"speechiness\"] >= a[\"speechiness\"].dropna().quantile(0.90)).astype(\"int8\") \\\n",
    "                              if a[\"speechiness\"].notna().any() else 0\n",
    "\n",
    "    if \"instrumentalness\" in a.columns:\n",
    "        a[\"instrumentalness\"] = pd.to_numeric(a[\"instrumentalness\"], errors=\"coerce\").clip(0, 1)\n",
    "        # classic threshold used often in practice\n",
    "        a[\"is_instrumental\"] = (a[\"instrumentalness\"] >= 0.5).astype(\"int8\")\n",
    "\n",
    "    # key/mode: categorical sanity\n",
    "    if \"key\" in a.columns:\n",
    "        a[\"key\"] = pd.to_numeric(a[\"key\"], errors=\"coerce\")\n",
    "        a.loc[~a[\"key\"].between(0, 11), \"key\"] = pd.NA\n",
    "\n",
    "    if \"mode\" in a.columns:\n",
    "        a[\"mode\"] = pd.to_numeric(a[\"mode\"], errors=\"coerce\")\n",
    "        a.loc[~a[\"mode\"].between(0, 1), \"mode\"] = pd.NA\n",
    "\n",
    "    cleaned[\"audio_features\"] = a\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ARTISTS rules\n",
    "# -------------------------\n",
    "if \"artists\" in cleaned:\n",
    "    ar = cleaned[\"artists\"].copy()\n",
    "\n",
    "    # followers: heavy tail -> do not drop, add log feature and extreme flag\n",
    "    if \"followers\" in ar.columns:\n",
    "        ar[\"followers\"] = pd.to_numeric(ar[\"followers\"], errors=\"coerce\")\n",
    "        ar.loc[ar[\"followers\"] < 0, \"followers\"] = pd.NA\n",
    "        _, hi = quantile_cap(ar[\"followers\"], q_low=0.001, q_high=0.999)\n",
    "        ar[\"is_followers_extreme\"] = (ar[\"followers\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        ar[\"followers_log1p\"] = np.log1p(ar[\"followers\"].fillna(0)).astype(\"float64\")\n",
    "\n",
    "    # popularity: bounded [0,100] and can be binned later\n",
    "    if \"popularity\" in ar.columns:\n",
    "        ar[\"popularity\"] = pd.to_numeric(ar[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "    cleaned[\"artists\"] = ar\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ALBUMS rules\n",
    "# -------------------------\n",
    "if \"albums\" in cleaned:\n",
    "    al = cleaned[\"albums\"].copy()\n",
    "\n",
    "    # release_date: validate year range; keep flag instead of dropping\n",
    "    # Notebook 2 already parsed release_date_parsed; enforce year sanity\n",
    "    if \"release_date_parsed\" in al.columns:\n",
    "        years = pd.to_datetime(al[\"release_date_parsed\"], errors=\"coerce\").dt.year\n",
    "        # conservative sanity range (Spotify catalog)\n",
    "        al[\"is_release_year_invalid\"] = ((years < 1900) | (years > 2035)).astype(\"int8\")\n",
    "        al.loc[(years < 1900) | (years > 2035), \"release_date_parsed\"] = pd.NaT\n",
    "\n",
    "        # add derived year features early (optional)\n",
    "        al[\"release_year\"] = pd.to_datetime(al[\"release_date_parsed\"], errors=\"coerce\").dt.year.astype(\"Int64\")\n",
    "\n",
    "    # album popularity: clip\n",
    "    if \"popularity\" in al.columns:\n",
    "        al[\"popularity\"] = pd.to_numeric(al[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "    cleaned[\"albums\"] = al\n",
    "\n",
    "\n",
    "print(\"Outlier rules applied: rule-based invalidation + quantile caps + flags (no IQR dropping).\")\n"
   ],
   "id": "75945b45f48460f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier rules applied: rule-based invalidation + quantile caps + flags (no IQR dropping).\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quality Gates",
   "id": "d621e738ad2aa172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:19:06.907903Z",
     "start_time": "2026-01-05T19:18:53.643307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PKs must be unique and non-null\n",
    "assert_gate(cleaned[\"tracks\"][\"track_id\"].notna().all(), \"tracks.track_id contains NA\")\n",
    "assert_gate(cleaned[\"tracks\"][\"track_id\"].is_unique, \"tracks.track_id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"audio_features\"][\"id\"].notna().all(), \"audio_features.id contains NA\")\n",
    "assert_gate(cleaned[\"audio_features\"][\"id\"].is_unique, \"audio_features.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"albums\"][\"id\"].notna().all(), \"albums.id contains NA\")\n",
    "assert_gate(cleaned[\"albums\"][\"id\"].is_unique, \"albums.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"artists\"][\"id\"].notna().all(), \"artists.id contains NA\")\n",
    "assert_gate(cleaned[\"artists\"][\"id\"].is_unique, \"artists.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"genres\"][\"id\"].notna().all(), \"genres.id contains NA\")\n",
    "assert_gate(cleaned[\"genres\"][\"id\"].is_unique, \"genres.id not unique\")\n",
    "\n",
    "# Range sanity\n",
    "if \"popularity\" in cleaned[\"tracks\"].columns:\n",
    "    assert_gate(cleaned[\"tracks\"][\"popularity\"].dropna().between(0, 100).all(), \"tracks.popularity out of [0,100]\")\n",
    "\n",
    "if \"duration\" in cleaned[\"tracks\"].columns:\n",
    "    assert_gate((cleaned[\"tracks\"][\"duration\"].dropna() > 0).all(), \"tracks.duration has non-positive values\")\n",
    "\n",
    "for c in POLICY.audio_01_cols:\n",
    "    if c in cleaned[\"audio_features\"].columns:\n",
    "        assert_gate(cleaned[\"audio_features\"][c].dropna().between(0.0, 1.0).all(), f\"audio_features.{c} out of [0,1]\")\n",
    "\n",
    "# Bridge referential integrity\n",
    "if \"r_albums_tracks\" in cleaned:\n",
    "    rat = cleaned[\"r_albums_tracks\"]\n",
    "    assert_gate(rat[\"album_id\"].isin(album_ids).all(), \"r_albums_tracks has invalid album_id\")\n",
    "    assert_gate(rat[\"track_id\"].isin(track_ids).all(), \"r_albums_tracks has invalid track_id\")\n",
    "\n",
    "if \"r_track_artist\" in cleaned:\n",
    "    rta = cleaned[\"r_track_artist\"]\n",
    "    assert_gate(rta[\"track_id\"].isin(track_ids).all(), \"r_track_artist has invalid track_id\")\n",
    "    assert_gate(rta[\"artist_id\"].isin(artist_ids).all(), \"r_track_artist has invalid artist_id\")\n",
    "\n",
    "if \"r_artist_genre\" in cleaned:\n",
    "    rag = cleaned[\"r_artist_genre\"]\n",
    "    assert_gate(rag[\"genre_id\"].isin(genre_ids).all(), \"r_artist_genre has invalid genre_id\")\n",
    "    assert_gate(rag[\"artist_id\"].isin(artist_ids).all(), \"r_artist_genre has invalid artist_id\")\n",
    "\n",
    "if \"r_albums_artists\" in cleaned:\n",
    "    raa = cleaned[\"r_albums_artists\"]\n",
    "    assert_gate(raa[\"album_id\"].isin(album_ids).all(), \"r_albums_artists has invalid album_id\")\n",
    "    assert_gate(raa[\"artist_id\"].isin(artist_ids).all(), \"r_albums_artists has invalid artist_id\")\n",
    "\n",
    "print(\" All quality gates passed.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# ============================================================\n",
    "# Save Clean Layer (CSV + Parquet)\n",
    "# ============================================================\n",
    "\n",
    "def save_table(df: pd.DataFrame, name: str):\n",
    "    csv_path = PATHS.clean_dir / f\"{name}.csv\"\n",
    "    pq_path = PATHS.parquet_dir / f\"{name}.parquet\"\n",
    "\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    # Parquet is the preferred format for Notebook 3 performance\n",
    "    df.to_parquet(pq_path, index=False)\n",
    "\n",
    "for name, df in cleaned.items():\n",
    "    save_table(df, name)\n",
    "\n",
    "print(\" Clean layer saved to:\")\n",
    "print(\" -\", PATHS.clean_dir)\n",
    "print(\" -\", PATHS.parquet_dir)"
   ],
   "id": "3e9cf50581c27799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All quality gates passed.\n",
      " Clean layer saved to:\n",
      " - ..\\data\\processed\\clean_csv\\slice_001\n",
      " - ..\\data\\processed\\parquet\\slice_001\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaning Report (JSON) - audit + reproducibility",
   "id": "d157538b7f19a45f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T19:19:06.928737Z",
     "start_time": "2026-01-05T19:19:06.918076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rowcount_delta: Dict[str, Dict[str, int]] = {}\n",
    "for name in cleaned.keys():\n",
    "    before = int(raw[name].shape[0]) if name in raw else 0\n",
    "    after = int(cleaned[name].shape[0])\n",
    "    rowcount_delta[name] = {\"before\": before, \"after\": after, \"delta\": after - before}\n",
    "\n",
    "report = {\n",
    "    \"run_meta\": RUN_META,\n",
    "    \"profiles_before\": profiles_before,\n",
    "    \"profiles_after\": profiles_after,\n",
    "    \"rowcount_delta\": rowcount_delta,\n",
    "    \"notes\": {\n",
    "        \"bridge_policy\": \"drop orphan rows (referential integrity enforced)\",\n",
    "        \"track_audio_feature_fk_policy\": \"invalid audio_feature_id set to NA (tracks not dropped)\",\n",
    "        \"popularity_policy\": \"clipped to [0,100]\",\n",
    "        \"audio_features_policy\": \"0..1 scalars clipped; loudness/key/mode validated\",\n",
    "        \"duplicate_policy\": \"kept the most complete row per PK\",\n",
    "        \"export_formats\": \"CSV + Parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "report_path = PATHS.report_path / \"cleaning_report.json\"\n",
    "report_path.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\" Cleaning report written:\", report_path)"
   ],
   "id": "6494a48ef8cab8fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning report written: ..\\data\\reports\\cleaning_and_rules\\slice_001\\cleaning_report.json\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
