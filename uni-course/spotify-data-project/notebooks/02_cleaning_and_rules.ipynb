{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02 — Cleaning & Rules\n",
    "\n",
    "## Zweck\n",
    "Dieses Notebook bereinigt den exportierten **verbundenen Subgraphen** (bis zu **300k Tracks**), erzwingt einen **stabilen Datenvertrag (Data Contract)** und erzeugt eine **ML-fertige Clean-Layer**.\n",
    "\n",
    "## Input\n",
    "- `../data/interim/converted_sqlite/*.csv`\n",
    "\n",
    "## Output\n",
    "- `../data/processed/clean_csv/*.csv`\n",
    "- `../data/processed/parquet/*.parquet`\n",
    "- `../data/reports/02_cleaning_and_rules/cleaning_report.json`\n",
    "\n",
    "## Erwartete Tabellen (aus dem Exporter)\n",
    "| Tabelle | Beschreibung | Schlüssel / Beziehung |\n",
    "|---|---|---|\n",
    "| `tracks` | Track-Stammdaten | PK: `track_id` |\n",
    "| `audio_features` | Audio-Features pro Track | PK: `id` |\n",
    "| `albums` | Album-Stammdaten | PK: `id` |\n",
    "| `artists` | Artist-Stammdaten | PK: `id` |\n",
    "| `genres` | Genre-Stammdaten | PK: `id` |\n",
    "| `r_albums_tracks` | Zuordnung Album ↔ Track | (`album_id`, `track_id`) |\n",
    "| `r_track_artist` | Zuordnung Track ↔ Artist | (`track_id`, `artist_id`) |\n",
    "| `r_artist_genre` | Zuordnung Artist ↔ Genre | (`genre_id`, `artist_id`) |\n",
    "| `r_albums_artists` | Zuordnung Album ↔ Artist | (`album_id`, `artist_id`)|\n",
    "\n",
    "## Ergebnis\n",
    "Am Ende steht eine **konsistente, validierte und reproduzierbare** Datenbasis:\n",
    "- bereinigte CSVs (für schnelle Inspektion),\n",
    "- Parquet (für effizientes Training/Batching),\n",
    "- ein JSON-Report mit Regeln, Checks und Statistiken.\n"
   ],
   "id": "98a8d858ee79f6ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "55db29e3093df49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:35.274570Z",
     "start_time": "2025-12-14T15:07:35.269494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproductibility\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ],
   "id": "d410767135122672",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config and Paths",
   "id": "e264bcee0e932d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:35.293660Z",
     "start_time": "2025-12-14T15:07:35.280578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class PipelinePaths:\n",
    "    raw_dir: Path = Path(\"../data/interim/converted_sqlite\")\n",
    "    clean_dir: Path = Path(\"../data/processed/clean_csv\")\n",
    "    parquet_dir: Path = Path(\"../data/processed/parquet\")\n",
    "    report_path: Path = Path(\"../data/reports/02_cleaning_and_rules\")\n",
    "\n",
    "PATHS = PipelinePaths()\n",
    "\n",
    "for p in [PATHS.clean_dir,PATHS.report_path,PATHS.raw_dir,PATHS.parquet_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CleaningPolicy:\n",
    "    drop_orphan_bridge_rows:bool = True\n",
    "    clip_popularity:bool = True\n",
    "    popularity_min:int = 0\n",
    "    popularity_max:int = 0\n",
    "    duration_cap_quantile:float = 0.999\n",
    "    tempo_cap_quantile:float = 0.999\n",
    "    audio_01_cols:Tuple[str,...] = (\n",
    "        \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n",
    "        \"liveness\", \"speechiness\", \"valence\"\n",
    "    )\n",
    "    loudness_range:Tuple[float,float] = (-60.0, 5.0)\n",
    "    key_range:Tuple[int,int] = (0,11)\n",
    "    mode_range: Tuple[int, int] = (0, 1)\n",
    "\n",
    "\n",
    "POLICY = CleaningPolicy()\n",
    "\n",
    "RUN_META = {\n",
    "    \"run_ts_unix\": int(time.time()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {k: str(v) for k, v in asdict(PATHS).items()},\n",
    "    \"policy\": asdict(POLICY),\n",
    "}"
   ],
   "id": "261977f3b858ad58",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Utilities",
   "id": "18b5febcdf6e2067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:35.307692Z",
     "start_time": "2025-12-14T15:07:35.298924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def snake_case(s:str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w]+\",\"_\",s)\n",
    "    s = re.sub(r\"__+\",\"_\",s)\n",
    "    return s.strip(\"_\").lower()\n",
    "\n",
    "def norm_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize whitespace and empty strings to NA.\"\"\"\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    s = s.replace(\"\", pd.NA)\n",
    "    return s\n",
    "def to_int(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def to_float(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "def to_bool(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust boolean parser to pandas BooleanDtype.\"\"\"\n",
    "    x = s.astype(\"string\").str.lower().str.strip()\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "    out[x.isin([\"1\", \"true\", \"t\", \"yes\", \"y\"])] = True\n",
    "    out[x.isin([\"0\", \"false\", \"f\", \"no\", \"n\"])] = False\n",
    "    return out\n",
    "\n",
    "def memory_mb(df: pd.DataFrame) -> float:\n",
    "    return float(df.memory_usage(deep=True).sum()) / (1024 ** 2)\n",
    "\n",
    "def keep_most_complete_row(df: pd.DataFrame, key_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Resolve duplicates by keeping the row with most non-null values.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"_nonnulls\"] = df.notna().sum(axis=1)\n",
    "    df = df.sort_values(\"_nonnulls\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    df = df.drop(columns=[\"_nonnulls\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def clip_series(s: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def parse_date_any(s: pd.Series) -> pd.Series:\n",
    "    # Handles YYYY, YYYY-MM, YYYY-MM-DD (Spotify style)\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def assert_gate(condition: bool, msg: str):\n",
    "    if not condition:\n",
    "        raise AssertionError(f\"QUALITY GATE FAILED: {msg}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TableProfile:\n",
    "    name: str\n",
    "    rows: int\n",
    "    cols: int\n",
    "    memory_mb: float\n",
    "    missing_by_col: Dict[str, int]\n",
    "    duplicate_rows_full: Optional[int] = None\n",
    "    duplicate_rows_on_keys: Optional[int] = None\n",
    "\n",
    "def profile_table(df: pd.DataFrame, name: str, key_cols: Optional[List[str]] = None) -> TableProfile:\n",
    "    missing = {c: int(df[c].isna().sum()) for c in df.columns}\n",
    "    prof = TableProfile(\n",
    "        name=name,\n",
    "        rows=int(len(df)),\n",
    "        cols=int(df.shape[1]),\n",
    "        memory_mb=round(memory_mb(df), 2),\n",
    "        missing_by_col=missing,\n",
    "    )\n",
    "    if key_cols:\n",
    "        prof.duplicate_rows_on_keys = int(df.duplicated(subset=key_cols).sum())\n",
    "    else:\n",
    "        prof.duplicate_rows_full = int(df.duplicated().sum())\n",
    "    return prof"
   ],
   "id": "ae056591dfba8a81",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load CSV Exports",
   "id": "b688d7f79e78003c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:38.197680Z",
     "start_time": "2025-12-14T15:07:35.313664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "EXPECTED_FILES = {\n",
    "    \"tracks\": \"tracks.csv\",\n",
    "    \"audio_features\": \"audio_features.csv\",\n",
    "    \"albums\": \"albums.csv\",\n",
    "    \"artists\": \"artists.csv\",\n",
    "    \"genres\": \"genres.csv\",\n",
    "    \"r_albums_tracks\": \"r_albums_tracks.csv\",\n",
    "    \"r_track_artist\": \"r_track_artist.csv\",\n",
    "    \"r_artist_genre\": \"r_artist_genre.csv\",\n",
    "    \"r_albums_artists\": \"r_albums_artists.csv\",\n",
    "}\n",
    "\n",
    "missing_files = [k for k, f in EXPECTED_FILES.items() if not (PATHS.raw_dir / f).exists()]\n",
    "if missing_files:\n",
    "    print(\" Missing CSV files: \", missing_files)\n",
    "else:\n",
    "    print(\"All expected CSV exports found !\")\n",
    "\n",
    "def load_csv(name: str) -> pd.DataFrame:\n",
    "    fp = PATHS.raw_dir / EXPECTED_FILES[name]\n",
    "    df = pd.read_csv(fp, low_memory=False)\n",
    "    df.columns = [snake_case(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "raw: Dict[str, pd.DataFrame] = {}\n",
    "for table in EXPECTED_FILES:\n",
    "    fp = PATHS.raw_dir / EXPECTED_FILES[table]\n",
    "    if fp.exists():\n",
    "        raw[table] = load_csv(table)\n",
    "\n",
    "{k: v.shape for k, v in raw.items()}"
   ],
   "id": "2c0b0f9031f9d812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All expected CSV exports found !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tracks': (294618, 10),\n",
       " 'audio_features': (294594, 15),\n",
       " 'albums': (129152, 6),\n",
       " 'artists': (139608, 4),\n",
       " 'genres': (5416, 1),\n",
       " 'r_albums_tracks': (305933, 2),\n",
       " 'r_track_artist': (391700, 2),\n",
       " 'r_artist_genre': (169289, 2),\n",
       " 'r_albums_artists': (147070, 2)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drop useless / high-missing columns",
   "id": "4b72e739ba9a886d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:38.236680Z",
     "start_time": "2025-12-14T15:07:38.212285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if \"tracks\" in raw:\n",
    "    raw[\"tracks\"] = raw[\"tracks\"].drop(columns=[\"is_playable\"], errors=\"ignore\")\n",
    "\n",
    "# albums: drop column \"album_group\" (100% missing)\n",
    "if \"albums\" in raw:\n",
    "    raw[\"albums\"] = raw[\"albums\"].drop(columns=[\"album_group\"], errors=\"ignore\")\n",
    "\n",
    "# OPTIONAL (recommended): keep only an indicator and drop URL column\n",
    "# (URLs are high-cardinality and not useful directly as a text feature in sklearn tabular models)\n",
    "if \"tracks\" in raw and \"preview_url\" in raw[\"tracks\"].columns:\n",
    "    raw[\"tracks\"][\"has_preview\"] = raw[\"tracks\"][\"preview_url\"].notna().astype(\"int8\")\n",
    "    raw[\"tracks\"] = raw[\"tracks\"].drop(columns=[\"preview_url\"], errors=\"ignore\")"
   ],
   "id": "a6fa3c60ca82f7d3",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre - profiles",
   "id": "b5f2e92ce5947814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:39.476581Z",
     "start_time": "2025-12-14T15:07:38.244751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "profiles_before: Dict[str, Any] = {}\n",
    "\n",
    "if \"tracks\" in raw:\n",
    "    profiles_before[\"tracks\"] = asdict(profile_table(raw[\"tracks\"], \"tracks\", key_cols=[\"track_id\"] if \"track_id\" in raw[\"tracks\"].columns else [\"id\"]))\n",
    "if \"audio_features\" in raw:\n",
    "    profiles_before[\"audio_features\"] = asdict(profile_table(raw[\"audio_features\"], \"audio_features\", key_cols=[\"id\"]))\n",
    "if \"albums\" in raw:\n",
    "    profiles_before[\"albums\"] = asdict(profile_table(raw[\"albums\"], \"albums\", key_cols=[\"id\"]))\n",
    "if \"artists\" in raw:\n",
    "    profiles_before[\"artists\"] = asdict(profile_table(raw[\"artists\"], \"artists\", key_cols=[\"id\"]))\n",
    "if \"genres\" in raw:\n",
    "    profiles_before[\"genres\"] = asdict(profile_table(raw[\"genres\"], \"genres\", key_cols=[\"id\"]))\n",
    "\n",
    "# bridges\n",
    "if \"r_albums_tracks\" in raw:\n",
    "    profiles_before[\"r_albums_tracks\"] = asdict(profile_table(raw[\"r_albums_tracks\"], \"r_albums_tracks\", key_cols=[\"album_id\",\"track_id\"]))\n",
    "if \"r_track_artist\" in raw:\n",
    "    profiles_before[\"r_track_artist\"] = asdict(profile_table(raw[\"r_track_artist\"], \"r_track_artist\", key_cols=[\"track_id\",\"artist_id\"]))\n",
    "if \"r_artist_genre\" in raw:\n",
    "    profiles_before[\"r_artist_genre\"] = asdict(profile_table(raw[\"r_artist_genre\"], \"r_artist_genre\", key_cols=[\"genre_id\",\"artist_id\"]))\n",
    "if \"r_albums_artists\" in raw:\n",
    "    profiles_before[\"r_albums_artists\"] = asdict(profile_table(raw[\"r_albums_artists\"], \"r_albums_artists\", key_cols=[\"album_id\",\"artist_id\"]))\n",
    "\n",
    "pd.DataFrame.from_dict(profiles_before, orient=\"index\")[[\"rows\",\"cols\",\"memory_mb\",\"duplicate_rows_on_keys\"]]"
   ],
   "id": "9d6d31ca41654348",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    rows  cols  memory_mb  duplicate_rows_on_keys\n",
       "tracks            294618     9      71.29                       0\n",
       "audio_features    294594    15      80.91                       0\n",
       "albums            129152     5      26.50                       0\n",
       "artists           139608     4      19.98                       0\n",
       "genres              5416     1       0.33                       0\n",
       "r_albums_tracks   305933     2      41.43                       0\n",
       "r_track_artist    391700     2      53.04                       0\n",
       "r_artist_genre    169289     2      21.42                       0\n",
       "r_albums_artists  147070     2      19.92                       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>cols</th>\n",
       "      <th>memory_mb</th>\n",
       "      <th>duplicate_rows_on_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tracks</th>\n",
       "      <td>294618</td>\n",
       "      <td>9</td>\n",
       "      <td>71.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_features</th>\n",
       "      <td>294594</td>\n",
       "      <td>15</td>\n",
       "      <td>80.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albums</th>\n",
       "      <td>129152</td>\n",
       "      <td>5</td>\n",
       "      <td>26.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artists</th>\n",
       "      <td>139608</td>\n",
       "      <td>4</td>\n",
       "      <td>19.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genres</th>\n",
       "      <td>5416</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_tracks</th>\n",
       "      <td>305933</td>\n",
       "      <td>2</td>\n",
       "      <td>41.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_track_artist</th>\n",
       "      <td>391700</td>\n",
       "      <td>2</td>\n",
       "      <td>53.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_artist_genre</th>\n",
       "      <td>169289</td>\n",
       "      <td>2</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_artists</th>\n",
       "      <td>147070</td>\n",
       "      <td>2</td>\n",
       "      <td>19.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaning Classes",
   "id": "7bd691c8c257c336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:39.570604Z",
     "start_time": "2025-12-14T15:07:39.548704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseCleaner:\n",
    "    name:str\n",
    "\n",
    "    def clean(self,df : pd.DataFrame) -> pd.DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TracksCleaner(BaseCleaner):\n",
    "    name = \"tracks\"\n",
    "\n",
    "    def clean(self,df : pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        # Ensure PK column name: track_id\n",
    "        if \"track_id\" not in df.columns and \"id\" in df.columns:\n",
    "            df = df.rename(columns={\"id\": \"track_id\"})\n",
    "\n",
    "        # Normalize strings\n",
    "        df[\"track_id\"] = norm_str(df[\"track_id\"])\n",
    "        df = df[df[\"track_id\"].notna()]\n",
    "\n",
    "        for c in [\"name\", \"preview_url\", \"audio_feature_id\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        # Numerics\n",
    "        for c in [\"disc_number\", \"track_number\", \"duration\", \"popularity\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = to_int(df[c])\n",
    "\n",
    "        # Booleans\n",
    "        if \"explicit\" in df.columns:\n",
    "            df[\"explicit\"] = to_bool(df[\"explicit\"])\n",
    "        if \"is_playable\" in df.columns:\n",
    "            df[\"is_playable\"] = to_bool(df[\"is_playable\"])\n",
    "\n",
    "        # Rules: duration > 0, cap extremes\n",
    "        if \"duration\" in df.columns:\n",
    "            df.loc[df[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "            cap = df[\"duration\"].dropna().quantile(POLICY.duration_cap_quantile) if df[\"duration\"].notna().any() else None\n",
    "            if cap and not math.isnan(cap):\n",
    "                df[\"duration\"] = df[\"duration\"].clip(upper=int(cap))\n",
    "\n",
    "        # Rules: popularity in [0, 100]\n",
    "        if \"popularity\" in df.columns and POLICY.clip_popularity:\n",
    "            df[\"popularity\"] = clip_series(df[\"popularity\"], POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        # Dedupe by track_id keeping the most complete row\n",
    "        df = keep_most_complete_row(df, [\"track_id\"])\n",
    "        return df\n",
    "\n",
    "class AudioFeaturesCleaner(BaseCleaner):\n",
    "    name = \"audio_features\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        # normalize URL-like or text columns\n",
    "        for c in [\"analysis_url\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        # floats\n",
    "        for c in POLICY.audio_01_cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = clip_series(to_float(df[c]), 0.0, 1.0)\n",
    "\n",
    "        # tempo\n",
    "        if \"tempo\" in df.columns:\n",
    "            df[\"tempo\"] = to_float(df[\"tempo\"])\n",
    "            df.loc[df[\"tempo\"] <= 0, \"tempo\"] = pd.NA\n",
    "            cap = df[\"tempo\"].dropna().quantile(POLICY.tempo_cap_quantile) if df[\"tempo\"].notna().any() else None\n",
    "            if cap and not math.isnan(cap):\n",
    "                df[\"tempo\"] = df[\"tempo\"].clip(upper=float(cap))\n",
    "\n",
    "        # loudness\n",
    "        if \"loudness\" in df.columns:\n",
    "            df[\"loudness\"] = to_float(df[\"loudness\"])\n",
    "            lo, hi = POLICY.loudness_range\n",
    "            df.loc[~df[\"loudness\"].between(lo, hi), \"loudness\"] = pd.NA\n",
    "\n",
    "        # ints\n",
    "        for c in [\"key\", \"mode\", \"time_signature\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = to_int(df[c])\n",
    "\n",
    "        if \"key\" in df.columns:\n",
    "            lo, hi = POLICY.key_range\n",
    "            df.loc[~df[\"key\"].between(lo, hi), \"key\"] = pd.NA\n",
    "\n",
    "        if \"mode\" in df.columns:\n",
    "            lo, hi = POLICY.mode_range\n",
    "            df.loc[~df[\"mode\"].between(lo, hi), \"mode\"] = pd.NA\n",
    "\n",
    "        # duration (audio_features.duration is float in your export)\n",
    "        if \"duration\" in df.columns:\n",
    "            df[\"duration\"] = to_float(df[\"duration\"])\n",
    "            df.loc[df[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class AlbumsCleaner(BaseCleaner):\n",
    "    name = \"albums\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        for c in [\"name\", \"album_group\", \"album_type\", \"release_date\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = norm_str(df[c])\n",
    "\n",
    "        if \"album_group\" in df.columns:\n",
    "            df[\"album_group\"] = df[\"album_group\"].str.lower()\n",
    "        if \"album_type\" in df.columns:\n",
    "            df[\"album_type\"] = df[\"album_type\"].str.lower()\n",
    "\n",
    "        if \"popularity\" in df.columns:\n",
    "            df[\"popularity\"] = clip_series(to_int(df[\"popularity\"]), POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        # parsed date (keep both)\n",
    "        if \"release_date\" in df.columns:\n",
    "            df[\"release_date_parsed\"] = parse_date_any(df[\"release_date\"])\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class ArtistsCleaner(BaseCleaner):\n",
    "    name = \"artists\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()]\n",
    "\n",
    "        if \"name\" in df.columns:\n",
    "            df[\"name\"] = norm_str(df[\"name\"])\n",
    "\n",
    "        if \"popularity\" in df.columns:\n",
    "            df[\"popularity\"] = clip_series(to_int(df[\"popularity\"]), POLICY.popularity_min, POLICY.popularity_max)\n",
    "\n",
    "        if \"followers\" in df.columns:\n",
    "            df[\"followers\"] = to_int(df[\"followers\"])\n",
    "            df[\"followers\"] = df[\"followers\"].clip(lower=0)\n",
    "\n",
    "        df = keep_most_complete_row(df, [\"id\"])\n",
    "        return df\n",
    "\n",
    "class GenresCleaner(BaseCleaner):\n",
    "    name = \"genres\"\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        # your genres export only has \"id\"\n",
    "        df[\"id\"] = norm_str(df[\"id\"])\n",
    "        df = df[df[\"id\"].notna()].drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "class BridgeCleaner(BaseCleaner):\n",
    "    \"\"\"Generic bridge cleaner for composite keys + string normalization + dedupe.\"\"\"\n",
    "    def __init__(self, name: str, key_cols: List[str]):\n",
    "        self.name = name\n",
    "        self.key_cols = key_cols\n",
    "\n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for c in self.key_cols:\n",
    "            df[c] = norm_str(df[c])\n",
    "        df = df.dropna(subset=self.key_cols)\n",
    "        df = df.drop_duplicates(subset=self.key_cols, keep=\"first\").reset_index(drop=True)\n",
    "        return df\n",
    "\n"
   ],
   "id": "9a5235ac4d94105",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Applying the cleaning",
   "id": "eac88fafd74fd286"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:48.996208Z",
     "start_time": "2025-12-14T15:07:39.597584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleaned: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "# Entities\n",
    "if \"tracks\" in raw:\n",
    "    cleaned[\"tracks\"] = TracksCleaner().clean(raw[\"tracks\"])\n",
    "if \"audio_features\" in raw:\n",
    "    cleaned[\"audio_features\"] = AudioFeaturesCleaner().clean(raw[\"audio_features\"])\n",
    "if \"albums\" in raw:\n",
    "    cleaned[\"albums\"] = AlbumsCleaner().clean(raw[\"albums\"])\n",
    "if \"artists\" in raw:\n",
    "    cleaned[\"artists\"] = ArtistsCleaner().clean(raw[\"artists\"])\n",
    "if \"genres\" in raw:\n",
    "    cleaned[\"genres\"] = GenresCleaner().clean(raw[\"genres\"])\n",
    "\n",
    "# Bridges\n",
    "if \"r_albums_tracks\" in raw:\n",
    "    cleaned[\"r_albums_tracks\"] = BridgeCleaner(\"r_albums_tracks\", [\"album_id\",\"track_id\"]).clean(raw[\"r_albums_tracks\"])\n",
    "if \"r_track_artist\" in raw:\n",
    "    cleaned[\"r_track_artist\"] = BridgeCleaner(\"r_track_artist\", [\"track_id\",\"artist_id\"]).clean(raw[\"r_track_artist\"])\n",
    "if \"r_artist_genre\" in raw:\n",
    "    cleaned[\"r_artist_genre\"] = BridgeCleaner(\"r_artist_genre\", [\"genre_id\",\"artist_id\"]).clean(raw[\"r_artist_genre\"])\n",
    "if \"r_albums_artists\" in raw:\n",
    "    cleaned[\"r_albums_artists\"] = BridgeCleaner(\"r_albums_artists\", [\"album_id\",\"artist_id\"]).clean(raw[\"r_albums_artists\"])\n",
    "\n",
    "{k: v.shape for k, v in cleaned.items()}\n"
   ],
   "id": "a2b1922c65d18af3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19476\\468345948.py:44: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(s, errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tracks': (294618, 9),\n",
       " 'audio_features': (294594, 15),\n",
       " 'albums': (129152, 6),\n",
       " 'artists': (139608, 4),\n",
       " 'genres': (5416, 1),\n",
       " 'r_albums_tracks': (305933, 2),\n",
       " 'r_track_artist': (391700, 2),\n",
       " 'r_artist_genre': (169289, 2),\n",
       " 'r_albums_artists': (147070, 2)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Referential Integrity Enforcement"
   ],
   "id": "b8c3148e6d0e3959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:50.631741Z",
     "start_time": "2025-12-14T15:07:49.004454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build ID sets\n",
    "track_ids = set(cleaned[\"tracks\"][\"track_id\"].unique()) if \"tracks\" in cleaned else set()\n",
    "album_ids = set(cleaned[\"albums\"][\"id\"].unique()) if \"albums\" in cleaned else set()\n",
    "artist_ids = set(cleaned[\"artists\"][\"id\"].unique()) if \"artists\" in cleaned else set()\n",
    "genre_ids = set(cleaned[\"genres\"][\"id\"].unique()) if \"genres\" in cleaned else set()\n",
    "af_ids = set(cleaned[\"audio_features\"][\"id\"].unique()) if \"audio_features\" in cleaned else set()\n",
    "\n",
    "def filter_fk(df: pd.DataFrame, col: str, allowed: set) -> pd.DataFrame:\n",
    "    return df[df[col].isin(allowed)].copy()\n",
    "\n",
    "# Bridge FK cleanup\n",
    "if POLICY.drop_orphan_bridge_rows:\n",
    "    if \"r_albums_tracks\" in cleaned:\n",
    "        rat = cleaned[\"r_albums_tracks\"]\n",
    "        rat = filter_fk(filter_fk(rat, \"album_id\", album_ids), \"track_id\", track_ids)\n",
    "        cleaned[\"r_albums_tracks\"] = rat.reset_index(drop=True)\n",
    "\n",
    "    if \"r_track_artist\" in cleaned:\n",
    "        rta = cleaned[\"r_track_artist\"]\n",
    "        rta = filter_fk(filter_fk(rta, \"track_id\", track_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_track_artist\"] = rta.reset_index(drop=True)\n",
    "\n",
    "    if \"r_artist_genre\" in cleaned:\n",
    "        rag = cleaned[\"r_artist_genre\"]\n",
    "        rag = filter_fk(filter_fk(rag, \"genre_id\", genre_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_artist_genre\"] = rag.reset_index(drop=True)\n",
    "\n",
    "    if \"r_albums_artists\" in cleaned:\n",
    "        raa = cleaned[\"r_albums_artists\"]\n",
    "        raa = filter_fk(filter_fk(raa, \"album_id\", album_ids), \"artist_id\", artist_ids)\n",
    "        cleaned[\"r_albums_artists\"] = raa.reset_index(drop=True)\n",
    "\n",
    "# Track -> audio_feature_id FK policy: set invalid to NA (do NOT drop track)\n",
    "if \"tracks\" in cleaned and \"audio_feature_id\" in cleaned[\"tracks\"].columns and af_ids:\n",
    "    bad = cleaned[\"tracks\"][\"audio_feature_id\"].notna() & ~cleaned[\"tracks\"][\"audio_feature_id\"].isin(af_ids)\n",
    "    cleaned[\"tracks\"].loc[bad, \"audio_feature_id\"] = pd.NA\n",
    "\n",
    "{k: v.shape for k, v in cleaned.items()}"
   ],
   "id": "fc15949062984921",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tracks': (294618, 9),\n",
       " 'audio_features': (294594, 15),\n",
       " 'albums': (129152, 6),\n",
       " 'artists': (139608, 4),\n",
       " 'genres': (5416, 1),\n",
       " 'r_albums_tracks': (305933, 2),\n",
       " 'r_track_artist': (391700, 2),\n",
       " 'r_artist_genre': (169289, 2),\n",
       " 'r_albums_artists': (142153, 2)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Post Cleaning",
   "id": "13a8fe12bcebad24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:51.967212Z",
     "start_time": "2025-12-14T15:07:50.638277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiles_after: Dict[str, Any] = {}\n",
    "\n",
    "profiles_after[\"tracks\"] = asdict(profile_table(cleaned[\"tracks\"], \"tracks\", key_cols=[\"track_id\"]))\n",
    "profiles_after[\"audio_features\"] = asdict(profile_table(cleaned[\"audio_features\"], \"audio_features\", key_cols=[\"id\"]))\n",
    "profiles_after[\"albums\"] = asdict(profile_table(cleaned[\"albums\"], \"albums\", key_cols=[\"id\"]))\n",
    "profiles_after[\"artists\"] = asdict(profile_table(cleaned[\"artists\"], \"artists\", key_cols=[\"id\"]))\n",
    "profiles_after[\"genres\"] = asdict(profile_table(cleaned[\"genres\"], \"genres\", key_cols=[\"id\"]))\n",
    "\n",
    "if \"r_albums_tracks\" in cleaned:\n",
    "    profiles_after[\"r_albums_tracks\"] = asdict(profile_table(cleaned[\"r_albums_tracks\"], \"r_albums_tracks\", key_cols=[\"album_id\",\"track_id\"]))\n",
    "if \"r_track_artist\" in cleaned:\n",
    "    profiles_after[\"r_track_artist\"] = asdict(profile_table(cleaned[\"r_track_artist\"], \"r_track_artist\", key_cols=[\"track_id\",\"artist_id\"]))\n",
    "if \"r_artist_genre\" in cleaned:\n",
    "    profiles_after[\"r_artist_genre\"] = asdict(profile_table(cleaned[\"r_artist_genre\"], \"r_artist_genre\", key_cols=[\"genre_id\",\"artist_id\"]))\n",
    "if \"r_albums_artists\" in cleaned:\n",
    "    profiles_after[\"r_albums_artists\"] = asdict(profile_table(cleaned[\"r_albums_artists\"], \"r_albums_artists\", key_cols=[\"album_id\",\"artist_id\"]))\n",
    "\n",
    "pd.DataFrame.from_dict(profiles_after, orient=\"index\")[[\"rows\",\"cols\",\"memory_mb\",\"duplicate_rows_on_keys\"]]\n"
   ],
   "id": "dfdb828144d91e3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    rows  cols  memory_mb  duplicate_rows_on_keys\n",
       "tracks            294618     9      70.73                       0\n",
       "audio_features    294594    15      81.76                       0\n",
       "albums            129152     6      34.24                       0\n",
       "artists           139608     4      20.24                       0\n",
       "genres              5416     1       0.33                       0\n",
       "r_albums_tracks   305933     2      41.43                       0\n",
       "r_track_artist    391700     2      53.04                       0\n",
       "r_artist_genre    169289     2      21.42                       0\n",
       "r_albums_artists  142153     2      19.25                       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>cols</th>\n",
       "      <th>memory_mb</th>\n",
       "      <th>duplicate_rows_on_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tracks</th>\n",
       "      <td>294618</td>\n",
       "      <td>9</td>\n",
       "      <td>70.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio_features</th>\n",
       "      <td>294594</td>\n",
       "      <td>15</td>\n",
       "      <td>81.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albums</th>\n",
       "      <td>129152</td>\n",
       "      <td>6</td>\n",
       "      <td>34.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artists</th>\n",
       "      <td>139608</td>\n",
       "      <td>4</td>\n",
       "      <td>20.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genres</th>\n",
       "      <td>5416</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_tracks</th>\n",
       "      <td>305933</td>\n",
       "      <td>2</td>\n",
       "      <td>41.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_track_artist</th>\n",
       "      <td>391700</td>\n",
       "      <td>2</td>\n",
       "      <td>53.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_artist_genre</th>\n",
       "      <td>169289</td>\n",
       "      <td>2</td>\n",
       "      <td>21.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_albums_artists</th>\n",
       "      <td>142153</td>\n",
       "      <td>2</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outlier & rule-based validation + flags\n",
    "\n",
    "\n",
    "In diesem Schritt wurden **keine Outlier pauschal gelöscht** (z. B. via IQR), weil viele Spalten in den Spotify-Daten **diskret**, **zero-inflated** oder **heavy-tail** verteilt sind. IQR wäre hier oft **zu aggressiv** und würde viele **gültige Spezialfälle** (z. B. Multi-Disc, Live-Tracks, Rap/Speech) fälschlich als Outlier markieren.\n",
    "Stattdessen nutzen wir ein **Contract-/Rule-based Cleaning**:\n",
    "\n",
    "1. **Ungültige Werte (Domain-Verletzung)** → auf `NaN` setzen oder clippen\n",
    "2. **Extreme, aber plausible Werte (Long-Tail)** → **Quantile-Capping** (z. B. 0.1% / 99.9%) statt Dropping\n",
    "3. **Signal behalten** → zusätzliche **Flag-Features** (`is_*`) für Extremfälle\n",
    "\n",
    "---\n",
    "\n",
    "#### Tracks – Regeln & Outlier-Handling\n",
    "\n",
    "- **popularity**: auf **[0,100]** clippen (Spotify-Definition), keine Zeilen löschen\n",
    "- **duration**: `<= 0` → `NaN`, außerdem **99.9%-Quantile-Cap** (Upper Tail); Flag **`is_long_track`**\n",
    "- **track_number**: diskret/albumabhängig → **Rule-based**: `<=0` und `>200` → `NaN`; Flag **`is_tracknum_extreme`**\n",
    "- **disc_number**: meist 1 (IQR wäre falsch) → Flag **`is_multidisc`** (`>1`), extreme Werte `>10` → `NaN`; Flag **`is_disc_extreme`**\n",
    "\n",
    "**Warum?**\n",
    "IQR markiert bei fast-konstanten/discreten Spalten (disc_number, track_number) zu viele gültige Fälle. Quantile-Cap ist robuster für Long-Tail-Variablen (duration).\n",
    "\n",
    "---\n",
    "\n",
    "#### Audio Features – Regeln & Outlier-Handling\n",
    "\n",
    "- **time_signature**: als **kategorial** behandeln; nur `{3,4,5}` gültig, sonst `NaN`; Flag **`is_time_signature_rare`**\n",
    "- **tempo**: `<=0` → `NaN`; **99.9%-Quantile-Cap**; Flag **`is_tempo_extreme`**\n",
    "- **loudness**: außerhalb **[-60, 5]** → `NaN` (Domain); Flag **`is_loudness_very_low`** (`<-40`)\n",
    "- **speechiness**: [0,1] clippen; Flag **`is_high_speech`** (>= 90%-Quantil)\n",
    "- **instrumentalness**: [0,1] clippen; Flag **`is_instrumental`** (>= 0.5)\n",
    "- **key/mode**: streng validieren (`key` 0–11, `mode` 0–1), sonst `NaN`\n",
    "\n",
    "**Warum?**\n",
    "Viele Audio-Features sind **bounded** (0..1) oder **kategorial** (time_signature, key/mode). Bei skew/zero-inflation (speechiness, instrumentalness) ist IQR ungeeignet → Flags + ggf. log/bins später.\n",
    "\n",
    "---\n",
    "\n",
    "#### Artists – Regeln & Outlier-Handling\n",
    "\n",
    "- **followers**: negative → `NaN`; **log1p-Feature** (`followers_log1p`) + Flag **`is_followers_extreme`** (99.9%-Quantil)\n",
    "- **artist popularity**: auf **[0,100]** clippen\n",
    "\n",
    "**Warum?**\n",
    "Followers sind stark **heavy-tail** (Superstars vs. Long Tail). Log-Transform ist Standard und stabilisiert Modelle.\n",
    "\n",
    "---\n",
    "\n",
    "#### Albums – Regeln & Outlier-Handling\n",
    "\n",
    "- **release_date**: in `datetime` konvertieren und **Year-Range** validieren (z. B. 1900–2035); invalid → `NaT`; Flag **`is_release_year_invalid`**\n",
    "- **album popularity**: auf **[0,100]** clippen\n",
    "\n",
    "**Warum?**\n",
    "Release-Daten können vereinzelt kaputt sein; wir **löschen keine Alben**, sondern korrigieren/flaggen.\n",
    "\n",
    "---\n",
    "\n",
    "### Ergebnis / Vorteil\n",
    "- **Stabile Features** für ML (weniger extreme Hebelwerte)\n",
    "- **Keine Join-/Graph-Schäden** durch massives Dropping\n",
    "- **Wichtige Rare-Cases bleiben erhalten** (über `is_*` Flags)\n",
    "- Die anschließenden **Quality Gates** prüfen dann die finalen, regel-konformen Daten.\n"
   ],
   "id": "6a34af45001265a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:07:52.254131Z",
     "start_time": "2025-12-14T15:07:51.985369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# OUTLIERS & RULE-BASED VALIDATION (based on your observations)\n",
    "# - NO IQR dropping (too aggressive for skew/discrete/zero-inflated cols)\n",
    "# - Use rule-based invalid -> NA\n",
    "# - Use quantile caps for heavy tails\n",
    "# - Add flags to preserve signal (rare cases)\n",
    "# ============================================================\n",
    "\n",
    "def quantile_cap(series: pd.Series, q_low: float = 0.001, q_high: float = 0.999) -> tuple[float, float]:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(s.quantile(q_low)), float(s.quantile(q_high)))\n",
    "\n",
    "# -------------------------\n",
    "# TRACKS rules\n",
    "# -------------------------\n",
    "if \"tracks\" in cleaned:\n",
    "    t = cleaned[\"tracks\"].copy()\n",
    "\n",
    "    # popularity: domain range [0,100]\n",
    "    if \"popularity\" in t.columns:\n",
    "        t[\"popularity\"] = pd.to_numeric(t[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "        # optional: binning helper (nice for Notebook 3)\n",
    "        # t[\"popularity_bin\"] = pd.cut(t[\"popularity\"], bins=[-1,0,20,40,60,80,100],\n",
    "        #                              labels=[\"0\",\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\"])\n",
    "\n",
    "    # duration: invalid -> NA, quantile cap, long-track flag\n",
    "    if \"duration\" in t.columns:\n",
    "        t[\"duration\"] = pd.to_numeric(t[\"duration\"], errors=\"coerce\")\n",
    "        t.loc[t[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "\n",
    "        lo, hi = quantile_cap(t[\"duration\"], q_low=0.001, q_high=0.999)\n",
    "        # cap only upper tail; lower tail usually valid if >0\n",
    "        t[\"is_long_track\"] = (t[\"duration\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            t[\"duration\"] = t[\"duration\"].clip(upper=int(hi))\n",
    "\n",
    "    # track_number: discrete + album-structural -> rule-based, NOT IQR\n",
    "    if \"track_number\" in t.columns:\n",
    "        t[\"track_number\"] = pd.to_numeric(t[\"track_number\"], errors=\"coerce\")\n",
    "        # rule: <=0 invalid\n",
    "        t.loc[t[\"track_number\"] <= 0, \"track_number\"] = pd.NA\n",
    "        # rule: extremely high track numbers probably broken\n",
    "        t[\"is_tracknum_extreme\"] = (t[\"track_number\"] > 200).astype(\"int8\")\n",
    "        t.loc[t[\"track_number\"] > 200, \"track_number\"] = pd.NA\n",
    "\n",
    "    # disc_number: mostly 1; create is_multidisc, rule extreme\n",
    "    if \"disc_number\" in t.columns:\n",
    "        t[\"disc_number\"] = pd.to_numeric(t[\"disc_number\"], errors=\"coerce\")\n",
    "        t.loc[t[\"disc_number\"] <= 0, \"disc_number\"] = pd.NA\n",
    "        t[\"is_multidisc\"] = (t[\"disc_number\"] > 1).astype(\"int8\")\n",
    "        t[\"is_disc_extreme\"] = (t[\"disc_number\"] > 10).astype(\"int8\")\n",
    "        t.loc[t[\"disc_number\"] > 10, \"disc_number\"] = pd.NA\n",
    "\n",
    "    cleaned[\"tracks\"] = t\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# AUDIO FEATURES rules\n",
    "# -------------------------\n",
    "if \"audio_features\" in cleaned:\n",
    "    a = cleaned[\"audio_features\"].copy()\n",
    "\n",
    "    # time_signature: treat as categorical; rule-based set invalid to NA\n",
    "    if \"time_signature\" in a.columns:\n",
    "        a[\"time_signature\"] = pd.to_numeric(a[\"time_signature\"], errors=\"coerce\")\n",
    "        valid_ts = {3, 4, 5}\n",
    "        a[\"is_time_signature_rare\"] = (~a[\"time_signature\"].isin(list(valid_ts))).astype(\"int8\")\n",
    "        a.loc[~a[\"time_signature\"].isin(list(valid_ts)), \"time_signature\"] = pd.NA\n",
    "\n",
    "    # tempo: invalid -> NA, quantile cap, keep rare fast/slow\n",
    "    if \"tempo\" in a.columns:\n",
    "        a[\"tempo\"] = pd.to_numeric(a[\"tempo\"], errors=\"coerce\")\n",
    "        a.loc[a[\"tempo\"] <= 0, \"tempo\"] = pd.NA\n",
    "        lo, hi = quantile_cap(a[\"tempo\"], q_low=0.001, q_high=0.999)\n",
    "        a[\"is_tempo_extreme\"] = ((a[\"tempo\"] < lo) | (a[\"tempo\"] > hi)).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            a[\"tempo\"] = a[\"tempo\"].clip(upper=hi)\n",
    "\n",
    "    # loudness: domain sanity; very low values likely bad\n",
    "    if \"loudness\" in a.columns:\n",
    "        a[\"loudness\"] = pd.to_numeric(a[\"loudness\"], errors=\"coerce\")\n",
    "        # \"hard\" domain guard\n",
    "        a.loc[~a[\"loudness\"].between(-60, 5), \"loudness\"] = pd.NA\n",
    "        # additional flag: suspiciously low (often data issue)\n",
    "        a[\"is_loudness_very_low\"] = (a[\"loudness\"] < -40).astype(\"int8\")\n",
    "\n",
    "    # duration in audio_features\n",
    "    if \"duration\" in a.columns:\n",
    "        a[\"duration\"] = pd.to_numeric(a[\"duration\"], errors=\"coerce\")\n",
    "        a.loc[a[\"duration\"] <= 0, \"duration\"] = pd.NA\n",
    "        _, hi = quantile_cap(a[\"duration\"], q_low=0.001, q_high=0.999)\n",
    "        a[\"is_af_long\"] = (a[\"duration\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        if not np.isnan(hi):\n",
    "            a[\"duration\"] = a[\"duration\"].clip(upper=hi)\n",
    "\n",
    "    # speechiness + instrumentalness: skew / zero inflation -> flags + (optional) log transform later\n",
    "    if \"speechiness\" in a.columns:\n",
    "        a[\"speechiness\"] = pd.to_numeric(a[\"speechiness\"], errors=\"coerce\").clip(0, 1)\n",
    "        a[\"is_high_speech\"] = (a[\"speechiness\"] >= a[\"speechiness\"].dropna().quantile(0.90)).astype(\"int8\") \\\n",
    "                              if a[\"speechiness\"].notna().any() else 0\n",
    "\n",
    "    if \"instrumentalness\" in a.columns:\n",
    "        a[\"instrumentalness\"] = pd.to_numeric(a[\"instrumentalness\"], errors=\"coerce\").clip(0, 1)\n",
    "        # classic threshold used often in practice\n",
    "        a[\"is_instrumental\"] = (a[\"instrumentalness\"] >= 0.5).astype(\"int8\")\n",
    "\n",
    "    # key/mode: categorical sanity\n",
    "    if \"key\" in a.columns:\n",
    "        a[\"key\"] = pd.to_numeric(a[\"key\"], errors=\"coerce\")\n",
    "        a.loc[~a[\"key\"].between(0, 11), \"key\"] = pd.NA\n",
    "\n",
    "    if \"mode\" in a.columns:\n",
    "        a[\"mode\"] = pd.to_numeric(a[\"mode\"], errors=\"coerce\")\n",
    "        a.loc[~a[\"mode\"].between(0, 1), \"mode\"] = pd.NA\n",
    "\n",
    "    cleaned[\"audio_features\"] = a\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ARTISTS rules\n",
    "# -------------------------\n",
    "if \"artists\" in cleaned:\n",
    "    ar = cleaned[\"artists\"].copy()\n",
    "\n",
    "    # followers: heavy tail -> do not drop, add log feature and extreme flag\n",
    "    if \"followers\" in ar.columns:\n",
    "        ar[\"followers\"] = pd.to_numeric(ar[\"followers\"], errors=\"coerce\")\n",
    "        ar.loc[ar[\"followers\"] < 0, \"followers\"] = pd.NA\n",
    "        _, hi = quantile_cap(ar[\"followers\"], q_low=0.001, q_high=0.999)\n",
    "        ar[\"is_followers_extreme\"] = (ar[\"followers\"] > hi).astype(\"int8\") if not np.isnan(hi) else 0\n",
    "        ar[\"followers_log1p\"] = np.log1p(ar[\"followers\"].fillna(0)).astype(\"float64\")\n",
    "\n",
    "    # popularity: bounded [0,100] and can be binned later\n",
    "    if \"popularity\" in ar.columns:\n",
    "        ar[\"popularity\"] = pd.to_numeric(ar[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "    cleaned[\"artists\"] = ar\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ALBUMS rules\n",
    "# -------------------------\n",
    "if \"albums\" in cleaned:\n",
    "    al = cleaned[\"albums\"].copy()\n",
    "\n",
    "    # release_date: validate year range; keep flag instead of dropping\n",
    "    # Notebook 2 already parsed release_date_parsed; enforce year sanity\n",
    "    if \"release_date_parsed\" in al.columns:\n",
    "        years = pd.to_datetime(al[\"release_date_parsed\"], errors=\"coerce\").dt.year\n",
    "        # conservative sanity range (Spotify catalog)\n",
    "        al[\"is_release_year_invalid\"] = ((years < 1900) | (years > 2035)).astype(\"int8\")\n",
    "        al.loc[(years < 1900) | (years > 2035), \"release_date_parsed\"] = pd.NaT\n",
    "\n",
    "        # add derived year features early (optional)\n",
    "        al[\"release_year\"] = pd.to_datetime(al[\"release_date_parsed\"], errors=\"coerce\").dt.year.astype(\"Int64\")\n",
    "\n",
    "    # album popularity: clip\n",
    "    if \"popularity\" in al.columns:\n",
    "        al[\"popularity\"] = pd.to_numeric(al[\"popularity\"], errors=\"coerce\").clip(0, 100)\n",
    "\n",
    "    cleaned[\"albums\"] = al\n",
    "\n",
    "\n",
    "print(\"Outlier rules applied: rule-based invalidation + quantile caps + flags (no IQR dropping).\")\n"
   ],
   "id": "26c86f36354987c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier rules applied: rule-based invalidation + quantile caps + flags (no IQR dropping).\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quality Gates",
   "id": "d621e738ad2aa172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:08:01.797943Z",
     "start_time": "2025-12-14T15:07:52.263945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PKs must be unique and non-null\n",
    "assert_gate(cleaned[\"tracks\"][\"track_id\"].notna().all(), \"tracks.track_id contains NA\")\n",
    "assert_gate(cleaned[\"tracks\"][\"track_id\"].is_unique, \"tracks.track_id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"audio_features\"][\"id\"].notna().all(), \"audio_features.id contains NA\")\n",
    "assert_gate(cleaned[\"audio_features\"][\"id\"].is_unique, \"audio_features.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"albums\"][\"id\"].notna().all(), \"albums.id contains NA\")\n",
    "assert_gate(cleaned[\"albums\"][\"id\"].is_unique, \"albums.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"artists\"][\"id\"].notna().all(), \"artists.id contains NA\")\n",
    "assert_gate(cleaned[\"artists\"][\"id\"].is_unique, \"artists.id not unique\")\n",
    "\n",
    "assert_gate(cleaned[\"genres\"][\"id\"].notna().all(), \"genres.id contains NA\")\n",
    "assert_gate(cleaned[\"genres\"][\"id\"].is_unique, \"genres.id not unique\")\n",
    "\n",
    "# Range sanity\n",
    "if \"popularity\" in cleaned[\"tracks\"].columns:\n",
    "    assert_gate(cleaned[\"tracks\"][\"popularity\"].dropna().between(0, 100).all(), \"tracks.popularity out of [0,100]\")\n",
    "\n",
    "if \"duration\" in cleaned[\"tracks\"].columns:\n",
    "    assert_gate((cleaned[\"tracks\"][\"duration\"].dropna() > 0).all(), \"tracks.duration has non-positive values\")\n",
    "\n",
    "for c in POLICY.audio_01_cols:\n",
    "    if c in cleaned[\"audio_features\"].columns:\n",
    "        assert_gate(cleaned[\"audio_features\"][c].dropna().between(0.0, 1.0).all(), f\"audio_features.{c} out of [0,1]\")\n",
    "\n",
    "# Bridge referential integrity\n",
    "if \"r_albums_tracks\" in cleaned:\n",
    "    rat = cleaned[\"r_albums_tracks\"]\n",
    "    assert_gate(rat[\"album_id\"].isin(album_ids).all(), \"r_albums_tracks has invalid album_id\")\n",
    "    assert_gate(rat[\"track_id\"].isin(track_ids).all(), \"r_albums_tracks has invalid track_id\")\n",
    "\n",
    "if \"r_track_artist\" in cleaned:\n",
    "    rta = cleaned[\"r_track_artist\"]\n",
    "    assert_gate(rta[\"track_id\"].isin(track_ids).all(), \"r_track_artist has invalid track_id\")\n",
    "    assert_gate(rta[\"artist_id\"].isin(artist_ids).all(), \"r_track_artist has invalid artist_id\")\n",
    "\n",
    "if \"r_artist_genre\" in cleaned:\n",
    "    rag = cleaned[\"r_artist_genre\"]\n",
    "    assert_gate(rag[\"genre_id\"].isin(genre_ids).all(), \"r_artist_genre has invalid genre_id\")\n",
    "    assert_gate(rag[\"artist_id\"].isin(artist_ids).all(), \"r_artist_genre has invalid artist_id\")\n",
    "\n",
    "if \"r_albums_artists\" in cleaned:\n",
    "    raa = cleaned[\"r_albums_artists\"]\n",
    "    assert_gate(raa[\"album_id\"].isin(album_ids).all(), \"r_albums_artists has invalid album_id\")\n",
    "    assert_gate(raa[\"artist_id\"].isin(artist_ids).all(), \"r_albums_artists has invalid artist_id\")\n",
    "\n",
    "print(\" All quality gates passed.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# ============================================================\n",
    "# Save Clean Layer (CSV + Parquet)\n",
    "# ============================================================\n",
    "\n",
    "def save_table(df: pd.DataFrame, name: str):\n",
    "    csv_path = PATHS.clean_dir / f\"{name}.csv\"\n",
    "    pq_path = PATHS.parquet_dir / f\"{name}.parquet\"\n",
    "\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    # Parquet is the preferred format for Notebook 3 performance\n",
    "    df.to_parquet(pq_path, index=False)\n",
    "\n",
    "for name, df in cleaned.items():\n",
    "    save_table(df, name)\n",
    "\n",
    "print(\" Clean layer saved to:\")\n",
    "print(\" -\", PATHS.clean_dir)\n",
    "print(\" -\", PATHS.parquet_dir)"
   ],
   "id": "3e9cf50581c27799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All quality gates passed.\n",
      " Clean layer saved to:\n",
      " - ..\\data\\processed\\clean_csv\n",
      " - ..\\data\\processed\\parquet\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaning Report (JSON) - audit + reproducibility",
   "id": "d157538b7f19a45f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T15:08:01.815083Z",
     "start_time": "2025-12-14T15:08:01.807719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rowcount_delta: Dict[str, Dict[str, int]] = {}\n",
    "for name in cleaned.keys():\n",
    "    before = int(raw[name].shape[0]) if name in raw else 0\n",
    "    after = int(cleaned[name].shape[0])\n",
    "    rowcount_delta[name] = {\"before\": before, \"after\": after, \"delta\": after - before}\n",
    "\n",
    "report = {\n",
    "    \"run_meta\": RUN_META,\n",
    "    \"profiles_before\": profiles_before,\n",
    "    \"profiles_after\": profiles_after,\n",
    "    \"rowcount_delta\": rowcount_delta,\n",
    "    \"notes\": {\n",
    "        \"bridge_policy\": \"drop orphan rows (referential integrity enforced)\",\n",
    "        \"track_audio_feature_fk_policy\": \"invalid audio_feature_id set to NA (tracks not dropped)\",\n",
    "        \"popularity_policy\": \"clipped to [0,100]\",\n",
    "        \"audio_features_policy\": \"0..1 scalars clipped; loudness/key/mode validated\",\n",
    "        \"duplicate_policy\": \"kept the most complete row per PK\",\n",
    "        \"export_formats\": \"CSV + Parquet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "report_path = PATHS.report_path / \"cleaning_report.json\"\n",
    "report_path.write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\" Cleaning report written:\", report_path)"
   ],
   "id": "6494a48ef8cab8fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning report written: ..\\data\\reports\\02_cleaning_and_rules\\cleaning_report.json\n"
     ]
    }
   ],
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
