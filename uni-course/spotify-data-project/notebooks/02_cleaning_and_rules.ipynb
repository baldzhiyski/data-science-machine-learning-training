{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02 — Cleaning & Rules\n",
    "\n",
    "## Zweck\n",
    "Dieses Notebook bereinigt den exportierten **verbundenen Subgraphen** (bis zu **300k Tracks**), erzwingt einen **stabilen Datenvertrag (Data Contract)** und erzeugt eine **ML-fertige Clean-Layer**.\n",
    "\n",
    "## Input\n",
    "- `../data/interim/converted_sqlite/*.csv`\n",
    "\n",
    "## Output\n",
    "- `../data/processed/clean_csv/*.csv`\n",
    "- `../data/processed/parquet/*.parquet`\n",
    "- `../data/reports/02_cleaning_and_rules/cleaning_report.json`\n",
    "\n",
    "## Erwartete Tabellen (aus dem Exporter)\n",
    "| Tabelle | Beschreibung | Schlüssel / Beziehung |\n",
    "|---|---|---|\n",
    "| `tracks` | Track-Stammdaten | PK: `track_id` |\n",
    "| `audio_features` | Audio-Features pro Track | PK: `id` |\n",
    "| `albums` | Album-Stammdaten | PK: `id` |\n",
    "| `artists` | Artist-Stammdaten | PK: `id` |\n",
    "| `genres` | Genre-Stammdaten | PK: `id` |\n",
    "| `r_albums_tracks` | Zuordnung Album ↔ Track | (`album_id`, `track_id`) |\n",
    "| `r_track_artist` | Zuordnung Track ↔ Artist | (`track_id`, `artist_id`) |\n",
    "| `r_artist_genre` | Zuordnung Artist ↔ Genre | (`genre_id`, `artist_id`) |\n",
    "| `r_albums_artists` | Zuordnung Album ↔ Artist | (`album_id`, `artist_id`)|\n",
    "\n",
    "## Ergebnis\n",
    "Am Ende steht eine **konsistente, validierte und reproduzierbare** Datenbasis:\n",
    "- bereinigte CSVs (für schnelle Inspektion),\n",
    "- Parquet (für effizientes Training/Batching),\n",
    "- ein JSON-Report mit Regeln, Checks und Statistiken.\n"
   ],
   "id": "98a8d858ee79f6ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "55db29e3093df49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T14:22:47.313695Z",
     "start_time": "2025-12-14T14:22:47.309273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproductibility\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ],
   "id": "d410767135122672",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config and Paths",
   "id": "e264bcee0e932d71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T14:28:03.264425Z",
     "start_time": "2025-12-14T14:28:03.245522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class PipelinePaths:\n",
    "    raw_dir: Path = Path(\"../data/interim/converted_sqlite\")\n",
    "    clean_dir: Path = Path(\"../data/processed/clean_csv\")\n",
    "    parquet_dir: Path = Path(\"../data/processed/parquet\")\n",
    "    report_path: Path = Path(\"../data/reports/02_cleaning_and_rules\")\n",
    "\n",
    "PATHS = PipelinePaths()\n",
    "\n",
    "for p in [PATHS.clean_dir,PATHS.report_path,PATHS.raw_dir,PATHS.parquet_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CleaningPolicy:\n",
    "    drop_orphan_bridge_rows:bool = True\n",
    "    clip_popularity:bool = True\n",
    "    popularity_min:int = 0\n",
    "    popularity_max:int = 0\n",
    "    duration_cap_quantile:float = 0.999\n",
    "    tempo_cap_quantile:float = 0.999\n",
    "    audio_01_cols:Tuple[str,...] = (\n",
    "        \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n",
    "        \"liveness\", \"speechiness\", \"valence\"\n",
    "    )\n",
    "    loudness_range:Tuple[float,float] = (-60.0, 5.0)\n",
    "    key_range:Tuple[int,int] = (0,11)\n",
    "    mode_values:Tuple[int,int] = (0,1)\n",
    "\n",
    "\n",
    "POLICY = CleaningPolicy()\n",
    "\n",
    "RUN_META = {\n",
    "    \"run_ts_unix\": int(time.time()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {k: str(v) for k, v in asdict(PATHS).items()},\n",
    "    \"policy\": asdict(POLICY),\n",
    "}"
   ],
   "id": "261977f3b858ad58",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Utilities",
   "id": "18b5febcdf6e2067"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def snake_case(s:str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w]+\",\"_\",s)\n",
    "    s = re.sub(r\"__+\",\"_\",s)\n",
    "    return s.strip(\"_\").lower()\n",
    "\n",
    "def norm_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize whitespace and empty strings to NA.\"\"\"\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    s = s.replace(\"\", pd.NA)\n",
    "    return s\n",
    "def to_int(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def to_float(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "def to_bool(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust boolean parser to pandas BooleanDtype.\"\"\"\n",
    "    x = s.astype(\"string\").str.lower().str.strip()\n",
    "    out = pd.Series(pd.NA, index=s.index, dtype=\"boolean\")\n",
    "    out[x.isin([\"1\", \"true\", \"t\", \"yes\", \"y\"])] = True\n",
    "    out[x.isin([\"0\", \"false\", \"f\", \"no\", \"n\"])] = False\n",
    "    return out\n",
    "\n",
    "def memory_mb(df: pd.DataFrame) -> float:\n",
    "    return float(df.memory_usage(deep=True).sum()) / (1024 ** 2)\n",
    "\n",
    "def keep_most_complete_row(df: pd.DataFrame, key_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Resolve duplicates by keeping the row with most non-null values.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"_nonnulls\"] = df.notna().sum(axis=1)\n",
    "    df = df.sort_values(\"_nonnulls\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    df = df.drop(columns=[\"_nonnulls\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def clip_series(s: pd.Series, lo: float, hi: float) -> pd.Series:\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def parse_date_any(s: pd.Series) -> pd.Series:\n",
    "    # Handles YYYY, YYYY-MM, YYYY-MM-DD (Spotify style)\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def assert_gate(condition: bool, msg: str):\n",
    "    if not condition:\n",
    "        raise AssertionError(f\"QUALITY GATE FAILED: {msg}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TableProfile:\n",
    "    name: str\n",
    "    rows: int\n",
    "    cols: int\n",
    "    memory_mb: float\n",
    "    missing_by_col: Dict[str, int]\n",
    "    duplicate_rows_full: Optional[int] = None\n",
    "    duplicate_rows_on_keys: Optional[int] = None\n",
    "\n",
    "def profile_table(df: pd.DataFrame, name: str, key_cols: Optional[List[str]] = None) -> TableProfile:\n",
    "    missing = {c: int(df[c].isna().sum()) for c in df.columns}\n",
    "    prof = TableProfile(\n",
    "        name=name,\n",
    "        rows=int(len(df)),\n",
    "        cols=int(df.shape[1]),\n",
    "        memory_mb=round(memory_mb(df), 2),\n",
    "        missing_by_col=missing,\n",
    "    )\n",
    "    if key_cols:\n",
    "        prof.duplicate_rows_on_keys = int(df.duplicated(subset=key_cols).sum())\n",
    "    else:\n",
    "        prof.duplicate_rows_full = int(df.duplicated().sum())\n",
    "    return prof"
   ],
   "id": "ae056591dfba8a81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
