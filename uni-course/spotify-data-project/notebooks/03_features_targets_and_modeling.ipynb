{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 03 — Target & Features with  scikit-learn\n",
    "\n",
    "## Ziel\n",
    "Dieses Notebook erstellt **professionelle ML-Datensätze** aus der **Clean-Layer** (Notebook 02) und trainiert mehrere Modelle **ausschließlich im scikit-learn-Ökosystem**.\n",
    "Am Ende stehen reproduzierbare Datasets, gespeicherte Pipelines/Modelle und strukturierte Reports.\n",
    "\n",
    "---\n",
    "\n",
    "## Anforderungen / Aufgaben\n",
    "Dieses Notebook deckt folgende ML-Use-Cases ab:\n",
    "\n",
    "1. **Track-Popularität** (Regression)\n",
    "2. **Album-Popularität** (Regression)\n",
    "3. **Hit-Prediction** (Binary Classification)\n",
    "4. **Explicit / Content-Prediction** (Binary Classification)\n",
    "5. **Mood Tags** (Multi-Label Classification)\n",
    "   - Labels werden aus Features abgeleitet (Rule-based / Derived Labels)\n",
    "6. **Artist Clustering / Community Detection** (Unsupervised Learning)\n",
    "\n",
    "---\n",
    "\n",
    "## Input (Clean-Layer aus Notebook 02)\n",
    "Bevorzugt:\n",
    "- `../data/processed/parquet/*.parquet`\n",
    "\n",
    "Fallback:\n",
    "- `../data/processed/clean_csv/*.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "### 1) Modellierungs-Datasets (Parquet)\n",
    "- `../data/processed/modeling/track_dataset.parquet`\n",
    "- `../data/processed/modeling/album_dataset.parquet`\n",
    "- `../data/processed/modeling/artist_dataset.parquet`\n",
    "\n",
    "### 2) Gespeicherte Modelle & Pipelines (joblib)\n",
    "- `../data/models/03_track_popularity_pipeline.joblib`\n",
    "- `../data/models/03_album_popularity_pipeline.joblib`\n",
    "- `../data/models/03_hit_pipeline.joblib`\n",
    "- `../data/models/03_explicit_pipeline.joblib`\n",
    "- `../data/models/03_mood_pipeline.joblib`\n",
    "- `../data/models/03_artist_clustering.joblib`\n",
    "\n",
    "### 3) Konfiguration & Reports\n",
    "- `../data/models/feature_config.json`\n",
    "- `../data/reports/03_target_and_features/*.json`\n",
    "\n",
    "---\n",
    "\n",
    "## Ergebnis\n",
    "Nach dem Notebook existieren:\n",
    "- modellierungsfertige Parquet-Datasets,\n",
    "- trainierte und gespeicherte scikit-learn Pipelines,\n",
    "- sowie Reports/Configs für nachvollziehbares Training und spätere Batch-Inferenz.\n"
   ],
   "id": "9672396cc52b06e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:19.224828Z",
     "start_time": "2025-12-14T17:22:19.217970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import platform\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from joblib import dump"
   ],
   "id": "c8a583208350f2c1",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Config",
   "id": "af6c0da03f435c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:19.248760Z",
     "start_time": "2025-12-14T17:22:19.244264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Leakage controls:\n",
    "# - If True: allow \"post-release / popularity-like\" proxy features (often boosts scores but less realistic)\n",
    "# - If False: drop strongest leakage/proxies (recommended for realistic evaluation)\n",
    "ALLOW_LEAKY_FEATURES = False\n",
    "\n",
    "# \"Main album per track\" selection strategy:\n",
    "# - \"earliest_release\": choose album with earliest release_date_parsed\n",
    "# - \"deterministic_id\": choose smallest album_id (stable fallback)\n",
    "MAIN_ALBUM_STRATEGY = \"earliest_release\"\n",
    "\n",
    "# Hit label definition\n",
    "HIT_PERCENTILE = 0.80\n",
    "HIT_FALLBACK_POP_THRESHOLD = 60\n",
    "\n",
    "# Genre multi-hot size\n",
    "TOP_K_GENRES = 50\n",
    "\n",
    "# Mood labels quantile rules (weak-label demonstration)\n",
    "MOOD_TAGS = [\n",
    "    (\"energetic\", \"energy\", 0.75, \"gt\"),\n",
    "    (\"danceable\", \"danceability\", 0.75, \"gt\"),\n",
    "    (\"acoustic\", \"acousticness\", 0.75, \"gt\"),\n",
    "    (\"instrumental\", \"instrumentalness\", 0.75, \"gt\"),\n",
    "    (\"happy\", \"valence\", 0.75, \"gt\"),\n",
    "    (\"sad\", \"valence\", 0.25, \"lt\"),\n",
    "    (\"chill\", \"energy\", 0.25, \"lt\"),\n",
    "]\n",
    "\n",
    "# Clustering\n",
    "K_CLUSTERS = 30\n",
    "TSNE_SAMPLE_MAX = 4000\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 250)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "try:\n",
    "    pd.options.mode.copy_on_write = True\n",
    "except Exception:\n",
    "    pass"
   ],
   "id": "4ab690c2419648a4",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paths",
   "id": "b6bb50c3a614111d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:19.264001Z",
     "start_time": "2025-12-14T17:22:19.256054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    clean_parquet_dir: Path = Path(\"../data/processed/parquet\")\n",
    "    clean_csv_dir: Path = Path(\"../data/processed/clean_csv\")\n",
    "\n",
    "    modeling_dir: Path = Path(\"../data/processed/modeling\")\n",
    "    models_dir: Path = Path(\"../data/models\")\n",
    "    reports_dir: Path = Path(\"../data/reports/03_target_and_features\")\n",
    "\n",
    "PATHS = Paths()\n",
    "for p in [PATHS.modeling_dir, PATHS.models_dir, PATHS.reports_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_META = {\n",
    "    \"run_ts_unix\": int(time.time()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"allow_leaky_features\": ALLOW_LEAKY_FEATURES,\n",
    "    \"main_album_strategy\": MAIN_ALBUM_STRATEGY,\n",
    "    \"paths\": {k: str(v) for k, v in asdict(PATHS).items()},\n",
    "}"
   ],
   "id": "c9edd2734a912c84",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading",
   "id": "f5740e6b93204582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:19.831502Z",
     "start_time": "2025-12-14T17:22:19.270730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TABLES = [\n",
    "    \"tracks\",\n",
    "    \"audio_features\",\n",
    "    \"albums\",\n",
    "    \"artists\",\n",
    "    \"genres\",\n",
    "    \"r_albums_tracks\",\n",
    "    \"r_track_artist\",\n",
    "    \"r_artist_genre\",\n",
    "    \"r_albums_artists\",\n",
    "]\n",
    "\n",
    "def load_table(name: str) -> pd.DataFrame:\n",
    "    pq = PATHS.clean_parquet_dir / f\"{name}.parquet\"\n",
    "    csv = PATHS.clean_csv_dir / f\"{name}.csv\"\n",
    "\n",
    "    if pq.exists():\n",
    "        return pd.read_parquet(pq)\n",
    "    if csv.exists():\n",
    "        return pd.read_csv(csv, low_memory=False)\n",
    "    raise FileNotFoundError(f\"Missing {name} in parquet/csv clean layer.\")\n",
    "\n",
    "data: Dict[str, pd.DataFrame] = {}\n",
    "for t in TABLES:\n",
    "    pq = PATHS.clean_parquet_dir / f\"{t}.parquet\"\n",
    "    csv = PATHS.clean_csv_dir / f\"{t}.csv\"\n",
    "    if pq.exists() or csv.exists():\n",
    "        data[t] = load_table(t)\n",
    "\n",
    "{k: v.shape for k, v in data.items()}"
   ],
   "id": "d9405634cbfbecdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tracks': (294618, 13),\n",
       " 'audio_features': (294594, 21),\n",
       " 'albums': (129152, 8),\n",
       " 'artists': (139608, 6),\n",
       " 'genres': (5416, 1),\n",
       " 'r_albums_tracks': (305933, 2),\n",
       " 'r_track_artist': (391700, 2),\n",
       " 'r_artist_genre': (169289, 2),\n",
       " 'r_albums_artists': (142153, 2)}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quick integrity sanity",
   "id": "a4817b7804f2c03b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:20.070176Z",
     "start_time": "2025-12-14T17:22:19.848913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "required = [\"tracks\", \"audio_features\", \"albums\", \"artists\", \"r_albums_tracks\", \"r_track_artist\", \"r_artist_genre\"]\n",
    "missing = [t for t in required if t not in data]\n",
    "assert not missing, f\"Missing required tables in clean layer: {missing}\"\n",
    "\n",
    "tracks = data[\"tracks\"].copy()\n",
    "audio = data[\"audio_features\"].copy()\n",
    "albums = data[\"albums\"].copy()\n",
    "artists = data[\"artists\"].copy()\n",
    "rat = data[\"r_albums_tracks\"].copy()\n",
    "rta = data[\"r_track_artist\"].copy()\n",
    "rag = data[\"r_artist_genre\"].copy()\n",
    "genres = data.get(\"genres\", pd.DataFrame(columns=[\"id\"]))  # optional\n",
    "raa = data.get(\"r_albums_artists\", pd.DataFrame(columns=[\"album_id\", \"artist_id\"])).copy()\n",
    "\n",
    "# PK expectations (guarded)\n",
    "assert \"track_id\" in tracks.columns, \"tracks must contain track_id\"\n",
    "assert tracks[\"track_id\"].is_unique\n",
    "\n",
    "assert \"id\" in audio.columns and audio[\"id\"].is_unique\n",
    "assert \"id\" in albums.columns and albums[\"id\"].is_unique\n",
    "assert \"id\" in artists.columns and artists[\"id\"].is_unique\n",
    "\n",
    "if not genres.empty and \"id\" in genres.columns:\n",
    "    assert genres[\"id\"].is_unique\n",
    "\n",
    "print(\"Clean layer looks consistent.\")"
   ],
   "id": "9a9f17ba7d8cecb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean layer looks consistent.\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper utilities",
   "id": "1a46c34d0eabf306"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:22:20.102022Z",
     "start_time": "2025-12-14T17:22:20.086797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def col_or_na(df: pd.DataFrame, col: str, dtype: Optional[str] = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return df[col] if it exists; otherwise return an all-NA Series with the same index.\n",
    "    Never returns None.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"col_or_na: df must be a pandas DataFrame\")\n",
    "\n",
    "    if col in df.columns:\n",
    "        s = df[col]\n",
    "        if dtype is not None:\n",
    "            try:\n",
    "                s = s.astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return s\n",
    "\n",
    "    return pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "def safe_len_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(\"string\").fillna(\"\").str.len().astype(\"int32\")\n",
    "\n",
    "def safe_word_count_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(\"string\").fillna(\"\").str.split().str.len().astype(\"int32\")\n",
    "\n",
    "def add_release_time_features(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Adds release_year/month/decade from a datetime-like column.\"\"\"\n",
    "    df = df.copy()\n",
    "    dt = pd.to_datetime(col_or_na(df, date_col), errors=\"coerce\")\n",
    "    df[\"release_year\"] = dt.dt.year.astype(\"Int64\")\n",
    "    df[\"release_month\"] = dt.dt.month.astype(\"Int64\")\n",
    "    df[\"release_decade\"] = ((dt.dt.year // 10) * 10).astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def log1p_numeric(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return np.log1p(x).astype(\"float64\")\n",
    "\n",
    "def ensure_list_column(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Ensure a column contains python lists.\n",
    "    Accepts:\n",
    "      - actual lists\n",
    "      - JSON strings\n",
    "      - repr strings like \"['a','b']\"\n",
    "      - NaN/None\n",
    "    \"\"\"\n",
    "    def parse_one(v):\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return []\n",
    "        if isinstance(v, str):\n",
    "            v = v.strip()\n",
    "            if not v:\n",
    "                return []\n",
    "            # try JSON\n",
    "            try:\n",
    "                parsed = json.loads(v)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except Exception:\n",
    "                pass\n",
    "            # try python literal\n",
    "            try:\n",
    "                parsed = ast.literal_eval(v)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except Exception:\n",
    "                pass\n",
    "        return []\n",
    "    return s.apply(parse_one)\n",
    "\n",
    "def top_k_list_counts(list_series: pd.Series, top_k: int) -> List[str]:\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for lst in list_series:\n",
    "        if isinstance(lst, list):\n",
    "            for x in lst:\n",
    "                if pd.notna(x):\n",
    "                    c[str(x)] += 1\n",
    "    return [k for k, _ in c.most_common(top_k)]\n",
    "\n",
    "def genres_to_multihot(df: pd.DataFrame, list_col: str, top_genres: List[str], prefix: str) -> pd.DataFrame:\n",
    "    if not top_genres:\n",
    "        return pd.DataFrame(index=df.index)\n",
    "    m = np.zeros((len(df), len(top_genres)), dtype=np.int8)\n",
    "    idx = {g: i for i, g in enumerate(top_genres)}\n",
    "    lists = df[list_col]\n",
    "    for r, lst in enumerate(lists):\n",
    "        if isinstance(lst, list):\n",
    "            for g in lst:\n",
    "                j = idx.get(str(g))\n",
    "                if j is not None:\n",
    "                    m[r, j] = 1\n",
    "    return pd.DataFrame(m, columns=[f\"{prefix}genre_{g}\" for g in top_genres])\n",
    "\n",
    "def onehot_encoder_compat() -> OneHotEncoder:\n",
    "    \"\"\"Handle sklearn versions where sparse_output may not exist.\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def kmeans_compat(n_clusters: int, random_state: int) -> KMeans:\n",
    "    \"\"\"Handle sklearn versions where n_init='auto' may not exist.\"\"\"\n",
    "    try:\n",
    "        return KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=random_state)\n",
    "    except TypeError:\n",
    "        return KMeans(n_clusters=n_clusters, n_init=10, random_state=random_state)\n",
    "\n",
    "def regression_report(y_true, y_pred) -> Dict[str, float]:\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    return {\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def classification_report_binary(y_true, y_proba, threshold=0.5) -> Dict[str, Any]:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    out = {\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, y_proba)) if len(np.unique(y_true)) > 1 else None,\n",
    "        \"pr_auc\": float(average_precision_score(y_true, y_proba)) if len(np.unique(y_true)) > 1 else None,\n",
    "        \"f1\": float(f1_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else None,\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "    }\n",
    "    return out"
   ],
   "id": "50becf1736ac7860",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Track-Level Dataset (eine Zeile = ein Track)\n",
    "\n",
    "**Ziel:** Wir bauen eine denormalisierte, ML-fertige Tabelle, in der **jede Zeile einen Track** repräsentiert.\n",
    "Dazu kombinieren wir Informationen aus mehreren Tabellen (Tracks, Audio-Features, Alben, Artists, Genres) und erzeugen zusätzlich **aggregierte** sowie **engineerte Features**.\n",
    "\n",
    "### Was passiert hier genau?\n",
    "\n",
    "1. **Tracks + Audio-Features (1:1 / left join)**\n",
    "   - Wir hängen die numerischen Audio-Features (z. B. energy, danceability, loudness, tempo) direkt an den Track.\n",
    "   - Falls für einzelne Tracks keine Audio-Features existieren, bleiben diese Felder `NaN` (left join).\n",
    "\n",
    "2. **Track → Album (Many-to-Many) und Auswahl eines „Main Albums“**\n",
    "   - Ein Track kann auf mehreren Alben vorkommen (Album, Compilation, Re-Release).\n",
    "   - Für ML brauchen wir aber **einen eindeutigen Album-Kontext** pro Track.\n",
    "   - Deshalb wählen wir deterministisch genau **ein Album pro Track** (z. B. das früheste Release-Datum).\n",
    "\n",
    "3. **Album-Metadaten an Track anhängen**\n",
    "   - Wir mergen Album-Infos (z. B. album_type, release_date, album_popularity) auf Track-Ebene.\n",
    "   - Danach erzeugen wir Zeitfeatures wie `release_year`, `release_month`, `release_decade`.\n",
    "\n",
    "4. **Track → Artists (Many-to-Many) + Aggregation**\n",
    "   - Ein Track kann mehrere Artists haben (feat., collabs).\n",
    "   - Wir speichern:\n",
    "     - `artist_ids` als Liste (für spätere Analysen)\n",
    "     - Aggregierte Artist-Statistiken pro Track:\n",
    "       - Anzahl Artists (`n_artists`)\n",
    "       - Mittelwert/Maximum von Artist-Popularität und Followers\n",
    "\n",
    "5. **Track → Genres über Artist-Genres (Many-to-Many, Union)**\n",
    "   - Genres hängen bei Spotify oft an Artists, nicht direkt an Tracks.\n",
    "   - Wir bauen:\n",
    "     - `artist_id -> [genre_ids]`\n",
    "     - `track_id -> union(artist_genres)` als `track_genres` (Liste)\n",
    "\n",
    "6. **Feature Engineering**\n",
    "   - Aus Text / Metadaten:\n",
    "     - `has_preview`: ob Preview-URL vorhanden ist (0/1)\n",
    "     - `name_len`, `name_words`: Länge und Wortanzahl des Track-Namens\n",
    "   - Log-Transforms:\n",
    "     - `log_duration`: reduziert Schiefe bei Dauer\n",
    "     - `log_artist_followers_*`: stabilisiert heavy-tailed follower counts\n",
    "   - Qualitätsindikatoren:\n",
    "     - `has_audio_features`: ob Audio-Features vorhanden sind (0/1)\n",
    "\n",
    "**Ergebnis:** `track_df` ist eine „Feature-Matrix“ auf Track-Ebene"
   ],
   "id": "4779449637d47f0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:00.602007Z",
     "start_time": "2025-12-14T17:22:20.109833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Join: tracks -> audio_features (left join)\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Audio features are core ML predictors (danceability, energy, loudness, tempo, ...)\n",
    "#   - Left join keeps all tracks even if audio features are missing for some rows.\n",
    "assert \"audio_feature_id\" in tracks.columns, \"tracks must contain audio_feature_id for join with audio_features.id\"\n",
    "\n",
    "# Rename audio PK 'id' to match tracks FK 'audio_feature_id'\n",
    "audio_small = audio.rename(columns={\"id\": \"audio_feature_id\"})\n",
    "\n",
    "# Merge track metadata + audio features into one wide table\n",
    "track_df = tracks.merge(audio_small, on=\"audio_feature_id\", how=\"left\", suffixes=(\"\", \"_af\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Track -> Album (Many-to-Many) and choose ONE \"main album\"\n",
    "# ------------------------------------------------------------\n",
    "# Problem:\n",
    "#   - A track can appear on multiple albums (releases, compilations, deluxe editions).\n",
    "# ML requirement:\n",
    "#   - We want a single album context per track to avoid duplicate rows / ambiguity.\n",
    "# Strategy:\n",
    "#   - Deterministic selection:\n",
    "#       MAIN_ALBUM_STRATEGY == \"earliest_release\" -> pick earliest release_date\n",
    "#       else -> pick smallest album_id (stable fallback)\n",
    "albums_for_pick = albums.copy().rename(columns={\"id\": \"album_id\"})\n",
    "albums_for_pick[\"release_date_parsed\"] = pd.to_datetime(\n",
    "    col_or_na(albums_for_pick, \"release_date_parsed\"), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Attach album release dates to the relationship table (album_id, track_id)\n",
    "rat2 = rat.merge(\n",
    "    albums_for_pick[[\"album_id\", \"release_date_parsed\"]],\n",
    "    on=\"album_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Pick main album per track based on strategy\n",
    "if MAIN_ALBUM_STRATEGY == \"earliest_release\":\n",
    "    rat2 = rat2.sort_values([\"track_id\", \"release_date_parsed\", \"album_id\"], ascending=[True, True, True])\n",
    "    main_album_per_track = rat2.drop_duplicates(\"track_id\", keep=\"first\")[[\"track_id\", \"album_id\"]]\n",
    "else:\n",
    "    rat2 = rat2.sort_values([\"track_id\", \"album_id\"], ascending=[True, True])\n",
    "    main_album_per_track = rat2.drop_duplicates(\"track_id\", keep=\"first\")[[\"track_id\", \"album_id\"]]\n",
    "\n",
    "# Merge the selected main album_id into track_df\n",
    "track_df = track_df.merge(main_album_per_track, on=\"track_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Merge album metadata onto track\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - album_type / release_date provide useful context\n",
    "#   - album_popularity is a strong proxy but can be leakage depending on your goal\n",
    "#     (you can later drop it via ALLOW_LEAKY_FEATURES switch in feature selection)\n",
    "albums_join = albums_for_pick.copy()\n",
    "\n",
    "# Rename to avoid name clash with track popularity\n",
    "rename_map = {}\n",
    "if \"popularity\" in albums_join.columns:\n",
    "    rename_map[\"popularity\"] = \"album_popularity\"\n",
    "albums_join = albums_join.rename(columns=rename_map)\n",
    "\n",
    "track_df = track_df.merge(albums_join, on=\"album_id\", how=\"left\", suffixes=(\"\", \"_album\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Add release time features (year/month/decade)\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Popularity and audio trends are time-dependent\n",
    "#   - Helps model capture temporal shift and era effects\n",
    "track_df = add_release_time_features(track_df, \"release_date_parsed\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Track -> Artists list (Many-to-Many)\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - A track can have multiple artists\n",
    "#   - Keeping a list can be useful for later analysis/debugging\n",
    "track_to_artists = (\n",
    "    rta.groupby(\"track_id\")[\"artist_id\"]\n",
    "       .apply(list)\n",
    "       .reset_index()\n",
    "       .rename(columns={\"artist_id\": \"artist_ids\"})\n",
    ")\n",
    "track_df = track_df.merge(track_to_artists, on=\"track_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Aggregate artist statistics per track\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Artist popularity/followers often correlate with track reach\n",
    "#   - For multi-artist tracks, we aggregate to stable numeric features\n",
    "artist_feat = artists.rename(\n",
    "    columns={\"id\": \"artist_id\", \"popularity\": \"artist_popularity\", \"followers\": \"artist_followers\"}\n",
    ")\n",
    "rta_art = rta.merge(artist_feat, on=\"artist_id\", how=\"left\")\n",
    "\n",
    "artist_agg = (\n",
    "    rta_art.groupby(\"track_id\")\n",
    "           .agg(\n",
    "               n_artists=(\"artist_id\", \"nunique\"),\n",
    "               artist_popularity_mean=(\"artist_popularity\", \"mean\"),\n",
    "               artist_popularity_max=(\"artist_popularity\", \"max\"),\n",
    "               artist_followers_mean=(\"artist_followers\", \"mean\"),\n",
    "               artist_followers_max=(\"artist_followers\", \"max\"),\n",
    "           )\n",
    "           .reset_index()\n",
    ")\n",
    "track_df = track_df.merge(artist_agg, on=\"track_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Track -> Genres (via artist genres), union per track\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Genres are usually attached to artists\n",
    "#   - We derive a track-level genre profile by taking the union across its artists\n",
    "#\n",
    "# Note:\n",
    "#   - We store genre IDs because they are stable keys.\n",
    "#   - Later you can convert to names or multi-hot encode Top-K genres.\n",
    "rag2 = rag.copy()\n",
    "if \"genre_id\" not in rag2.columns and \"id\" in rag2.columns:\n",
    "    rag2 = rag2.rename(columns={\"id\": \"genre_id\"})\n",
    "\n",
    "# Build artist -> [genre_id] list\n",
    "artist_to_genres = (\n",
    "    rag2.groupby(\"artist_id\")[\"genre_id\"]\n",
    "        .apply(lambda x: sorted(set([g for g in x.dropna().tolist()])))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"genre_id\": \"artist_genres\"})\n",
    ")\n",
    "\n",
    "# Join artist genres into track-artist mapping, then union genres per track\n",
    "rta_gen = rta.merge(artist_to_genres, on=\"artist_id\", how=\"left\")\n",
    "track_to_genres = (\n",
    "    rta_gen.groupby(\"track_id\")[\"artist_genres\"]\n",
    "           .apply(lambda rows: sorted(set([g for lst in rows.dropna()\n",
    "                                          for g in (lst if isinstance(lst, list) else [])])))\n",
    "           .reset_index()\n",
    "           .rename(columns={\"artist_genres\": \"track_genres\"})\n",
    ")\n",
    "track_df = track_df.merge(track_to_genres, on=\"track_id\", how=\"left\")\n",
    "\n",
    "# Ensure list type (important when loading from CSV where lists may become strings)\n",
    "track_df[\"track_genres\"] = ensure_list_column(col_or_na(track_df, \"track_genres\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Feature Engineering (binary flags, text-derived, log transforms)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Preview availability: a simple content/availability indicator\n",
    "track_df[\"has_preview\"] = col_or_na(track_df, \"preview_url\").notna().astype(\"int8\")\n",
    "\n",
    "# Track name features: cheap but sometimes useful\n",
    "track_df[\"name_len\"] = safe_len_series(col_or_na(track_df, \"name\"))\n",
    "track_df[\"name_words\"] = safe_word_count_series(col_or_na(track_df, \"name\"))\n",
    "\n",
    "# Duration robust handling: datasets often have duration_ms instead of duration\n",
    "dur_col = \"duration\" if \"duration\" in track_df.columns else (\"duration_ms\" if \"duration_ms\" in track_df.columns else None)\n",
    "track_df[\"log_duration\"] = log1p_numeric(track_df[dur_col]) if dur_col else pd.Series(np.nan, index=track_df.index)\n",
    "\n",
    "# Followers are heavy-tailed -> log helps stabilize scale\n",
    "track_df[\"log_artist_followers_max\"] = log1p_numeric(col_or_na(track_df, \"artist_followers_max\"))\n",
    "track_df[\"log_artist_followers_mean\"] = log1p_numeric(col_or_na(track_df, \"artist_followers_mean\"))\n",
    "\n",
    "# Indicator whether audio features are present (helps model handle missingness)\n",
    "track_df[\"has_audio_features\"] = col_or_na(track_df, \"audio_feature_id\").notna().astype(\"int8\")\n",
    "\n",
    "print(\"Track-level dataset shape:\", track_df.shape)\n",
    "track_df.head(3)"
   ],
   "id": "c878d632e22819d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track-level dataset shape: (294618, 56)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 track_id  disc_number  duration  explicit        audio_feature_id                                   name  track_number  popularity  has_preview  is_long_track  \\\n",
       "0  2DZN6ceJ7fMU2X6YWuIGHk            1    285053     False  2DZN6ceJ7fMU2X6YWuIGHk                     Toccada del 3 Tono            14           0            0              0   \n",
       "1  1dizvxctg9dHEyaYTFufVi            1    275893      True  1dizvxctg9dHEyaYTFufVi  Gz And Hustlas (feat. Nancy Fletcher)            12           0            0              0   \n",
       "2  2g8HN35AnVGIk7B8yMucww            1    252746      True  2g8HN35AnVGIk7B8yMucww              Big Poppa - 2005 Remaster            13           0            0              0   \n",
       "\n",
       "   is_tracknum_extreme  is_multidisc  is_disc_extreme  acousticness                                       analysis_url  danceability  duration_af  energy  instrumentalness  key  \\\n",
       "0                    0             0                0         0.621  https://api.spotify.com/v1/audio-analysis/2DZN...         0.147     285053.0   0.148             0.282    7   \n",
       "1                    0             0                0         0.164  https://api.spotify.com/v1/audio-analysis/1diz...         0.652     275893.0   0.814             0.000    1   \n",
       "2                    0             0                0         0.430  https://api.spotify.com/v1/audio-analysis/2g8H...         0.780     252747.0   0.575             0.000    9   \n",
       "\n",
       "   liveness  loudness  mode  speechiness      tempo  time_signature  valence  is_time_signature_rare  is_tempo_extreme  is_loudness_very_low  is_af_long  is_high_speech  \\\n",
       "0     0.151   -21.444     1       0.0324  80.634003               3   0.0367                     0.0               0.0                   0.0         0.0             0.0   \n",
       "1     0.360    -4.901     1       0.3100  91.888000               4   0.7880                     0.0               0.0                   0.0         0.0             0.0   \n",
       "2     0.143    -7.247     0       0.2730  84.491997               4   0.7730                     0.0               0.0                   0.0         0.0             0.0   \n",
       "\n",
       "   is_instrumental                album_id                                         name_album album_type   release_date  album_popularity release_date_parsed  \\\n",
       "0              0.0  5v0bDDSl25qgrxOzxqoWXJ  Pedro Ruimonte en Bruselas (Música en la Corte...      album  1509062400000                 0          2017-10-27   \n",
       "1              0.0                    <NA>                                               <NA>       <NA>           <NA>              <NA>                 NaT   \n",
       "2              0.0  2HTbQ0RHwukKVXAlTmCZP2                        Ready to Die (The Remaster)      album   779414400000                 0          1994-09-13   \n",
       "\n",
       "   is_release_year_invalid  release_year  release_month  release_decade                                         artist_ids  n_artists  artist_popularity_mean  \\\n",
       "0                      0.0          2017             10            2010  [6xadlZzmcIMmgspceWCkt3, 0HWL7UfTuSRYVCrvTW5tj...          4                     0.0   \n",
       "1                      NaN          <NA>           <NA>            <NA>   [7hJcb9fa4alzcOq3EaNPoG, 3E2vuvr0IQbReTbXw2MhX8]          2                     0.0   \n",
       "2                      0.0          1994              9            1990                           [5me0Irg2ANcsgc93uaYrpb]          1                     0.0   \n",
       "\n",
       "   artist_popularity_max  artist_followers_mean  artist_followers_max                                       track_genres  name_len  name_words  log_duration  \\\n",
       "0                      0                  112.5                   390                                   [musica antigua]        18           4     12.560434   \n",
       "1                      0              3416346.5               6831895  [g funk, gangster rap, hip hop, pop rap, rap, ...        37           6     12.527772   \n",
       "2                      0              6258716.0               6258716  [east coast hip hop, gangster rap, hardcore hi...        25           5     12.440144   \n",
       "\n",
       "   log_artist_followers_max  log_artist_followers_mean  has_audio_features  \n",
       "0                  5.968708                   4.731803                   1  \n",
       "1                 15.737113                  15.044083                   1  \n",
       "2                 15.649486                  15.649486                   1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>disc_number</th>\n",
       "      <th>duration</th>\n",
       "      <th>explicit</th>\n",
       "      <th>audio_feature_id</th>\n",
       "      <th>name</th>\n",
       "      <th>track_number</th>\n",
       "      <th>popularity</th>\n",
       "      <th>has_preview</th>\n",
       "      <th>is_long_track</th>\n",
       "      <th>is_tracknum_extreme</th>\n",
       "      <th>is_multidisc</th>\n",
       "      <th>is_disc_extreme</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_af</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>is_time_signature_rare</th>\n",
       "      <th>is_tempo_extreme</th>\n",
       "      <th>is_loudness_very_low</th>\n",
       "      <th>is_af_long</th>\n",
       "      <th>is_high_speech</th>\n",
       "      <th>is_instrumental</th>\n",
       "      <th>album_id</th>\n",
       "      <th>name_album</th>\n",
       "      <th>album_type</th>\n",
       "      <th>release_date</th>\n",
       "      <th>album_popularity</th>\n",
       "      <th>release_date_parsed</th>\n",
       "      <th>is_release_year_invalid</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_decade</th>\n",
       "      <th>artist_ids</th>\n",
       "      <th>n_artists</th>\n",
       "      <th>artist_popularity_mean</th>\n",
       "      <th>artist_popularity_max</th>\n",
       "      <th>artist_followers_mean</th>\n",
       "      <th>artist_followers_max</th>\n",
       "      <th>track_genres</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_words</th>\n",
       "      <th>log_duration</th>\n",
       "      <th>log_artist_followers_max</th>\n",
       "      <th>log_artist_followers_mean</th>\n",
       "      <th>has_audio_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2DZN6ceJ7fMU2X6YWuIGHk</td>\n",
       "      <td>1</td>\n",
       "      <td>285053</td>\n",
       "      <td>False</td>\n",
       "      <td>2DZN6ceJ7fMU2X6YWuIGHk</td>\n",
       "      <td>Toccada del 3 Tono</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2DZN...</td>\n",
       "      <td>0.147</td>\n",
       "      <td>285053.0</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.282</td>\n",
       "      <td>7</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-21.444</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>80.634003</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5v0bDDSl25qgrxOzxqoWXJ</td>\n",
       "      <td>Pedro Ruimonte en Bruselas (Música en la Corte...</td>\n",
       "      <td>album</td>\n",
       "      <td>1509062400000</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "      <td>[6xadlZzmcIMmgspceWCkt3, 0HWL7UfTuSRYVCrvTW5tj...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112.5</td>\n",
       "      <td>390</td>\n",
       "      <td>[musica antigua]</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>12.560434</td>\n",
       "      <td>5.968708</td>\n",
       "      <td>4.731803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dizvxctg9dHEyaYTFufVi</td>\n",
       "      <td>1</td>\n",
       "      <td>275893</td>\n",
       "      <td>True</td>\n",
       "      <td>1dizvxctg9dHEyaYTFufVi</td>\n",
       "      <td>Gz And Hustlas (feat. Nancy Fletcher)</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1diz...</td>\n",
       "      <td>0.652</td>\n",
       "      <td>275893.0</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-4.901</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>91.888000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[7hJcb9fa4alzcOq3EaNPoG, 3E2vuvr0IQbReTbXw2MhX8]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3416346.5</td>\n",
       "      <td>6831895</td>\n",
       "      <td>[g funk, gangster rap, hip hop, pop rap, rap, ...</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>12.527772</td>\n",
       "      <td>15.737113</td>\n",
       "      <td>15.044083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2g8HN35AnVGIk7B8yMucww</td>\n",
       "      <td>1</td>\n",
       "      <td>252746</td>\n",
       "      <td>True</td>\n",
       "      <td>2g8HN35AnVGIk7B8yMucww</td>\n",
       "      <td>Big Poppa - 2005 Remaster</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2g8H...</td>\n",
       "      <td>0.780</td>\n",
       "      <td>252747.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-7.247</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>84.491997</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2HTbQ0RHwukKVXAlTmCZP2</td>\n",
       "      <td>Ready to Die (The Remaster)</td>\n",
       "      <td>album</td>\n",
       "      <td>779414400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1994-09-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1994</td>\n",
       "      <td>9</td>\n",
       "      <td>1990</td>\n",
       "      <td>[5me0Irg2ANcsgc93uaYrpb]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6258716.0</td>\n",
       "      <td>6258716</td>\n",
       "      <td>[east coast hip hop, gangster rap, hardcore hi...</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>12.440144</td>\n",
       "      <td>15.649486</td>\n",
       "      <td>15.649486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Album-Level Dataset (eine Zeile = ein Album)\n",
    "\n",
    "**Ziel:** Wir bauen eine ML-fertige Tabelle, in der **jede Zeile ein Album** repräsentiert.\n",
    "Da ein Album aus vielen Tracks besteht und oft mehrere Artists hat, erzeugen wir vor allem **Aggregations-Features**.\n",
    "\n",
    "### Was passiert hier genau?\n",
    "\n",
    "1. **Album-Stammdaten + Release-Time-Features**\n",
    "   - Wir starten mit `albums` (Album-Metadaten).\n",
    "   - Wir parsen `release_date_parsed` und erzeugen daraus:\n",
    "     - `release_year`, `release_month`, `release_decade`\n",
    "\n",
    "2. **Album-Größe (Track-Anzahl)**\n",
    "   - Über `r_albums_tracks` zählen wir:\n",
    "     - `n_tracks` = Anzahl eindeutiger Tracks pro Album\n",
    "   - Das ist ein starkes Strukturfeature (Singles/EPs vs. Alben).\n",
    "\n",
    "3. **Album-Audio-Profil (Aggregierte Track-Audio-Features)**\n",
    "   - Über alle Tracks eines Albums aggregieren wir Audio-Features:\n",
    "     - z. B. `album_mean_energy`, `album_mean_danceability`, `album_mean_loudness`, `album_mean_tempo`\n",
    "   - Dadurch entsteht eine „Audio-Signatur“ des Albums.\n",
    "\n",
    "4. **Album-Artist-Profil (falls `r_albums_artists` vorhanden)**\n",
    "   - Ein Album kann mehrere Artists haben.\n",
    "   - Wir aggregieren Artists pro Album:\n",
    "     - `n_album_artists`\n",
    "     - Popularity/Follower Mittelwert und Maximum\n",
    "\n",
    "5. **Album-Genre-Profil (Union der Genres der Album-Artists)**\n",
    "   - Genres kommen typischerweise von Artists.\n",
    "   - Wir bilden `album_genres` als Vereinigung aller Artist-Genres im Album.\n",
    "\n",
    "6. **Feature Engineering**\n",
    "   - `log_n_tracks`: log-transform gegen Schiefe\n",
    "   - `name_len`, `name_words`: simple Text-Features aus Albumname\n",
    "\n",
    "**Ergebnis:** `album_df` ist eine Album-Feature-Matrix"
   ],
   "id": "10d96e4f70b21b5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:10.330007Z",
     "start_time": "2025-12-14T17:23:00.614320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Start from album master data + parse dates\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Album-level tasks (e.g., album popularity regression) need a single row per album\n",
    "#   - Release time features capture era effects and time bias\n",
    "\n",
    "album_df=albums.copy()\n",
    "album_df=album_df.rename(columns={\"id\": \"album_id\"})\n",
    "\n",
    "album_df[\"release_date_parsed\"] = pd.to_datetime(\n",
    "    col_or_na(album_df,\"release_date_parsed\"), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Add derived time features: year / month / decade\n",
    "album_df = add_release_time_features(album_df, \"release_date_parsed\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Album size feature: number of tracks per album\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Singles/EPs vs albums differ structurally (track count)\n",
    "#   - Useful as a predictor and for data sanity checks\n",
    "album_track_counts = (\n",
    "    rat.groupby(\"album_id\")[\"track_id\"]\n",
    "       .nunique()\n",
    "       .reset_index()\n",
    "       .rename(columns={\"track_id\": \"n_tracks\"})\n",
    ")\n",
    "\n",
    "# Fixed merge: both sides use album_id\n",
    "album_df = album_df.merge(album_track_counts, on=\"album_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Album audio signature: mean of track audio features\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Albums consist of multiple tracks; we aggregate to get a stable album-level profile\n",
    "#   - Mean is a strong baseline aggregation (you could also add std/min/max later)\n",
    "POLICY_AUDIO = [\n",
    "    \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\",\n",
    "    \"speechiness\", \"valence\", \"loudness\", \"tempo\"\n",
    "]\n",
    "\n",
    "# Keep only audio columns that exist (robust to schema differences)\n",
    "audio_cols_present = [c for c in POLICY_AUDIO if c in track_df.columns]\n",
    "\n",
    "# Join album-track relation to track audio features\n",
    "rat_track_audio = rat.merge(track_df[[\"track_id\"] + audio_cols_present], on=\"track_id\", how=\"left\")\n",
    "\n",
    "# Aggregate per album (mean)\n",
    "album_audio_agg = rat_track_audio.groupby(\"album_id\")[audio_cols_present].mean().reset_index()\n",
    "\n",
    "# Prefix columns so they are clearly album-aggregates\n",
    "album_audio_agg = album_audio_agg.add_prefix(\"album_mean_\").rename(columns={\"album_mean_album_id\": \"album_id\"})\n",
    "\n",
    "# Merge audio aggregates back to album table\n",
    "album_df = album_df.merge(album_audio_agg, on=\"album_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Album -> artists aggregates (optional)\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Albums can have multiple artists; their popularity/followers often influence album success\n",
    "#   - This block runs only if r_albums_artists exists in your export\n",
    "if not raa.empty and \"album_id\" in raa.columns and \"artist_id\" in raa.columns:\n",
    "    raa_art = raa.merge(artist_feat, on=\"artist_id\", how=\"left\")\n",
    "\n",
    "    album_artist_agg = (\n",
    "        raa_art.groupby(\"album_id\")\n",
    "              .agg(\n",
    "                  n_album_artists=(\"artist_id\", \"nunique\"),\n",
    "                  album_artist_popularity_mean=(\"artist_popularity\", \"mean\"),\n",
    "                  album_artist_popularity_max=(\"artist_popularity\", \"max\"),\n",
    "                  album_artist_followers_mean=(\"artist_followers\", \"mean\"),\n",
    "                  album_artist_followers_max=(\"artist_followers\", \"max\"),\n",
    "              )\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    album_df = album_df.merge(album_artist_agg, on=\"album_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Album genres union\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Spotify-like schemas often attach genres to artists\n",
    "#   - We derive an album's genre profile as the union of all album artists' genres\n",
    "if not raa.empty:\n",
    "    raa_gen = raa.merge(artist_to_genres, on=\"artist_id\", how=\"left\")\n",
    "\n",
    "    album_to_genres = (\n",
    "        raa_gen.groupby(\"album_id\")[\"artist_genres\"]\n",
    "              .apply(lambda rows: sorted(set([\n",
    "                  g for lst in rows.dropna()\n",
    "                  for g in (lst if isinstance(lst, list) else [])\n",
    "              ])))\n",
    "              .reset_index()\n",
    "              .rename(columns={\"artist_genres\": \"album_genres\"})\n",
    "    )\n",
    "\n",
    "    album_df = album_df.merge(album_to_genres, on=\"album_id\", how=\"left\")\n",
    "else:\n",
    "    # Keep a consistent schema even if we can't compute genres\n",
    "    album_df[\"album_genres\"] = [[] for _ in range(len(album_df))]\n",
    "\n",
    "# Ensure list type (important for CSV fallback)\n",
    "album_df[\"album_genres\"] = ensure_list_column(col_or_na(album_df, \"album_genres\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Feature engineering (log transforms, name features)\n",
    "# ------------------------------------------------------------\n",
    "# log transform track count (often heavy-tailed: singles vs compilations)\n",
    "album_df[\"log_n_tracks\"] = log1p_numeric(col_or_na(album_df, \"n_tracks\"))\n",
    "\n",
    "# Simple text features from album name\n",
    "album_df[\"name_len\"] = safe_len_series(col_or_na(album_df, \"name\"))\n",
    "album_df[\"name_words\"] = safe_word_count_series(col_or_na(album_df, \"name\"))\n",
    "\n",
    "print(\"Album-level dataset shape:\", album_df.shape)\n",
    "album_df.head(3)"
   ],
   "id": "138d08a7bf5bb238",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Album-level dataset shape: (129152, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 album_id                                             name album_type   release_date  popularity release_date_parsed  is_release_year_invalid  release_year  \\\n",
       "0  7zr66qWybr1mAMSUVVosKU                                          Reflexo      album  1464220800000           0          2016-05-26                        0          2016   \n",
       "1  7zrLd0zddHOwA9DGlsDr4h                                   Floating World      album  1410652800000           0          2014-09-14                        0          2014   \n",
       "2  7zri1pX9eMh0IqwpxMxOwp  Arne Aano's Beste - Slepp Himlen I Sjela Di Inn      album  1236729600000           0          2009-03-11                        0          2009   \n",
       "\n",
       "   release_month  release_decade  n_tracks  album_mean_acousticness  album_mean_danceability  album_mean_energy  album_mean_instrumentalness  album_mean_liveness  \\\n",
       "0              5            2010         1                 0.519000                    0.726              0.491                     0.000007               0.0965   \n",
       "1              9            2010         1                 0.000516                    0.335              0.823                     0.331000               0.2130   \n",
       "2              3            2000         1                 0.888000                    0.537              0.303                     0.000000               0.1350   \n",
       "\n",
       "   album_mean_speechiness  album_mean_valence  album_mean_loudness  album_mean_tempo  n_album_artists  album_artist_popularity_mean  album_artist_popularity_max  \\\n",
       "0                  0.1260              0.2870              -11.166        109.935997              1.0                           0.0                            0   \n",
       "1                  0.0437              0.0699               -7.041         90.175003              1.0                           0.0                            0   \n",
       "2                  0.0365              0.5410               -9.413        137.932007              1.0                           0.0                            0   \n",
       "\n",
       "   album_artist_followers_mean  album_artist_followers_max      album_genres  log_n_tracks  name_len  name_words  \n",
       "0                     147089.0                      147089    [hip hop tuga]      0.693147         7           1  \n",
       "1                         75.0                          75  [crossover prog]      0.693147        14           2  \n",
       "2                          0.0                           0                []      0.693147        47          10  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album_id</th>\n",
       "      <th>name</th>\n",
       "      <th>album_type</th>\n",
       "      <th>release_date</th>\n",
       "      <th>popularity</th>\n",
       "      <th>release_date_parsed</th>\n",
       "      <th>is_release_year_invalid</th>\n",
       "      <th>release_year</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_decade</th>\n",
       "      <th>n_tracks</th>\n",
       "      <th>album_mean_acousticness</th>\n",
       "      <th>album_mean_danceability</th>\n",
       "      <th>album_mean_energy</th>\n",
       "      <th>album_mean_instrumentalness</th>\n",
       "      <th>album_mean_liveness</th>\n",
       "      <th>album_mean_speechiness</th>\n",
       "      <th>album_mean_valence</th>\n",
       "      <th>album_mean_loudness</th>\n",
       "      <th>album_mean_tempo</th>\n",
       "      <th>n_album_artists</th>\n",
       "      <th>album_artist_popularity_mean</th>\n",
       "      <th>album_artist_popularity_max</th>\n",
       "      <th>album_artist_followers_mean</th>\n",
       "      <th>album_artist_followers_max</th>\n",
       "      <th>album_genres</th>\n",
       "      <th>log_n_tracks</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7zr66qWybr1mAMSUVVosKU</td>\n",
       "      <td>Reflexo</td>\n",
       "      <td>album</td>\n",
       "      <td>1464220800000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-26</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.2870</td>\n",
       "      <td>-11.166</td>\n",
       "      <td>109.935997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>147089.0</td>\n",
       "      <td>147089</td>\n",
       "      <td>[hip hop tuga]</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7zrLd0zddHOwA9DGlsDr4h</td>\n",
       "      <td>Floating World</td>\n",
       "      <td>album</td>\n",
       "      <td>1410652800000</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-14</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>-7.041</td>\n",
       "      <td>90.175003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75</td>\n",
       "      <td>[crossover prog]</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7zri1pX9eMh0IqwpxMxOwp</td>\n",
       "      <td>Arne Aano's Beste - Slepp Himlen I Sjela Di Inn</td>\n",
       "      <td>album</td>\n",
       "      <td>1236729600000</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-03-11</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.5410</td>\n",
       "      <td>-9.413</td>\n",
       "      <td>137.932007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Artist-Level Dataset (eine Zeile = ein Artist)\n",
    "\n",
    "**Ziel:** Wir bauen eine ML-fertige Tabelle, in der **jede Zeile einen Artist** repräsentiert.\n",
    "Diese Tabelle wird vor allem für **Clustering / Community Detection** (unsupervised) genutzt, kann aber später auch für supervised Tasks (z. B. Artist-Popularity) verwendet werden.\n",
    "\n",
    "### Was passiert hier genau?\n",
    "\n",
    "1. **Artist-Stammdaten**\n",
    "   - Wir starten mit `artists` und benennen die ID-Spalte zu `artist_id`, damit Joins konsistent sind.\n",
    "\n",
    "2. **Artist-Style-Profil aus Tracks (Aggregation)**\n",
    "   - Über `r_track_artist` verknüpfen wir Artists mit ihren Tracks.\n",
    "   - Wir hängen die Track-Features an (Audio + optional Popularity/Explicit) und aggregieren dann pro Artist:\n",
    "     - `n_tracks`: Anzahl eindeutiger Tracks\n",
    "     - `track_pop_mean`: durchschnittliche Track-Popularität (falls vorhanden)\n",
    "     - `explicit_rate`: Anteil „explicit“-Tracks (falls vorhanden)\n",
    "     - `mean_<audio_feature>`: durchschnittliche Audio-Signatur (z. B. mean_energy, mean_danceability, …)\n",
    "\n",
    "   Ergebnis: Jeder Artist bekommt einen stabilen numerischen Vektor, der seinen „Sound“ beschreibt.\n",
    "\n",
    "3. **Genres pro Artist**\n",
    "   - Wir mergen die Liste der Genres (`artist_genres`) pro Artist (aus `r_artist_genre`).\n",
    "   - Diese Liste kann später z. B. als Multi-Hot-Features genutzt werden.\n",
    "\n",
    "4. **Feature Engineering**\n",
    "   - `log_followers`: Log-Transform für heavy-tailed Followers\n",
    "   - `log_n_tracks`: Log-Transform, da Track-Anzahl oft sehr schief verteilt ist\n",
    "\n",
    "**Ergebnis:** `artist_df` enthält pro Artist:\n",
    "- Stammdaten (name, popularity, followers, …)\n",
    "- Aggregierte Track-Audio-Signatur\n",
    "- Genre-Liste\n",
    "- log-transformierte Stabilitätsfeatures\n"
   ],
   "id": "6301de187e1db68c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:35.703274Z",
     "start_time": "2025-12-14T17:23:10.340534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Start from artist master data\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - We want a single vector per artist for clustering / similarity analysis\n",
    "#   - Rename PK to artist_id for consistent joins across tables\n",
    "artist_df = artists.rename(columns={\"id\": \"artist_id\"}).copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Build artist \"style profile\" by aggregating over all their tracks\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Artists have many tracks (Many-to-Many: r_track_artist)\n",
    "#   - We want stable numeric features per artist:\n",
    "#       * number of tracks\n",
    "#       * average audio signature (mean_energy, mean_danceability, ...)\n",
    "#       * optionally: average track popularity and explicit rate\n",
    "cols_for_artist_agg = [\"track_id\"] + audio_cols_present\n",
    "\n",
    "if \"popularity\" in track_df.columns:\n",
    "    cols_for_artist_agg += [\"popularity\"]\n",
    "\n",
    "if \"explicit\" in track_df.columns:\n",
    "    cols_for_artist_agg += [\"explicit\"]\n",
    "\n",
    "rta_track_audio = rta.merge(track_df[cols_for_artist_agg], on=\"track_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Helper explicit rate per artist\n",
    "\n",
    "def explicit_rate_fn(x):\n",
    "    xx = pd.to_numeric(x,errors=\"coerce\")\n",
    "    if xx.dropna().empty:\n",
    "        return np.nan\n",
    "    return float(np.nanmean(xx))\n",
    "\n",
    "agg_dict = {\n",
    "    \"n_tracks\":(\"track_id\",\"nunique\")\n",
    "}\n",
    "\n",
    "# Optional: average track popularity (proxy of how popular their tracks tend to be)\n",
    "if \"popularity\" in rta_track_audio.columns:\n",
    "    agg_dict[\"track_pop_mean\"] = (\"popularity\", \"mean\")\n",
    "\n",
    "# Optional: explicit rate (share of explicit tracks)\n",
    "if \"explicit\" in rta_track_audio.columns:\n",
    "    agg_dict[\"explicit_rate\"] = (\"explicit\", explicit_rate_fn)\n",
    "\n",
    "# Core: mean audio signature per artist\n",
    "for c in audio_cols_present:\n",
    "    agg_dict[f\"mean_{c}\"] = (c, \"mean\")\n",
    "\n",
    "artist_audio_agg = (\n",
    "    rta_track_audio.groupby(\"artist_id\")\n",
    "    .agg(**agg_dict)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge aggregated features back into artist table\n",
    "artist_df = artist_df.merge(artist_audio_agg, on=\"artist_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Attach genres list per artist\n",
    "# ------------------------------------------------------------\n",
    "# Why:\n",
    "#   - Genres are usually provided at artist-level\n",
    "#   - We keep them as list for later multi-hot encoding (Top-K)\n",
    "artist_df = artist_df.merge(artist_to_genres, on=\"artist_id\", how=\"left\")\n",
    "artist_df[\"artist_genres\"] = ensure_list_column(col_or_na(artist_df, \"artist_genres\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Feature engineering (log transforms for heavy-tailed counts)\n",
    "# ------------------------------------------------------------\n",
    "# followers and track counts are typically very skewed -> log stabilizes scale\n",
    "artist_df[\"log_followers\"] = log1p_numeric(col_or_na(artist_df, \"followers\"))\n",
    "artist_df[\"log_n_tracks\"] = log1p_numeric(col_or_na(artist_df, \"n_tracks\"))\n",
    "\n",
    "print(\"Artist-level dataset shape:\", artist_df.shape)\n",
    "artist_df.head(3)\n"
   ],
   "id": "d2c3fc683ac6e877",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist-level dataset shape: (139608, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                artist_id              name  popularity  followers  is_followers_extreme  followers_log1p  n_tracks  track_pop_mean  explicit_rate  mean_acousticness  \\\n",
       "0  7zzl8HQ2v9hVdLh0Ygkwgc        Megatherio           0         59                     0         4.094345         3             0.0            1.0           0.000101   \n",
       "1  00045gNg7mLEf9UY9yhD0t  Kubus & BangBang           0        820                     0         6.710523        11             0.0            1.0           0.121626   \n",
       "2  000xagx3GkcunHTFdB4ly0              Moxa           0        156                     0         5.056246         1             0.0            0.0           0.000151   \n",
       "\n",
       "   mean_danceability  mean_energy  mean_instrumentalness  mean_liveness  mean_speechiness  mean_valence  mean_loudness  mean_tempo             artist_genres  log_followers  \\\n",
       "0           0.267333        0.990               0.013101       0.104633            0.0902      0.311900      -4.583000  136.755666  [brazilian thrash metal]       4.094345   \n",
       "1           0.658273        0.651               0.000177       0.246955            0.3073      0.497918      -8.265273  122.092817           [dutch hip hop]       6.710523   \n",
       "2           0.441000        0.959               0.334000       0.229000            0.0611      0.171000      -4.694000  138.009003               [indie emo]       5.056246   \n",
       "\n",
       "   log_n_tracks  \n",
       "0      1.386294  \n",
       "1      2.484907  \n",
       "2      0.693147  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>is_followers_extreme</th>\n",
       "      <th>followers_log1p</th>\n",
       "      <th>n_tracks</th>\n",
       "      <th>track_pop_mean</th>\n",
       "      <th>explicit_rate</th>\n",
       "      <th>mean_acousticness</th>\n",
       "      <th>mean_danceability</th>\n",
       "      <th>mean_energy</th>\n",
       "      <th>mean_instrumentalness</th>\n",
       "      <th>mean_liveness</th>\n",
       "      <th>mean_speechiness</th>\n",
       "      <th>mean_valence</th>\n",
       "      <th>mean_loudness</th>\n",
       "      <th>mean_tempo</th>\n",
       "      <th>artist_genres</th>\n",
       "      <th>log_followers</th>\n",
       "      <th>log_n_tracks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7zzl8HQ2v9hVdLh0Ygkwgc</td>\n",
       "      <td>Megatherio</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.267333</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>0.104633</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>-4.583000</td>\n",
       "      <td>136.755666</td>\n",
       "      <td>[brazilian thrash metal]</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00045gNg7mLEf9UY9yhD0t</td>\n",
       "      <td>Kubus &amp; BangBang</td>\n",
       "      <td>0</td>\n",
       "      <td>820</td>\n",
       "      <td>0</td>\n",
       "      <td>6.710523</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.121626</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.246955</td>\n",
       "      <td>0.3073</td>\n",
       "      <td>0.497918</td>\n",
       "      <td>-8.265273</td>\n",
       "      <td>122.092817</td>\n",
       "      <td>[dutch hip hop]</td>\n",
       "      <td>6.710523</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000xagx3GkcunHTFdB4ly0</td>\n",
       "      <td>Moxa</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>5.056246</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>-4.694000</td>\n",
       "      <td>138.009003</td>\n",
       "      <td>[indie emo]</td>\n",
       "      <td>5.056246</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save modeling datasets",
   "id": "41d4accd859b105e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:37.872696Z",
     "start_time": "2025-12-14T17:23:35.715592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "track_out = PATHS.modeling_dir / \"track_dataset.parquet\"\n",
    "album_out = PATHS.modeling_dir / \"album_dataset.parquet\"\n",
    "artist_out = PATHS.modeling_dir / \"artist_dataset.parquet\"\n",
    "\n",
    "track_df.to_parquet(track_out, index=False)\n",
    "album_df.to_parquet(album_out, index=False)\n",
    "artist_df.to_parquet(artist_out, index=False)\n",
    "\n",
    "print(\" Saved modeling datasets:\")\n",
    "print(\" -\", track_out)\n",
    "print(\" -\", album_out)\n",
    "print(\" -\", artist_out)"
   ],
   "id": "b1bb7addcee6bb2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved modeling datasets:\n",
      " - ..\\data\\processed\\modeling\\track_dataset.parquet\n",
      " - ..\\data\\processed\\modeling\\album_dataset.parquet\n",
      " - ..\\data\\processed\\modeling\\artist_dataset.parquet\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#  Targets\n",
    "   In this section we construct all targets used in this project:\n",
    "   (A) Track popularity regression target (continuous)\n",
    "   (B) Album popularity regression target (continuous)\n",
    "   (C) Hit prediction target (binary) using year-relative threshold\n",
    "   (D) Explicit/content target (binary)\n",
    "   (E) Mood tags target (multi-label; weak supervision via audio feature quantiles)\n",
    "\n",
    " Why separate targets from features?\n",
    " - Prevent leakage: targets are derived ONLY from allowed columns.\n",
    " - Reproducibility: same label definition used later (Notebook 4 scoring).\n"
   ],
   "id": "55291a885f28da7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:38.102849Z",
     "start_time": "2025-12-14T17:23:37.881395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (A) Track popularity regression\n",
    "# Popularity is typically in [0,100]. Some rows may have missing popularity -> keep as NaN and mask later.\n",
    "assert \"popularity\" in track_df.columns, \"track_df must contain 'popularity' for track popularity target\"\n",
    "y_track_pop = pd.to_numeric(track_df[\"popularity\"], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# (B) Album popularity regression\n",
    "# Similar to tracks, popularity is the numeric target, and NaN indicates missing label.\n",
    "assert \"popularity\" in album_df.columns, \"album_df must contain 'popularity' for album popularity target\"\n",
    "y_album_pop = pd.to_numeric(album_df[\"popularity\"], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# (C) Hit prediction (binary)\n",
    "# Default definition:\n",
    "# - A \"hit\" is defined within each release year using a percentile threshold.\n",
    "# - This is more fair than a fixed popularity threshold across decades.\n",
    "#\n",
    "# Fallback:\n",
    "# - If release_year is missing OR threshold can't be computed for a year, use HIT_FALLBACK_POP_THRESHOLD.\n",
    "\n",
    "def build_hit_labels_robust(\n",
    "    df: pd.DataFrame,\n",
    "    hit_percentile: float = 0.90,        # \"top 10%\" (within year if possible)\n",
    "    desired_rate: float = 0.10,          # safety fallback target positive rate\n",
    "    min_tracks_per_year: int = 200,      # lower this if your sample per year is small\n",
    "    use_nonzero: bool = True             # ignore popularity==0 when computing thresholds\n",
    ") -> pd.Series:\n",
    "    pop = pd.to_numeric(df[\"popularity\"], errors=\"coerce\").astype(\"float64\")\n",
    "    year = pd.to_numeric(df.get(\"release_year\", np.nan), errors=\"coerce\").round()\n",
    "\n",
    "    # ---------- global threshold (non-zero aware) ----------\n",
    "    if use_nonzero:\n",
    "        nz = pop[(pop > 0) & pop.notna()]\n",
    "    else:\n",
    "        nz = pop.dropna()\n",
    "\n",
    "    if len(nz) > 0:\n",
    "        global_thr = float(nz.quantile(hit_percentile))\n",
    "    else:\n",
    "        # if everything is 0/NaN, fall back to regular quantile\n",
    "        global_thr = float(pop.dropna().quantile(hit_percentile)) if pop.notna().any() else 0.0\n",
    "\n",
    "    # ---------- per-year thresholds (only for \"good\" years) ----------\n",
    "    y = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "\n",
    "    if year.notna().any():\n",
    "        tmp = pd.DataFrame({\"year\": year, \"pop\": pop}).dropna(subset=[\"year\", \"pop\"])\n",
    "\n",
    "        # keep only years with enough rows\n",
    "        counts = tmp[\"year\"].value_counts()\n",
    "        good_years = counts[counts >= min_tracks_per_year].index\n",
    "        tmp_good = tmp[tmp[\"year\"].isin(good_years)]\n",
    "\n",
    "        if len(tmp_good) > 0:\n",
    "            def year_thr_func(s: pd.Series) -> float:\n",
    "                s = s.dropna()\n",
    "                if use_nonzero:\n",
    "                    s = s[s > 0]\n",
    "                if len(s) == 0:\n",
    "                    return np.nan\n",
    "                return float(s.quantile(hit_percentile))\n",
    "\n",
    "            year_thr = tmp_good.groupby(\"year\")[\"pop\"].apply(year_thr_func)\n",
    "            thr = year.map(year_thr)  # NaN for missing/rare years\n",
    "\n",
    "            # year rule where threshold exists\n",
    "            y = (pop >= thr).where(thr.notna(), np.nan)\n",
    "\n",
    "    # ---------- fill missing with global rule ----------\n",
    "    y = y.where(pd.notna(y), pop >= global_thr)\n",
    "\n",
    "    # finalize boolean -> int8\n",
    "    y = pd.Series(y).fillna(False).astype(bool).astype(\"int8\")\n",
    "\n",
    "    # ---------- safety: if label became one-class, force top-K globally ----------\n",
    "    if y.nunique() < 2:\n",
    "        n = int(pop.notna().sum())\n",
    "        k = max(1, int(desired_rate * n))\n",
    "\n",
    "        # Take top-k by popularity (ties handled)\n",
    "        top_idx = pop.fillna(-1).nlargest(k).index\n",
    "        y = pd.Series(0, index=df.index, dtype=\"int8\")\n",
    "        y.loc[top_idx] = 1\n",
    "\n",
    "    return y\n",
    "\n",
    "y_hit = build_hit_labels_robust(\n",
    "    track_df,\n",
    "    hit_percentile=HIT_PERCENTILE,\n",
    "    desired_rate=0.10,\n",
    "    min_tracks_per_year=200,   # if your sample per year is smaller, set 50\n",
    "    use_nonzero=True\n",
    ")\n",
    "\n",
    "print(\"Hit label distribution:\", y_hit.value_counts(dropna=False).to_dict())\n",
    "print(\"Hit positive rate:\", float(y_hit.mean()))\n",
    "\n",
    "\n",
    "# (D) Explicit prediction (binary)\n",
    "# explicit is already (0/1) in most Spotify dumps. Missing -> 0 (conservative).\n",
    "if \"explicit\" in track_df.columns:\n",
    "    y_explicit = pd.to_numeric(track_df[\"explicit\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "else:\n",
    "    y_explicit = pd.Series(0, index=track_df.index, dtype=\"int8\")\n",
    "\n",
    "# (E) Mood tags (multi-label)\n",
    "# We create weak supervision labels using quantiles of audio features.\n",
    "# Example: \"high_energy\" = 1 if energy is above 80th percentile.\n",
    "#\n",
    "# NOTE on evaluation:\n",
    "# - Strict: compute thresholds on TRAIN ONLY to avoid slight leakage.\n",
    "# - Demo/baseline: compute thresholds on FULL data (fast and stable).\n",
    "mood_thresholds: Dict[tuple, float] = {}\n",
    "\n",
    "for name, col, q, direction in MOOD_TAGS:\n",
    "    if col in track_df.columns:\n",
    "        vals = pd.to_numeric(track_df[col], errors=\"coerce\").dropna()\n",
    "        mood_thresholds[(name, col, q, direction)] = float(vals.quantile(q)) if len(vals) else np.nan\n",
    "    else:\n",
    "        mood_thresholds[(name, col, q, direction)] = np.nan\n",
    "\n",
    "def build_mood_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a multi-label target matrix Y_mood of shape (n_samples, n_labels).\n",
    "    Each label is derived from a threshold on an audio feature.\n",
    "    Missing audio feature values become label=0 (no evidence for tag).\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for name, col, q, direction in MOOD_TAGS:\n",
    "        if col not in df.columns:\n",
    "            out[name] = 0\n",
    "            continue\n",
    "\n",
    "        thr = mood_thresholds.get((name, col, q, direction), np.nan)\n",
    "        x = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        if np.isnan(thr):\n",
    "            out[name] = 0\n",
    "            continue\n",
    "\n",
    "        if direction == \"gt\":\n",
    "            out[name] = (x >= thr).fillna(False).astype(\"int8\")\n",
    "        else:\n",
    "            out[name] = (x <= thr).fillna(False).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "Y_mood = build_mood_labels(track_df)\n",
    "\n",
    "print(\"Targets prepared:\")\n",
    "print(\" - y_track_pop:\", y_track_pop.shape, \"missing_rate:\", float(y_track_pop.isna().mean()))\n",
    "print(\" - y_album_pop:\", y_album_pop.shape, \"missing_rate:\", float(y_album_pop.isna().mean()))\n",
    "print(\" - y_hit dist:\", y_hit.value_counts(dropna=False).to_dict())\n",
    "print(\" - y_explicit dist:\", y_explicit.value_counts(dropna=False).to_dict())\n",
    "print(\" - Y_mood:\", Y_mood.shape)"
   ],
   "id": "b89f65e387bbdf66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit label distribution: {0: 265157, 1: 29461}\n",
      "Hit positive rate: 0.09999728461940546\n",
      "Targets prepared:\n",
      " - y_track_pop: (294618,) missing_rate: 0.0\n",
      " - y_album_pop: (129152,) missing_rate: 0.0\n",
      " - y_hit dist: {0: 265157, 1: 29461}\n",
      " - y_explicit dist: {0: 214945, 1: 79673}\n",
      " - Y_mood: (294618, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32608\\3971996365.py:71: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = pd.Series(y).fillna(False).astype(bool).astype(\"int8\")\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Genre multi-hop (Top-K) for track/album/artist\n",
    "\n",
    " Genres are stored as LISTS (e.g. track_genres = [genre_id1, genre_id2, ...]).\n",
    " Most ML models need fixed-size numeric vectors, so we:\n",
    "   1) pick the Top-K most frequent genres in track_df\n",
    "   2) create a multi-hot encoding (0/1 columns) for those Top-K genres\n",
    "\n",
    " Why Top-K?\n",
    " - The full genre space can be huge.\n",
    " - Top-K keeps dimensionality reasonable and avoids sparse explosion.\n",
    " - Rare genres can be grouped into \"other\" implicitly (all zeros)."
   ],
   "id": "9f5253c720a39360"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:39.157702Z",
     "start_time": "2025-12-14T17:23:38.110979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_genres = top_k_list_counts(track_df[\"track_genres\"], top_k=TOP_K_GENRES) if \"track_genres\" in track_df.columns else []\n",
    "\n",
    "track_genre_mh = (\n",
    "    genres_to_multihot(track_df, \"track_genres\", top_genres, prefix=\"track_\")\n",
    "    if top_genres else pd.DataFrame(index=track_df.index)\n",
    ")\n",
    "album_genre_mh = (\n",
    "    genres_to_multihot(album_df, \"album_genres\", top_genres, prefix=\"album_\")\n",
    "    if (top_genres and \"album_genres\" in album_df.columns) else pd.DataFrame(index=album_df.index)\n",
    ")\n",
    "artist_genre_mh = (\n",
    "    genres_to_multihot(artist_df, \"artist_genres\", top_genres, prefix=\"artist_\")\n",
    "    if (top_genres and \"artist_genres\" in artist_df.columns) else pd.DataFrame(index=artist_df.index)\n",
    ")\n",
    "\n",
    "print(\"Genre multi-hot shapes:\", track_genre_mh.shape, album_genre_mh.shape, artist_genre_mh.shape)\n"
   ],
   "id": "963868a53a975e5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre multi-hot shapes: (294618, 50) (129152, 50) (139608, 50)\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection & Leakage Guards\n",
    "\n",
    "In this step we decide **which columns are allowed as model inputs** (features).\n",
    "The goal is to build a **stable, reproducible feature schema** that can be reused during inference (Notebook 4).\n",
    "\n",
    "### Track feature groups\n",
    "The track-level feature set is composed of several feature families:\n",
    "\n",
    "- **Track metadata**\n",
    "  - Example: `duration`, `disc_number`, `track_number`, text-derived features like `name_len`, `name_words`\n",
    "- **Release time features**\n",
    "  - Example: `release_year`, `release_month`, `release_decade`\n",
    "  - Reason: popularity and audio trends shift across eras\n",
    "- **Artist aggregates**\n",
    "  - Example: `n_artists`, `artist_followers_mean/max`, `artist_popularity_mean/max`, plus `log1p` variants\n",
    "  - Reason: artist reach often correlates with track exposure\n",
    "- **Audio features**\n",
    "  - Example: `energy`, `valence`, `tempo`, `loudness`, `danceability`, `speechiness`, etc.\n",
    "  - Reason: core predictors for mood/content modeling and popularity structure\n",
    "- **Genre vectors**\n",
    "  - Multi-hot encoded **Top-K genres** derived from artist genres aggregated to track-level\n",
    "\n",
    "### Leakage guards (high importance)\n",
    "Some features can act as **post-success proxies** and may cause unrealistic performance estimates.\n",
    "\n",
    "- **`album_popularity`**\n",
    "  - Often reflects the same popularity ecosystem as track popularity.\n",
    "  - Using it to predict track popularity can leak information and inflate results.\n",
    "  - **Default policy:** excluded unless explicitly allowed via `ALLOW_LEAKY_FEATURES`.\n",
    "\n",
    "- **`artist_popularity_*`**\n",
    "  - Artist popularity can be updated after hits and may correlate strongly with success.\n",
    "  - Depending on the deployment scenario, it may be partially leaky.\n",
    "  - **Default policy (recommended):** remove `artist_popularity_mean/max` when `ALLOW_LEAKY_FEATURES = False`.\n",
    "\n",
    "These guards ensure the model is closer to a real-world setting where only **pre-available / non-prox**\n"
   ],
   "id": "ee5d4440f210de57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:39.305752Z",
     "start_time": "2025-12-14T17:23:39.166120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Audio feature columns (policy-driven)\n",
    "track_audio_extra = [c for c in [\"key\", \"mode\", \"time_signature\"] if c in track_df.columns]\n",
    "track_audio_main = [c for c in POLICY_AUDIO if c in track_df.columns]\n",
    "\n",
    "# Duration base column (depends on your dataset naming)\n",
    "duration_feature = None\n",
    "if isinstance(dur_col, str) and dur_col.strip() and (dur_col in track_df.columns):\n",
    "    duration_feature = dur_col\n",
    "\n",
    "\n",
    "# Track numeric columns\n",
    "TRACK_NUMERIC = [\n",
    "    \"disc_number\", \"track_number\",\n",
    "    *( [duration_feature] if duration_feature else [] ),\n",
    "    \"log_duration\",\n",
    "    \"has_preview\",\n",
    "    \"has_audio_features\",\n",
    "    \"release_year\", \"release_month\", \"release_decade\",\n",
    "    \"n_artists\",\n",
    "    \"artist_popularity_mean\", \"artist_popularity_max\",\n",
    "    \"artist_followers_mean\", \"artist_followers_max\",\n",
    "    \"log_artist_followers_mean\", \"log_artist_followers_max\",\n",
    "    \"name_len\", \"name_words\",\n",
    "] + track_audio_main + track_audio_extra\n",
    "\n",
    "# Track categorical columns (keep small-cardinality only)\n",
    "# album_group should be gone; keep album_type if present.\n",
    "TRACK_CATEGORICAL = [c for c in [\"album_type\"] if c in track_df.columns]\n",
    "\n",
    "# Remove missing columns safely\n",
    "TRACK_NUMERIC = [\n",
    "    c for c in TRACK_NUMERIC\n",
    "    if (c is not None) and (not pd.isna(c)) and (c in track_df.columns)\n",
    "]\n",
    "\n",
    "TRACK_CATEGORICAL = [\n",
    "    c for c in TRACK_CATEGORICAL\n",
    "    if (c is not None) and (not pd.isna(c)) and (c in track_df.columns)\n",
    "]\n",
    "\n",
    "# Leakage guard:\n",
    "# album_popularity is a very strong proxy; default OFF to avoid leakage.\n",
    "if \"album_popularity\" in track_df.columns and ALLOW_LEAKY_FEATURES:\n",
    "    TRACK_NUMERIC = TRACK_NUMERIC + [\"album_popularity\"]\n",
    "\n",
    "# If leakage is OFF, optionally remove artist popularity proxies too.\n",
    "if not ALLOW_LEAKY_FEATURES:\n",
    "    TRACK_NUMERIC = [c for c in TRACK_NUMERIC if c not in {\"artist_popularity_mean\", \"artist_popularity_max\"}]\n",
    "\n",
    "# Build X_track with base features + genre multi-hot\n",
    "X_track_base = track_df[TRACK_NUMERIC + TRACK_CATEGORICAL].copy()\n",
    "X_track = pd.concat([X_track_base.reset_index(drop=True), track_genre_mh.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# --- Create task-specific masks ---\n",
    "# Regression requires non-null target\n",
    "mask_track_pop = y_track_pop.notna()\n",
    "X_track_pop = X_track.loc[mask_track_pop].reset_index(drop=True)\n",
    "y_track_pop_clean = y_track_pop.loc[mask_track_pop].reset_index(drop=True)\n",
    "\n",
    "# Hit requires popularity (already used to construct label)\n",
    "mask_hit = track_df[\"popularity\"].notna()\n",
    "X_track_hit = X_track.loc[mask_hit].reset_index(drop=True)\n",
    "y_hit_clean = y_hit.loc[mask_hit].reset_index(drop=True)\n",
    "\n",
    "# Explicit uses all rows; missing explicit defaulted to 0\n",
    "X_track_explicit = X_track.reset_index(drop=True)\n",
    "y_explicit_clean = y_explicit.reset_index(drop=True)\n",
    "\n",
    "# Mood requires audio features available (otherwise weak labels meaningless)\n",
    "mask_mood = (track_df[\"has_audio_features\"] == 1)\n",
    "X_track_mood = X_track.loc[mask_mood].reset_index(drop=True)\n",
    "Y_mood_clean = Y_mood.loc[mask_mood].reset_index(drop=True)\n",
    "\n",
    "print(\"X_track shapes:\")\n",
    "print(\" - pop:\", X_track_pop.shape, y_track_pop_clean.shape)\n",
    "print(\" - hit:\", X_track_hit.shape, y_hit_clean.shape)\n",
    "print(\" - explicit:\", X_track_explicit.shape, y_explicit_clean.shape)\n",
    "print(\" - mood:\", X_track_mood.shape, Y_mood_clean.shape)\n",
    "\n",
    "# ---------- Album features ----------\n",
    "# Album numeric: time + counts + aggregated features (from earlier album building)\n",
    "ALBUM_NUMERIC = [\n",
    "    \"release_year\", \"release_month\", \"release_decade\",\n",
    "    \"n_tracks\", \"log_n_tracks\",\n",
    "    \"name_len\", \"name_words\",\n",
    "] + [c for c in album_df.columns if c.startswith(\"album_mean_\") or c.startswith(\"album_artist_\")]\n",
    "\n",
    "ALBUM_NUMERIC = [c for c in ALBUM_NUMERIC if c in album_df.columns]\n",
    "\n",
    "# album_group should be gone (100% missing); keep album_type if present.\n",
    "ALBUM_CATEGORICAL = [c for c in [\"album_type\"] if c in album_df.columns]\n",
    "\n",
    "X_album_base = album_df[ALBUM_NUMERIC + ALBUM_CATEGORICAL].copy()\n",
    "X_album = pd.concat([X_album_base.reset_index(drop=True), album_genre_mh.reset_index(drop=True)], axis=1)\n",
    "\n",
    "mask_album_pop = y_album_pop.notna()\n",
    "X_album_pop = X_album.loc[mask_album_pop].reset_index(drop=True)\n",
    "y_album_pop_clean = y_album_pop.loc[mask_album_pop].reset_index(drop=True)\n",
    "\n",
    "print(\"X_album_pop:\", X_album_pop.shape, y_album_pop_clean.shape)\n"
   ],
   "id": "930b2060d79f3909",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_track shapes:\n",
      " - pop: (294618, 79) (294618,)\n",
      " - hit: (294618, 79) (294618,)\n",
      " - explicit: (294618, 79) (294618,)\n",
      " - mood: (294616, 79) (294616, 7)\n",
      "X_album_pop: (129152, 71) (129152,)\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Builder (sklearn ColumnTransformer)\n",
    "\n",
    "We use a **ColumnTransformer** to apply different preprocessing steps to **numeric** and **categorical** feature groups.\n",
    "This is a best-practice approach in scikit-learn because it keeps the entire transformation logic inside a single, reproducible pipeline.\n",
    "\n",
    "### Numeric pipeline\n",
    "For numeric columns we apply:\n",
    "\n",
    "- **Median imputation**\n",
    "  - Robust against outliers and skewed distributions (common in followers, popularity proxies, etc.)\n",
    "- **Standard scaling (`StandardScaler`)**\n",
    "  - We use `with_mean=False` to remain compatible with sparse matrices produced downstream\n",
    "  - This matters because after One-Hot / multi-hot encoding, the final feature matrix is typically sparse\n",
    "\n",
    "### Categorical pipeline\n",
    "For categorical columns we apply:\n",
    "\n",
    "- **Most-frequent imputation**\n",
    "  - Ensures missing categories don’t break training\n",
    "- **One-Hot encoding (`OneHotEncoder(handle_unknown=\"ignore\")`)**\n",
    "  - Converts categories into binary indicator columns\n",
    "  - `handle_unknown=\"ignore\"` prevents inference crashes when unseen categories appear in new data\n",
    "\n",
    "### Why we build preprocessing as a pipeline\n",
    "Using a pipeline is essential because it:\n",
    "\n",
    "- **Prevents training/serving skew**\n",
    "  - The same preprocessing logic is used during training and during inference (Notebook 4)\n",
    "- **Improves reproducibility**\n",
    "  - Models become portable artifacts (single saved `.joblib` pipeline)\n",
    "- **Supports large-scale inference**\n",
    "  - The output can be sparse (efficient memory and speed for hundreds of thousands to millions of rows)\n"
   ],
   "id": "c998d249068467ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:39.319740Z",
     "start_time": "2025-12-14T17:23:39.313553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def build_preprocessor(X: pd.DataFrame) -> Tuple[ColumnTransformer, List[str], List[str]]:\n",
    "    # Identify numeric columns by dtype\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    # Remaining columns are treated as categorical\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),  # safe when combined with sparse matrices\n",
    "    ])\n",
    "\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", onehot_encoder_compat()),         # should do handle_unknown=\"ignore\"\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_cols),\n",
    "            (\"cat\", cat_pipe, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "    return pre, numeric_cols, categorical_cols\n"
   ],
   "id": "e8c2f0f7dd4ae51a",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train: Track Popularity Regression\n",
    "\n",
    "This step trains a regression model to predict **track popularity** (`tracks.popularity`, typically 0–100).\n",
    "\n",
    "### Why Ridge Regression?\n",
    "We choose **Ridge Regression** as a strong baseline because:\n",
    "\n",
    "- It is **stable and robust** on high-dimensional tabular data\n",
    "- It works very well with **sparse feature matrices**, which we get after:\n",
    "  - One-Hot encoding (categorical features)\n",
    "  - Multi-hot encoding (Top-K genres)\n",
    "- It handles **multicollinearity** (many correlated features) via L2 regularization\n",
    "- It is computationally efficient and scales well to **~300k tracks**\n",
    "\n",
    "### Training setup\n",
    "- We use a standard **train/test split** (e.g., 80/20) with a fixed seed for reproducibility\n",
    "- Preprocessing (imputation + scaling + one-hot) is integrated into the pipeline to avoid leakage and serving skew\n",
    "\n",
    "### Evaluation metrics\n",
    "We evaluate using common regression metrics:\n",
    "\n",
    "- **MAE (Mean Absolute Error)**\n",
    "  Interpretable average absolute deviation in popularity points\n",
    "- **RMSE (Root Mean Squared Error)**\n",
    "  Penalizes larger errors more strongly than MAE\n",
    "- **R² (Coefficient of Determination)**\n",
    "  Measures how much variance is explained by the model\n",
    "\n",
    "These metrics are computed using the project’s `regression_report` helper.\n"
   ],
   "id": "b2cb93c5fc8cab04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:42.050873Z",
     "start_time": "2025-12-14T17:23:39.328570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def sklearn_sanitize_df(X):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        return X\n",
    "\n",
    "    X = X.copy()\n",
    "\n",
    "    # Convert NaT -> np.nan\n",
    "    X = X.replace({pd.NaT: np.nan})\n",
    "\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "\n",
    "        # pandas string or categorical -> object + np.nan\n",
    "        if pd.api.types.is_string_dtype(dt) or isinstance(dt, pd.CategoricalDtype):\n",
    "            X[c] = X[c].astype(\"object\")\n",
    "            X[c] = X[c].where(pd.notna(X[c]), np.nan)\n",
    "\n",
    "        # pandas nullable boolean -> float (0/1/nan)\n",
    "        elif str(dt) == \"boolean\":\n",
    "            X[c] = X[c].astype(\"float64\")\n",
    "\n",
    "        # pandas nullable integer (Int64, Int32...) -> float (so missing -> np.nan)\n",
    "        elif str(dt).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float64\")\n",
    "\n",
    "        # object columns might still contain pd.NA -> replace with np.nan\n",
    "        elif X[c].dtype == \"object\":\n",
    "            X[c] = X[c].where(pd.notna(X[c]), np.nan)\n",
    "\n",
    "    return X\n",
    "\n",
    "sanitize_tf = FunctionTransformer(sklearn_sanitize_df, feature_names_out=\"one-to-one\")\n",
    "\n",
    "\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X_track_pop, y_track_pop_clean,\n",
    "    test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "pre_track, num_cols_track, cat_cols_track = build_preprocessor(X_track_pop)\n",
    "\n",
    "pipe_track_pop = Pipeline(steps=[\n",
    "    (\"sanitize\", sanitize_tf),\n",
    "    (\"pre\", pre_track),\n",
    "    (\"model\", Ridge(alpha=2.0))\n",
    "])\n",
    "\n",
    "pipe_track_pop.fit(Xtr, ytr)\n",
    "pred = pipe_track_pop.predict(Xte)\n",
    "\n",
    "track_pop_metrics = regression_report(yte, pred)\n",
    "\n",
    "dump(pipe_track_pop, PATHS.models_dir / \"03_track_popularity_pipeline.joblib\")"
   ],
   "id": "12474ea3a87cfd0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\models\\\\03_track_popularity_pipeline.joblib']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train: Album Popularity Regression\n",
    "\n",
    "This step trains a regression model to predict **album popularity** (`albums.popularity`, typically 0–100) using the **album-level feature table** built earlier.\n",
    "\n",
    "### Why this model and setup?\n",
    "We use the **same approach as track popularity regression** because the data characteristics are similar:\n",
    "\n",
    "- The feature space can be **high-dimensional** due to:\n",
    "  - One-Hot encoded categorical fields (e.g., `album_type`)\n",
    "  - Optional genre multi-hot vectors (Top-K)\n",
    "- The design matrix is often **sparse**, so we prefer models that handle sparse input efficiently.\n",
    "\n",
    "### Model choice: Ridge Regression\n",
    "**Ridge Regression** is a strong and scalable baseline for album popularity because:\n",
    "\n",
    "- It performs well on linear relationships with many correlated predictors\n",
    "- It remains stable when features are correlated (L2 regularization)\n",
    "- It trains fast on large datasets and works with sparse matrices\n",
    "\n",
    "### Training pipeline (best practice)\n",
    "The pipeline is:\n",
    "\n",
    "1. **Sanitize input** (convert pandas `pd.NA` to `np.nan`, avoid nullable dtypes)\n",
    "2. **Preprocess features** using a `ColumnTransformer`\n",
    "   - numeric: median imputation + scaling\n",
    "   - categorical: most-frequent imputation + one-hot encoding\n",
    "3. **Fit Ridge regression**\n",
    "4. **Evaluate on a held-out test split**\n",
    "5. **Persist the full pipeline** (`.joblib`) for reuse in Notebook 4\n",
    "\n",
    "### Evaluation metrics\n",
    "We report standard regression metrics:\n",
    "\n",
    "- **MAE** — average absolute error in popularity points\n",
    "- **RMSE** — penalizes large errors more strongly\n",
    "- **R²** — explained variance of the target\n",
    "\n",
    "These are computed via your `regression_report` helper.\n"
   ],
   "id": "386178a863105ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:23:43.116196Z",
     "start_time": "2025-12-14T17:23:42.059566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X_album_pop, y_album_pop_clean,\n",
    "    test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "pre_album, num_cols_album, cat_cols_album = build_preprocessor(X_album_pop)\n",
    "\n",
    "pipe_album_pop = Pipeline(steps=[\n",
    "    (\"sanitize\", sanitize_tf),\n",
    "    (\"pre\", pre_album),\n",
    "    (\"model\", Ridge(alpha=2.0))\n",
    "])\n",
    "\n",
    "pipe_album_pop.fit(Xtr, ytr)\n",
    "pred = pipe_album_pop.predict(Xte)\n",
    "\n",
    "album_pop_metrics = regression_report(yte, pred)\n",
    "\n",
    "dump(pipe_album_pop, PATHS.models_dir / \"03_album_popularity_pipeline.joblib\")\n",
    "\n"
   ],
   "id": "901a350c3b3a7361",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\models\\\\03_album_popularity_pipeline.joblib']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train: Hit Prediction (Binary Classification)\n",
    "\n",
    "This step trains a binary classifier to predict whether a track is a **“hit”** (`y_hit ∈ {0,1}`).\n",
    "\n",
    "### Target definition recap (robust + scalable)\n",
    "A track is labeled as a hit using a **time-aware, zero-inflation-aware rule**:\n",
    "\n",
    "1. **Preferred (time-aware):**\n",
    "   If `release_year` is available and there are enough samples per year, we define a hit as:\n",
    "   - **Hit = 1** if track popularity is in the **top `HIT_PERCENTILE`** *within its release year*\n",
    "     (e.g., top 10% of tracks from the same year)\n",
    "\n",
    "2. **Zero-aware fallback:**\n",
    "   Popularity is often **zero-inflated** (many tracks have `popularity = 0`).\n",
    "   If per-year thresholds are not reliable for some rows/years, we use a **global percentile computed on non-zero popularity** to avoid the “everything is a hit” problem.\n",
    "\n",
    "3. **Safety fallback (guarantees two classes):**\n",
    "   If the above would still produce a single-class label (all 0 or all 1), we fallback to a deterministic **top-K rule** to ensure the dataset contains both classes for training.\n",
    "\n",
    "This definition reduces bias across eras and is stable on sampled datasets.\n",
    "\n",
    "### Model choice: SGDClassifier (logistic loss)\n",
    "We use **`SGDClassifier(loss=\"log_loss\")`**, which is effectively a **linear logistic regression model trained with stochastic gradient descent**.\n",
    "\n",
    "Why this model (especially for 300k rows + sparse features)?\n",
    "\n",
    "- Extremely fast and memory-efficient on **large sparse matrices**\n",
    "  - One-Hot encoding and genre multi-hot vectors produce sparse inputs\n",
    "- Scales well to hundreds of thousands (or millions) of rows\n",
    "- Supports probability outputs via `predict_proba` when using `loss=\"log_loss\"`\n",
    "\n",
    "We also set:\n",
    "\n",
    "- `class_weight=\"balanced\"`\n",
    "  to handle class imbalance (hits are rarer than non-hits), preventing the model from collapsing to majority predictions.\n",
    "\n",
    "### Training pipeline (best practice)\n",
    "The pipeline follows a reproducible structure:\n",
    "\n",
    "1. **Sanitize** pandas missing values (`pd.NA → np.nan`) for sklearn stability\n",
    "2. **ColumnTransformer preprocessing**\n",
    "   - numeric: median imputation + scaling (sparse-safe)\n",
    "   - categorical: most-frequent imputation + one-hot encoding (ignore unknown categories)\n",
    "3. **Fit SGD logistic classifier**\n",
    "4. Evaluate on a **stratified** train/test split\n",
    "5. Save the fitted pipeline for Notebook 4 batch scoring\n",
    "\n",
    "### Evaluation metrics\n",
    "We evaluate using metrics suited for imbalanced classification:\n",
    "\n",
    "- **ROC-AUC** — ranking quality across thresholds\n",
    "- **PR-AUC** — more informative when the positive class is rare\n",
    "- **F1 score** — balances precision and recall at a chosen threshold\n",
    "- **Precision/Recall** — interpretable trade-off for business decisions\n",
    "\n",
    "Metrics are computed from predicted probabilities (`predict_proba`) and a chosen decision threshold (default 0.5, but you can tune it to optimize F1/precision/recall).\n"
   ],
   "id": "8cb546342ada5133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:33:26.331983Z",
     "start_time": "2025-12-14T17:32:01.832797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X_track_hit, y_hit_clean,\n",
    "    test_size=0.2, random_state=RANDOM_SEED, stratify=y_hit_clean\n",
    ")\n",
    "\n",
    "pre_hit, _, _ = build_preprocessor(X_track_hit)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "hit_model = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    alpha=1e-4,\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "\n",
    "pipe_hit = Pipeline(steps=[\n",
    "    (\"sanitize\", sanitize_tf),\n",
    "    (\"pre\", pre_hit),\n",
    "    (\"model\", hit_model)\n",
    "])\n",
    "\n",
    "pipe_hit.fit(Xtr, ytr)\n",
    "proba = pipe_hit.predict_proba(Xte)[:, 1]\n",
    "\n",
    "hit_metrics = classification_report_binary(yte, proba, threshold=0.5)\n",
    "\n",
    "\n",
    "dump(pipe_hit, PATHS.models_dir / \"03_hit_pipeline.joblib\")"
   ],
   "id": "2f12c91918a44a15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\models\\\\03_hit_pipeline.joblib']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train: Explicit / Content Prediction (Binary Classification)\n",
    "\n",
    "This step trains a binary classifier to predict whether a track is **explicit** (`y_explicit ∈ {0,1}`).\n",
    "In practice, this is a simple but important “content” prediction task.\n",
    "\n",
    "### Target definition\n",
    "- **Explicit = 1** if the track is marked as explicit in the metadata\n",
    "- **Explicit = 0** otherwise\n",
    "\n",
    "> Note: This target is directly taken from your dataset (no heuristic labeling required).\n",
    "\n",
    "### Model choice: SGDClassifier (logistic loss) — fast on sparse data\n",
    "We use **`SGDClassifier(loss=\"log_loss\")`**, which is effectively a **linear logistic classifier trained with stochastic gradient descent**.\n",
    "\n",
    "Why this model?\n",
    "\n",
    "- Our feature matrix is **high-dimensional and sparse**\n",
    "  - One-Hot encoding for categorical columns\n",
    "  - Multi-hot encoding for genres\n",
    "- `SGDClassifier` scales extremely well to **hundreds of thousands of rows**\n",
    "- It is typically **much faster** than classic `LogisticRegression(saga)` on large sparse inputs\n",
    "- It outputs probabilities via `predict_proba`, enabling threshold tuning\n",
    "\n",
    "We also use:\n",
    "- `class_weight=\"balanced\"` to handle class imbalance\n",
    "- `early_stopping=True` (when enabled) to stop training automatically once the validation score no longer improves\n",
    "\n",
    "### Training pipeline (best practice)\n",
    "The pipeline mirrors the structure used for the hit model:\n",
    "\n",
    "1. **Sanitize** pandas missing values (`pd.NA → np.nan`) for sklearn stability\n",
    "2. **ColumnTransformer preprocessing**\n",
    "   - numeric: median imputation + scaling\n",
    "   - categorical: most-frequent imputation + one-hot encoding (`handle_unknown=\"ignore\"`)\n",
    "3. **Fit SGD logistic classifier**\n",
    "4. Evaluate on a stratified train/test split\n",
    "5. Save the fitted pipeline for Notebook 4 batch scoring\n",
    "\n",
    "### Evaluation metrics\n",
    "Since this is binary classification (and can be imbalanced), we report:\n",
    "\n",
    "- **ROC-AUC**\n",
    "- **PR-AUC**\n",
    "- **F1 score**\n",
    "- **Precision / Recall** at a chosen threshold (default 0.5, can be tuned)\n"
   ],
   "id": "a75db18bc76b57c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:47:05.334917Z",
     "start_time": "2025-12-14T17:46:56.037422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "explicit_model = SGDClassifier(\n",
    "    loss=\"log_loss\",          # logistic regression via SGD\n",
    "    alpha=1e-4,               # regularization strength (tune later)\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "pre_exp, _, _ = build_preprocessor(X_track_explicit)\n",
    "\n",
    "pipe_explicit = Pipeline(steps=[\n",
    "    (\"sanitize\", sanitize_tf),\n",
    "    (\"pre\", pre_exp),\n",
    "    (\"model\", explicit_model)\n",
    "])\n",
    "\n",
    "pipe_explicit.fit(Xtr, ytr)\n",
    "proba = pipe_explicit.predict_proba(Xte)[:, 1]\n",
    "explicit_metrics = classification_report_binary(yte, proba, threshold=0.5)\n",
    "\n",
    "dump(pipe_explicit, PATHS.models_dir / \"03_explicit_pipeline.joblib\")\n"
   ],
   "id": "f3ad93484a89121",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\models\\\\03_explicit_pipeline.joblib']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train: Mood Tags (Multi-label Classification)\n",
    "\n",
    "This step trains a **multi-label classifier** that predicts multiple “mood tags” for each track.\n",
    "\n",
    "### Why multi-label?\n",
    "Mood is not mutually exclusive:\n",
    "- A track can be **happy** *and* **danceable** at the same time.\n",
    "- Therefore the target is a **set of labels per track**, not a single class.\n",
    "\n",
    "In the notebook, mood tags are constructed from audio features using threshold rules (e.g., high `valence` → “happy”, high `energy` → “energetic”).\n",
    "This produces a target matrix:\n",
    "\n",
    "- `Y_mood` with shape `(n_tracks, n_labels)`\n",
    "- each cell is `0/1`\n",
    "\n",
    "### Model choice: One-vs-Rest (OvR) with a sparse-friendly linear base learner\n",
    "We use **OneVsRestClassifier** because it is a standard, scalable approach for multi-label problems:\n",
    "\n",
    "- It trains **one binary classifier per mood label**\n",
    "  - e.g., a separate classifier for `happy`, another for `energetic`, etc.\n",
    "- Each classifier predicts `P(label=1)` independently\n",
    "- Works extremely well with sparse, high-dimensional inputs (OneHot + multi-hot genres)\n",
    "\n",
    "The base model is a **linear classifier** (e.g., Logistic Regression with a sparse-friendly solver or `SGDClassifier(loss=\"log_loss\")`) because:\n",
    "- the feature matrix is sparse and wide\n",
    "- linear models scale well to large datasets\n",
    "- `predict_proba` enables threshold tuning per label (optional advanced step)\n",
    "\n",
    "### Training pipeline (best practice)\n",
    "The multi-label pipeline mirrors the binary pipelines:\n",
    "\n",
    "1. **Sanitize** missing values (`pd.NA → np.nan`)\n",
    "2. **ColumnTransformer preprocessing**\n",
    "   - numeric: median imputation + scaling\n",
    "   - categorical: most-frequent imputation + one-hot encoding\n",
    "3. **Fit OneVsRestClassifier(base_model)** on the multi-label target matrix\n",
    "4. Predict probabilities for each label and convert to binary predictions via a threshold (default `0.5`)\n",
    "5. Save the fitted pipeline for Notebook 4 batch scoring\n",
    "\n",
    "### Evaluation metrics\n",
    "Multi-label evaluation is different from single-label classification. We report:\n",
    "\n",
    "- **Micro F1**\n",
    "  - aggregates contributions of all labels\n",
    "  - sensitive to frequent labels (good global signal)\n",
    "- **Macro F1**\n",
    "  - computes F1 per label and averages them equally\n",
    "  - highlights performance on rare/hard labels\n",
    "- **Per-label F1**\n",
    "  - diagnostic view to see which moods are easy vs difficult\n",
    "\n",
    "These metrics are computed from predicted label probabilities (or binary predictions after thresholding).\n"
   ],
   "id": "6ce6b17219225c65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:54:03.470117Z",
     "start_time": "2025-12-14T17:53:44.594317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, Ytr, Yte = train_test_split(\n",
    "    X_track_mood, Y_mood_clean,\n",
    "    test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "pre_mood, _, _ = build_preprocessor(X_track_mood)\n",
    "\n",
    "base_sgd = SGDClassifier(\n",
    "    loss=\"log_loss\",          # logistic\n",
    "    alpha=1e-4,               # regularization (tune later)\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "mood_model = OneVsRestClassifier(base_sgd, n_jobs=-1)\n",
    "\n",
    "pipe_mood = Pipeline(steps=[\n",
    "    (\"sanitize\", sanitize_tf),\n",
    "    (\"pre\", pre_mood),\n",
    "    (\"model\", mood_model),\n",
    "])\n",
    "\n",
    "pipe_mood.fit(Xtr, Ytr)\n",
    "\n",
    "proba = pipe_mood.predict_proba(Xte)          # (n_samples, n_labels)\n",
    "pred  = (proba >= 0.5).astype(int)            # threshold can be tuned per-label\n",
    "\n",
    "mood_micro_f1 = float(f1_score(Yte, pred, average=\"micro\"))\n",
    "mood_macro_f1 = float(f1_score(Yte, pred, average=\"macro\"))\n",
    "per_label_f1  = {col: float(f1_score(Yte[col], pred[:, i])) for i, col in enumerate(Yte.columns)}\n",
    "\n",
    "mood_metrics = {\n",
    "    \"micro_f1\": mood_micro_f1,\n",
    "    \"macro_f1\": mood_macro_f1,\n",
    "    \"per_label_f1\": per_label_f1\n",
    "}\n",
    "\n",
    "dump(pipe_mood, PATHS.models_dir / \"03_mood_pipeline.joblib\")\n",
    "mood_metrics\n"
   ],
   "id": "72d4a932477b0c40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'micro_f1': 0.9299329063738945,\n",
       " 'macro_f1': 0.9313848087716066,\n",
       " 'per_label_f1': {'energetic': 0.9760701496034003,\n",
       "  'danceable': 0.8377549281372493,\n",
       "  'acoustic': 0.9797505605381166,\n",
       "  'instrumental': 0.9684679976307445,\n",
       "  'happy': 0.9361661766100731,\n",
       "  'sad': 0.861611300838249,\n",
       "  'chill': 0.9598725480434134}}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Artist Clustering (Unsupervised)\n",
    "\n",
    "This step builds **artist clusters/segments** for exploration and potential downstream use (e.g., recommendations, catalog segmentation, marketing personas, or as features in other models).\n",
    "\n",
    "> Note: This is not true graph-based “community detection” (like Louvain/Leiden on an artist collaboration graph).\n",
    "> Instead, it is a **scalable proxy** using feature-based clustering.\n",
    "\n",
    "### Goal\n",
    "- Group artists into meaningful segments based on:\n",
    "  - popularity / followers\n",
    "  - catalog size and track statistics\n",
    "  - (optionally) genre profile via multi-hot encoding\n",
    "\n",
    "### Approach overview\n",
    "We follow a standard unsupervised ML pipeline:\n",
    "\n",
    "1. **Select artist-level features**\n",
    "   - numeric artist attributes (e.g., followers, popularity, track aggregates)\n",
    "   - optional genre multi-hot vectors (Top-K genres)\n",
    "\n",
    "2. **Impute missing values**\n",
    "   - use median imputation to keep clustering stable and avoid dropping artists\n",
    "\n",
    "3. **Scale features**\n",
    "   - clustering is distance-based, so scaling is required\n",
    "   - we use standardization (mean=0, std=1)\n",
    "   - `with_mean=True` is fine here because we work with a dense numeric matrix at this stage\n",
    "\n",
    "4. **Dimensionality reduction (PCA)**\n",
    "   - reduces noise and feature collinearity\n",
    "   - improves KMeans stability and speed on high-dimensional inputs\n",
    "   - we keep a bounded number of components (e.g., up to 30)\n",
    "\n",
    "5. **KMeans clustering**\n",
    "   - assigns each artist to one of `K` clusters\n",
    "   - produces a `cluster` label per artist which can be saved and reused later\n",
    "\n",
    "6. **Optional visualization (t-SNE on a sample)**\n",
    "   - t-SNE is expensive, so we run it only on a random subset\n",
    "   - useful for inspecting whether clusters visually separate (diagnostics only)\n",
    "\n",
    "### Outputs\n",
    "- `artist_df[\"cluster\"]`: the assigned cluster label per artist\n",
    "- saved clustering artifacts (preprocessing + PCA + KMeans) for reuse in Notebook 4 and future analysis\n"
   ],
   "id": "1dd7f6539a1d9c3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:54:43.445250Z",
     "start_time": "2025-12-14T17:54:16.416635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ARTIST_NUM = [\n",
    "    \"popularity\", \"followers\", \"log_followers\",\n",
    "    \"n_tracks\", \"log_n_tracks\",\n",
    "    \"track_pop_mean\", \"explicit_rate\",\n",
    "] + [c for c in artist_df.columns if c.startswith(\"mean_\")]\n",
    "\n",
    "ARTIST_NUM = [c for c in ARTIST_NUM if c in artist_df.columns]\n",
    "X_artist_base = artist_df[ARTIST_NUM].copy()\n",
    "\n",
    "# Optional: add genre multihot (numeric features)\n",
    "X_artist = pd.concat([X_artist_base.reset_index(drop=True), artist_genre_mh.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Keep only numeric columns for clustering (KMeans requires numeric)\n",
    "num_cols = [c for c in X_artist.columns if pd.api.types.is_numeric_dtype(X_artist[c])]\n",
    "X_num = X_artist[num_cols].copy()\n",
    "\n",
    "cluster_pre = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True)),\n",
    "])\n",
    "\n",
    "X_scaled = cluster_pre.fit_transform(X_num)\n",
    "\n",
    "# PCA: reduce dims to make clustering easier and more stable\n",
    "pca_components = min(30, X_scaled.shape[1])\n",
    "pca = PCA(n_components=pca_components, random_state=RANDOM_SEED)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# KMeans clusters\n",
    "kmeans = kmeans_compat(n_clusters=K_CLUSTERS, random_state=RANDOM_SEED)\n",
    "artist_clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "artist_df[\"cluster\"] = artist_clusters\n",
    "\n",
    "SAMPLE_TSNE = min(TSNE_SAMPLE_MAX, len(artist_df))\n",
    "sample_idx = np.random.choice(len(artist_df), size=SAMPLE_TSNE, replace=False)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"pca\", random_state=RANDOM_SEED)\n",
    "X_tsne = tsne.fit_transform(X_pca[sample_idx])\n",
    "\n",
    "artist_cluster_artifact = {\n",
    "    \"k\": int(K_CLUSTERS),\n",
    "    \"pca_components\": int(pca_components),\n",
    "    \"pca_explained_variance_ratio_sum\": float(np.sum(pca.explained_variance_ratio_)),\n",
    "    \"tsne_sample_n\": int(SAMPLE_TSNE),\n",
    "}\n",
    "\n",
    "# Save clustering artifacts for reuse (Notebook 4 or analysis)\n",
    "cluster_bundle = {\n",
    "    \"cluster_pre\": cluster_pre,\n",
    "    \"pca\": pca,\n",
    "    \"kmeans\": kmeans,\n",
    "    \"numeric_columns_used\": num_cols,\n",
    "    \"k\": int(K_CLUSTERS),\n",
    "}\n",
    "dump(cluster_bundle, PATHS.models_dir / \"03_artist_clustering.joblib\")\n",
    "artist_df.to_parquet(PATHS.modeling_dir / \"artist_dataset_with_clusters.parquet\", index=False)\n"
   ],
   "id": "9201fbc9a264459a",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save `feature_config.json` (Critical for Notebook 4)\n",
    "\n",
    "At this point we export a **`feature_config.json`** file.\n",
    "This file acts as the **scoring contract** between Notebook 3 (training) and Notebook 4 (full-data batch scoring).\n",
    "\n",
    "### Why this file is critical\n",
    "When you score millions of rows later, you must apply **exactly the same rules** as during training.\n",
    "`feature_config.json` makes the pipeline reproducible by storing the key decisions that would otherwise be “hidden” in notebook code.\n",
    "\n",
    "### What we store in `feature_config.json`\n",
    "The config contains the most important pieces of information needed for consistent inference:\n",
    "\n",
    "- **Feature lists**\n",
    "  - which numeric columns are used\n",
    "  - which categorical columns are used\n",
    "  - which multi-hot genre columns were created\n",
    "\n",
    "- **Genre encoding contract**\n",
    "  - the exact `top_genres` list used for Top-K multi-hot encoding\n",
    "  (same order and same set → ensures consistent column layout)\n",
    "\n",
    "- **Mood tag definitions**\n",
    "  - the thresholds used to create each mood label (e.g., quantiles on `valence`, `energy`, etc.)\n",
    "  - the list of mood tags and their logic (`gt` / `lt`)\n",
    "\n",
    "- **Hit label definition**\n",
    "  - parameters used to create the hit target (e.g., `HIT_PERCENTILE`, fallback rule)\n",
    "  - ensures we can reproduce hit labeling consistently across datasets\n",
    "\n",
    "- **Run metadata (optional but recommended)**\n",
    "  - timestamp / sampling settings / dataset version identifiers\n",
    "  - helps trace which config belongs to which model training run\n"
   ],
   "id": "a9ccc94ddbac0c3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:56:17.619552Z",
     "start_time": "2025-12-14T17:56:17.608760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_config = {\n",
    "    \"run_meta\": RUN_META,\n",
    "    \"top_genres\": top_genres,\n",
    "    \"mood_thresholds\": {str(k): v for k, v in mood_thresholds.items()},  # keys must be JSON-serializable\n",
    "    \"mood_tags\": MOOD_TAGS,\n",
    "    \"track_features\": {\n",
    "        \"numeric\": TRACK_NUMERIC,\n",
    "        \"categorical\": TRACK_CATEGORICAL,\n",
    "        \"genre_multi_hot_cols\": list(track_genre_mh.columns),\n",
    "    },\n",
    "    \"album_features\": {\n",
    "        \"numeric\": ALBUM_NUMERIC,\n",
    "        \"categorical\": ALBUM_CATEGORICAL,\n",
    "        \"genre_multi_hot_cols\": list(album_genre_mh.columns),\n",
    "    },\n",
    "    \"artist_features\": {\n",
    "        \"numeric_used_for_clustering\": num_cols,\n",
    "        \"genre_multi_hot_cols\": list(artist_genre_mh.columns),\n",
    "        \"kmeans_k\": int(K_CLUSTERS),\n",
    "    },\n",
    "    \"targets\": {\n",
    "        \"hit_percentile_within_year\": float(HIT_PERCENTILE),\n",
    "        \"hit_fallback_popularity_threshold\": int(HIT_FALLBACK_POP_THRESHOLD),\n",
    "    }\n",
    "}\n",
    "\n",
    "(PATHS.models_dir / \"feature_config.json\").write_text(json.dumps(feature_config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved feature_config.json\")\n"
   ],
   "id": "fe4f5c2e9754f8d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature_config.json\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Write Reports (JSON)\n",
    "\n",
    "In this step we persist the most important outputs of Notebook 3 as **machine-readable reports**.\n",
    "\n",
    "### Why we write JSON reports\n",
    "Saving metrics only as printed notebook output is not reproducible.\n",
    "A JSON report allows you to:\n",
    "\n",
    "- compare experiments across runs (baseline vs tuned models)\n",
    "- track progress over time (model improvements)\n",
    "- integrate results into dashboards or CI pipelines\n",
    "- make Notebook 4 and later steps auditable\n",
    "\n",
    "### What we store\n",
    "The report typically includes:\n",
    "\n",
    "- **Model metrics**\n",
    "  - track popularity regression (MAE / RMSE / R²)\n",
    "  - album popularity regression (MAE / RMSE / R²)\n",
    "  - hit prediction (ROC-AUC / PR-AUC / F1 / precision / recall)\n",
    "  - explicit prediction (ROC-AUC / PR-AUC / F1 / precision / recall)\n",
    "  - mood multi-label metrics (micro F1 / macro F1 / per-label F1)\n",
    "  - artist clustering artifacts (e.g., `k`, PCA variance explained)\n",
    "\n",
    "- **Dataset shapes**\n",
    "  - shapes of the prepared datasets (`track_df`, `album_df`, `artist_df`)\n",
    "  - shapes of the feature matrices used for training\n",
    "  - helps detect accidental row drops or mismatched joins later\n",
    "\n",
    "### Output location\n",
    "The JSON report is written into the `reports/` directory (e.g. `metrics_report.json`).\n",
    "This becomes the single source of truth for model performance for this run.\n"
   ],
   "id": "8426d9b6ec2fe819"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:58:10.597152Z",
     "start_time": "2025-12-14T17:58:10.586384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reports = {\n",
    "    \"track_popularity_regression\": track_pop_metrics,\n",
    "    \"album_popularity_regression\": album_pop_metrics,\n",
    "    \"hit_prediction\": hit_metrics,\n",
    "    \"explicit_prediction\": explicit_metrics,\n",
    "    \"mood_multilabel\": mood_metrics,\n",
    "    \"artist_clustering\": artist_cluster_artifact,\n",
    "    \"dataset_shapes\": {\n",
    "        \"track_df\": [int(track_df.shape[0]), int(track_df.shape[1])],\n",
    "        \"album_df\": [int(album_df.shape[0]), int(album_df.shape[1])],\n",
    "        \"artist_df\": [int(artist_df.shape[0]), int(artist_df.shape[1])],\n",
    "        \"X_track_pop\": [int(X_track_pop.shape[0]), int(X_track_pop.shape[1])],\n",
    "        \"X_album_pop\": [int(X_album_pop.shape[0]), int(X_album_pop.shape[1])],\n",
    "    },\n",
    "}\n",
    "\n",
    "(PATHS.reports_dir / \"metrics_report.json\").write_text(json.dumps(reports, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote metrics report:\", PATHS.reports_dir / \"metrics_report.json\")\n"
   ],
   "id": "c5bddb5838d1722a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote metrics report: ..\\data\\reports\\03_target_and_features\\metrics_report.json\n"
     ]
    }
   ],
   "execution_count": 145
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
