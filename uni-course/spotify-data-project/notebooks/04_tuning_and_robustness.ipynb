{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04 — Hypertuning (Baseline laden → Tuning → Hypertuned speichern)\n",
    "\n",
    "Dieses Notebook:\n",
    "1) lädt die Baseline-Modelle aus `models/baseline/`\n",
    "2) lädt die Modell-Datasets (Features/Targets)\n",
    "3) evaluiert Baseline-Modelle als Referenz\n",
    "4) führt Hypertuning mit RandomizedSearchCV durch\n",
    "5) evaluiert die besten Modelle auf dem Testset\n",
    "6) speichert die hypertuned Modelle nach `models/hypertuned/`\n",
    "7) schreibt Reports (`json`) für spätere Notebooks\n"
   ],
   "id": "ef817af335343aad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "b22586766eee8ccc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:54.732319Z",
     "start_time": "2025-12-31T15:29:51.271484Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import load, dump\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konfiguration",
   "id": "c25f913ba933ccdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:54.776569Z",
     "start_time": "2025-12-31T15:29:54.762457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = globals().get(\"RANDOM_SEED\", 42)\n",
    "\n",
    "BASE_EXPORT_DIR = Path(\"../data/interim/converted_sqlite_samples\")\n",
    "CURRENT_SAMPLE_PATH = BASE_EXPORT_DIR / \"current_sample.json\"\n",
    "cfg = json.loads(CURRENT_SAMPLE_PATH.read_text())\n",
    "SAMPLE_NAME = cfg[\"SAMPLE_NAME\"]\n",
    "\n",
    "# Ordnerstruktur (wie du es beschrieben hast)\n",
    "MODELS_BASELINE_DIR = Path(\"../data/models/baseline\") / SAMPLE_NAME\n",
    "MODELS_HYPER_DIR = Path(\"../data/models/hypertuned\") / SAMPLE_NAME\n",
    "MODELS_HYPER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reports\n",
    "REPORT_PATH = Path(\"../data/reports/04_hypertuning\") / SAMPLE_NAME\n",
    "BEST_PARAMS_PATH = Path(\"../data/reports/04_hypertuning\") / SAMPLE_NAME\n",
    "\n",
    "REPORT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PARAMS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "TUNE_ITER_REG = 18\n",
    "TUNE_ITER_CLS = 22\n",
    "CV_FOLDS = 3\n",
    "\n",
    "\n",
    "# Ausgabe-Container\n",
    "report: Dict[str, Any] = {}\n",
    "best_params: Dict[str, Any] = {}\n"
   ],
   "id": "256001d23af8e629",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "eb274d63d8a17e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:54.848284Z",
     "start_time": "2025-12-31T15:29:54.826559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _identity(X):\n",
    "    return X\n",
    "\n",
    "def build_preprocessor_tree(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für XGBoost/Tree-Modelle (ohne Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def build_preprocessor_linear(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für lineare Modelle (mit Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def regression_report(y_true, y_pred) -> Dict[str, float]:\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def classification_report_binary(y_true, proba, threshold=0.5) -> Dict[str, Any]:\n",
    "    pred = (proba >= threshold).astype(int)\n",
    "    roc = float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    pr = float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    f1 = float(f1_score(y_true, pred))\n",
    "    cm = confusion_matrix(y_true, pred).tolist()\n",
    "    return {\"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"confusion_matrix\": cm}\n",
    "\n",
    "def best_f1_threshold(y_true, proba, thresholds=np.linspace(0.05, 0.95, 19)):\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in thresholds:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "\n",
    "from sanityze import  sklearn_sanitize_df\n",
    "\n",
    "\n",
    "sanitize_tf = FunctionTransformer(sklearn_sanitize_df, feature_names_out=\"one-to-one\")"
   ],
   "id": "1aacd242493bd3d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten Laden",
   "id": "1adca59ee8b67d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:55.819875Z",
     "start_time": "2025-12-31T15:29:54.861608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = Path(\"../data/datasets\") / SAMPLE_NAME\n",
    "\n",
    "def _load_parquet(p: Path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {p}\")\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "files = {\n",
    "    \"X_track_pop\": DATA_DIR / \"X_track_pop.parquet\",\n",
    "    \"y_track_pop\": DATA_DIR / \"y_track_pop.parquet\",\n",
    "    \"X_album_pop\": DATA_DIR / \"X_album_pop.parquet\",\n",
    "    \"y_album_pop\": DATA_DIR / \"y_album_pop.parquet\",\n",
    "    \"X_track_hit\": DATA_DIR / \"X_track_hit.parquet\",\n",
    "    \"y_hit\": DATA_DIR / \"y_hit.parquet\",\n",
    "    \"X_track_explicit\": DATA_DIR / \"X_track_explicit.parquet\",\n",
    "    \"y_explicit\": DATA_DIR / \"y_explicit.parquet\",\n",
    "    \"X_track_mood\": DATA_DIR / \"X_track_mood.parquet\",\n",
    "    \"Y_mood\": DATA_DIR / \"Y_mood.parquet\",\n",
    "}\n",
    "\n",
    "X_track_pop = _load_parquet(files[\"X_track_pop\"])\n",
    "y_track_pop = _load_parquet(files[\"y_track_pop\"]).squeeze()\n",
    "\n",
    "X_album_pop = _load_parquet(files[\"X_album_pop\"])\n",
    "y_album_pop = _load_parquet(files[\"y_album_pop\"]).squeeze()\n",
    "\n",
    "X_track_hit = _load_parquet(files[\"X_track_hit\"])\n",
    "y_hit = _load_parquet(files[\"y_hit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_explicit = _load_parquet(files[\"X_track_explicit\"])\n",
    "y_explicit = _load_parquet(files[\"y_explicit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_mood = _load_parquet(files[\"X_track_mood\"])\n",
    "y_mood = _load_parquet(files[\"Y_mood\"])"
   ],
   "id": "fe46e569ed6f7e83",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:55.843798Z",
     "start_time": "2025-12-31T15:29:55.834802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X_track_pop:\", X_track_mood.shape)\n",
    "print(\"y_track_pop:\", y_mood.shape)\n",
    "print(\"X index unique:\", X_track_pop.index.is_unique)\n",
    "print(\"y index unique:\", y_track_pop.index.is_unique)\n",
    "print(\"Index equal:\", X_track_pop.index.equals(y_track_pop.index))\n"
   ],
   "id": "7c9e4a565e2e9e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_track_pop: (294616, 70)\n",
      "y_track_pop: (294616, 7)\n",
      "X index unique: True\n",
      "y index unique: True\n",
      "Index equal: True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline-Modelle laden",
   "id": "92edf052f9efe44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:56.757104Z",
     "start_time": "2025-12-31T15:29:55.860227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_paths = {\n",
    "    \"track_popularity\": MODELS_BASELINE_DIR / \"03_track_popularity_pipeline_xgb.joblib\",\n",
    "    \"album_popularity\": MODELS_BASELINE_DIR / \"03_album_popularity_pipeline_xgb.joblib\",\n",
    "    \"hit\": MODELS_BASELINE_DIR / \"03_hit_pipeline_xgb.joblib\",\n",
    "    \"explicit\": MODELS_BASELINE_DIR / \"03_explicit_pipeline_xgb.joblib\",\n",
    "    \"mood\": MODELS_BASELINE_DIR / \"03_mood_pipeline.joblib\",\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "for k, p in baseline_paths.items():\n",
    "    if p.exists():\n",
    "        baseline_models[k] = load(p)\n",
    "    else:\n",
    "        print(f\"Baseline nicht gefunden (übersprungen): {p}\")\n",
    "\n",
    "list(baseline_models.keys())\n"
   ],
   "id": "2666e1a630658fe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_popularity', 'album_popularity', 'hit', 'explicit', 'mood']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gemeinsame Splits (Baseline & Hypertuned müssen gleich evaluieren)",
   "id": "d1b7190b41b089a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:57.616715Z",
     "start_time": "2025-12-31T15:29:56.841484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = {}\n",
    "\n",
    "# Regression Splits (kein stratify)\n",
    "splits[\"track_pop\"] = train_test_split(\n",
    "    X_track_pop, y_track_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "splits[\"album_pop\"] = train_test_split(\n",
    "    X_album_pop, y_album_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Classification Splits (stratifiziert)\n",
    "splits[\"hit\"] = train_test_split(\n",
    "    X_track_hit, y_hit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_hit\n",
    ")\n",
    "splits[\"explicit\"] = train_test_split(\n",
    "    X_track_explicit, y_explicit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_explicit\n",
    ")\n",
    "\n",
    "# Mood optional\n",
    "splits[\"mood\"] = train_test_split(\n",
    "        X_track_mood, y_mood, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n"
   ],
   "id": "71799f0884756e07",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tuning-Funktion",
   "id": "133f49c1885a134c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:29:57.654823Z",
     "start_time": "2025-12-31T15:29:57.629750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tempfile\n",
    "from joblib import Memory\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class XGBRegressorES(XGBRegressor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        valid_size: float = 0.15,\n",
    "        early_stopping_rounds: int = 50,\n",
    "        eval_metric: str = \"mae\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.valid_size = valid_size\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.eval_metric = eval_metric\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        rs = getattr(self, \"random_state\", None)\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X, y, test_size=self.valid_size, random_state=rs\n",
    "        )\n",
    "\n",
    "        # NEW API: set on estimator, not in fit()\n",
    "        params = {}\n",
    "        if self.eval_metric is not None:\n",
    "            params[\"eval_metric\"] = self.eval_metric\n",
    "        if self.early_stopping_rounds is not None:\n",
    "            params[\"early_stopping_rounds\"] = self.early_stopping_rounds\n",
    "        # set_params is safe here\n",
    "        self.set_params(**params)\n",
    "\n",
    "        fit_kwargs = dict(kwargs)\n",
    "        fit_kwargs.setdefault(\"eval_set\", [(X_val, y_val)])\n",
    "        fit_kwargs.setdefault(\"verbose\", False)\n",
    "\n",
    "        return super().fit(X_tr, y_tr, **fit_kwargs)\n",
    "\n",
    "\n",
    "class XGBClassifierES(XGBClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        valid_size: float = 0.15,\n",
    "        early_stopping_rounds: int = 50,\n",
    "        eval_metric: str = \"aucpr\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.valid_size = valid_size\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.eval_metric = eval_metric\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        rs = getattr(self, \"random_state\", None)\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X, y, test_size=self.valid_size, random_state=rs, stratify=y\n",
    "        )\n",
    "\n",
    "        params = {}\n",
    "        if self.eval_metric is not None:\n",
    "            params[\"eval_metric\"] = self.eval_metric\n",
    "        if self.early_stopping_rounds is not None:\n",
    "            params[\"early_stopping_rounds\"] = self.early_stopping_rounds\n",
    "        self.set_params(**params)\n",
    "\n",
    "        fit_kwargs = dict(kwargs)\n",
    "        fit_kwargs.setdefault(\"eval_set\", [(X_val, y_val)])\n",
    "        fit_kwargs.setdefault(\"verbose\", False)\n",
    "\n",
    "        return super().fit(X_tr, y_tr, **fit_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def tune_xgb_regression(Xtr, ytr, preprocessor):\n",
    "    # cache transformers to avoid repeating sanitize+pre for each hyperparam candidate (per fold)\n",
    "    cache_dir = os.path.join(tempfile.gettempdir(), \"sklearn_xgb_cache\")\n",
    "    memory = Memory(location=cache_dir, verbose=0)\n",
    "\n",
    "    # NOTE: set n_estimators high; early stopping + halving controls effective trees\n",
    "    base = XGBRegressorES(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1,                    # CV parallel, XGB single-thread\n",
    "        n_estimators=3000,           # upper bound; early stopping will cut it\n",
    "        early_stopping_rounds=50,\n",
    "        valid_size=0.15,\n",
    "        eval_metric=\"mae\",\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"sanitize\", sanitize_tf), (\"pre\", preprocessor), (\"model\", base)],\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    # smaller + practical ranges (big speedup, still strong)\n",
    "    param_dist = {\n",
    "        \"model__learning_rate\": loguniform(0.02, 0.12),\n",
    "        \"model__max_depth\": randint(3, 9),\n",
    "        \"model__min_child_weight\": loguniform(0.5, 10.0),\n",
    "        \"model__subsample\": uniform(0.7, 0.3),          # 0.7–1.0\n",
    "        \"model__colsample_bytree\": uniform(0.7, 0.3),   # 0.7–1.0\n",
    "        \"model__gamma\": loguniform(1e-8, 2.0),\n",
    "        \"model__reg_lambda\": loguniform(1e-2, 20.0),\n",
    "        \"model__reg_alpha\": loguniform(1e-8, 2.0),\n",
    "    }\n",
    "\n",
    "    # Fast path: Successive halving over n_estimators (resource)\n",
    "    try:\n",
    "        from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "        from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "        search = HalvingRandomSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=CV_FOLDS,\n",
    "            n_candidates=TUNE_ITER_REG,      # e.g. 20–60\n",
    "            factor=3,\n",
    "            resource=\"model__n_estimators\",\n",
    "            min_resources=200,\n",
    "            max_resources=3000,\n",
    "            aggressive_elimination=True,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score=\"raise\",\n",
    "        )\n",
    "    except Exception:\n",
    "        # Fallback: faster randomized search (still capped)\n",
    "        param_dist_fallback = dict(param_dist)\n",
    "        param_dist_fallback[\"model__n_estimators\"] = randint(400, 2500)\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_dist_fallback,\n",
    "            n_iter=min(TUNE_ITER_REG, 40),\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=CV_FOLDS,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            error_score=\"raise\",\n",
    "        )\n",
    "\n",
    "    search.fit(Xtr, ytr)\n",
    "    return search\n",
    "\n",
    "\n",
    "def tune_xgb_classification(Xtr, ytr, preprocessor, scale_pos_weight: float):\n",
    "    cache_dir = os.path.join(tempfile.gettempdir(), \"sklearn_xgb_cache\")\n",
    "    memory = Memory(location=cache_dir, verbose=0)\n",
    "\n",
    "    base = XGBClassifierES(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        tree_method=\"hist\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1,\n",
    "        n_estimators=4000,           # upper bound; early stopping will cut it\n",
    "        early_stopping_rounds=50,\n",
    "        valid_size=0.15,\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"sanitize\", sanitize_tf), (\"pre\", preprocessor), (\"model\", base)],\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__learning_rate\": loguniform(0.02, 0.12),\n",
    "        \"model__max_depth\": randint(3, 8),\n",
    "        \"model__min_child_weight\": loguniform(0.5, 10.0),\n",
    "        \"model__subsample\": uniform(0.7, 0.3),\n",
    "        \"model__colsample_bytree\": uniform(0.7, 0.3),\n",
    "        \"model__gamma\": loguniform(1e-8, 2.0),\n",
    "        \"model__reg_lambda\": loguniform(1e-2, 20.0),\n",
    "        \"model__reg_alpha\": loguniform(1e-8, 2.0),\n",
    "        \"model__max_delta_step\": randint(0, 6),   # small range; enough\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "        from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "        search = HalvingRandomSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            scoring=\"average_precision\",\n",
    "            cv=CV_FOLDS,\n",
    "            n_candidates=TUNE_ITER_CLS,      # e.g. 20–60\n",
    "            factor=3,\n",
    "            resource=\"model__n_estimators\",\n",
    "            min_resources=200,\n",
    "            max_resources=4000,\n",
    "            aggressive_elimination=True,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score=\"raise\",\n",
    "        )\n",
    "    except Exception:\n",
    "        param_dist_fallback = dict(param_dist)\n",
    "        param_dist_fallback[\"model__n_estimators\"] = randint(400, 3000)\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_dist_fallback,\n",
    "            n_iter=min(TUNE_ITER_CLS, 40),\n",
    "            scoring=\"average_precision\",\n",
    "            cv=CV_FOLDS,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            error_score=\"raise\",\n",
    "        )\n",
    "\n",
    "    search.fit(Xtr, ytr)\n",
    "    return search\n"
   ],
   "id": "5c0da7d1890aab69",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Track Popularity Tuning",
   "id": "bcfdbd56221c220a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:47:06.196324Z",
     "start_time": "2025-12-31T15:29:57.666700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"track_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_pop)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"track_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"track_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_track_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"track_popularity\"]"
   ],
   "id": "fdf3cb7380a9052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 200\n",
      "max_resources_: 3000\n",
      "aggressive_elimination: True\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 18\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 600\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 1800\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\GitHub\\data-science\\.venv\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [17:44:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"valid_size\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'MAE': 5.818463971111249,\n",
       "  'RMSE': 8.810495900618461,\n",
       "  'R2': 0.8384540900560095},\n",
       " 'cv_best_mae': 5.871692883729415}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Album Popularity Tuning",
   "id": "5f7d9f3811ae8f85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:55:10.575361Z",
     "start_time": "2025-12-31T15:47:06.243181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"album_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_album_pop)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"album_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"album_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_album_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"album_popularity\"]\n"
   ],
   "id": "95e07df45a9f421f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 200\n",
      "max_resources_: 3000\n",
      "aggressive_elimination: True\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 18\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 6\n",
      "n_resources: 600\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 1800\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\GitHub\\data-science\\.venv\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [17:53:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"valid_size\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'MAE': 7.5831143772808005,\n",
       "  'RMSE': 10.86154944847349,\n",
       "  'R2': 0.7490514018937675},\n",
       " 'cv_best_mae': 7.70479044651911}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hit Tuning + Threshhold\n",
   "id": "1e036896b3a25ca4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-31T15:55:10.618497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"hit\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_hit)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"hit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"hit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_hit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"hit_prediction\"]\n"
   ],
   "id": "67fda4b58ececf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 200\n",
      "max_resources_: 4000\n",
      "aggressive_elimination: True\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 22\n",
      "n_resources: 200\n",
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Explicit Tuning + Threshold",
   "id": "e68005067d391d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"explicit\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_explicit)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"explicit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"explicit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_explicit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"explicit_prediction\"]\n"
   ],
   "id": "5b3c93a70e7af2e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mood Tuning",
   "id": "6e229830d3bfb21c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def tune_logreg_multilabel_mood(Xtr, Ytr, preprocessor):\n",
    "    # Cache preprocessing across CV folds & candidates\n",
    "    cache_dir = os.path.join(tempfile.gettempdir(), \"sklearn_linear_cache\")\n",
    "    memory = Memory(location=cache_dir, verbose=0)\n",
    "\n",
    "    base = OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            solver=\"saga\",\n",
    "            max_iter=300,           # will be controlled by halving (resource)\n",
    "            tol=1e-3,               # practical default; can be tuned\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=1                # keep single-thread; CV parallel outside\n",
    "        ),\n",
    "        n_jobs=1                   # avoid oversubscription with CV n_jobs=-1\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"sanitize\", sanitize_tf), (\"pre\", preprocessor), (\"model\", base)],\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    # Conditional distributions: l1_ratio only when elasticnet\n",
    "    param_dist = [\n",
    "        {\n",
    "            \"model__estimator__penalty\": [\"l2\"],\n",
    "            \"model__estimator__C\": loguniform(1e-2, 20.0),\n",
    "            \"model__estimator__tol\": loguniform(1e-4, 1e-2),\n",
    "        },\n",
    "        {\n",
    "            \"model__estimator__penalty\": [\"elasticnet\"],\n",
    "            \"model__estimator__C\": loguniform(1e-2, 20.0),\n",
    "            \"model__estimator__l1_ratio\": uniform(0.0, 1.0),\n",
    "            \"model__estimator__tol\": loguniform(1e-4, 1e-2),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Fast path: HalvingRandomSearchCV (massive speedup vs full 2000 iters always)\n",
    "    try:\n",
    "        from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "        from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "        search = HalvingRandomSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            scoring=\"f1_micro\",\n",
    "            cv=CV_FOLDS,                 # use 3 if you can, 2 if speed is critical\n",
    "            n_candidates=max(12, TUNE_ITER_CLS),\n",
    "            factor=3,\n",
    "            resource=\"model__estimator__max_iter\",\n",
    "            min_resources=100,\n",
    "            max_resources=2000,\n",
    "            aggressive_elimination=True,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score=\"raise\"\n",
    "        )\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback: faster randomized search with capped max_iter range\n",
    "        param_dist_fallback = [\n",
    "            {\n",
    "                **d,\n",
    "                \"model__estimator__max_iter\": [200, 400, 800, 1200, 2000],\n",
    "            }\n",
    "            for d in param_dist\n",
    "        ]\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_distributions=param_dist_fallback,\n",
    "            n_iter=12,\n",
    "            scoring=\"f1_micro\",\n",
    "            cv=2,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            error_score=\"raise\"\n",
    "        )\n",
    "\n",
    "    search.fit(Xtr, Ytr)\n",
    "    return search\n"
   ],
   "id": "cc6986cb141b726b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Xtr, Xte, Ytr, Yte = splits[\"mood\"]\n",
    "pre, _, _ = build_preprocessor_linear(X_track_mood)\n",
    "search = tune_logreg_multilabel_mood(Xtr, Ytr, pre)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "Ypred = best_model.predict(Xte)\n",
    "\n",
    "micro_f1 = float(f1_score(Yte, Ypred, average=\"micro\"))\n",
    "macro_f1 = float(f1_score(Yte, Ypred, average=\"macro\"))\n",
    "\n",
    "report[\"mood_multilabel\"] = {\n",
    "    \"hypertuned\": {\"micro_f1\": micro_f1, \"macro_f1\": macro_f1},\n",
    "    \"cv_best_f1_micro\": float(search.best_score_)\n",
    "}\n",
    "best_params[\"mood_multilabel\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_mood_multilabel_logreg_hypertuned.joblib\")\n"
   ],
   "id": "bbbfd4813f088078",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reports Saving",
   "id": "64c200f35d048540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write hypertuning report\n",
    "with open(REPORT_PATH / \"04_hypertuning_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Write best parameters\n",
    "with open(BEST_PARAMS_PATH / \"04_hypertuning_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(\"Hypertuning fertig. Gespeichert unter:\")\n",
    "print(\" -\", MODELS_HYPER_DIR)\n",
    "print(\" -\", REPORT_PATH)\n",
    "print(\" -\", BEST_PARAMS_PATH)\n"
   ],
   "id": "1f02a85343f7a96d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
