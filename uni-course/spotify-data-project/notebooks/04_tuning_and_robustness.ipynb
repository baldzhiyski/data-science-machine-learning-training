{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 04 — Hypertuning (Baseline laden → Tuning → Hypertuned speichern)\n",
    "\n",
    "Dieses Notebook:\n",
    "1) lädt die Baseline-Modelle aus `models/baseline/`\n",
    "2) lädt die Modell-Datasets (Features/Targets)\n",
    "3) evaluiert Baseline-Modelle als Referenz\n",
    "4) führt Hypertuning mit RandomizedSearchCV durch\n",
    "5) evaluiert die besten Modelle auf dem Testset\n",
    "6) speichert die hypertuned Modelle nach `models/hypertuned/`\n",
    "7) schreibt Reports (`json`) für spätere Notebooks\n"
   ],
   "id": "ef817af335343aad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "b22586766eee8ccc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-29T07:48:49.771808Z",
     "start_time": "2025-12-29T07:48:36.253509Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import load, dump\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Konfiguration",
   "id": "c25f913ba933ccdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:02.265797Z",
     "start_time": "2025-12-29T07:49:02.246742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = globals().get(\"RANDOM_SEED\", 42)\n",
    "\n",
    "# Ordnerstruktur (wie du es beschrieben hast)\n",
    "MODELS_BASELINE_DIR = Path(\"../data/models/baseline\")\n",
    "MODELS_HYPER_DIR = Path(\"../data/models/hypertuned\")\n",
    "MODELS_HYPER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reports\n",
    "REPORT_PATH = Path(\"../data/reports/04_hypertuning\")\n",
    "BEST_PARAMS_PATH =Path( \"../data/reports/04_hypertuning\")\n",
    "\n",
    "REPORT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PARAMS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Tuning\n",
    "TUNE_ITER_REG = 25  # Regression\n",
    "TUNE_ITER_CLS = 35  # Classification\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# Ausgabe-Container\n",
    "report: Dict[str, Any] = {}\n",
    "best_params: Dict[str, Any] = {}\n"
   ],
   "id": "256001d23af8e629",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "eb274d63d8a17e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:06.376328Z",
     "start_time": "2025-12-29T07:49:06.346454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _identity(X):\n",
    "    return X\n",
    "\n",
    "def build_preprocessor_tree(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für XGBoost/Tree-Modelle (ohne Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def build_preprocessor_linear(X: pd.DataFrame):\n",
    "    \"\"\"Preprocessing für lineare Modelle (mit Scaling).\"\"\"\n",
    "    numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[(\"num\", num_pipe, numeric_cols), (\"cat\", cat_pipe, categorical_cols)],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "    return pre, numeric_cols, categorical_cols\n",
    "\n",
    "def regression_report(y_true, y_pred) -> Dict[str, float]:\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def classification_report_binary(y_true, proba, threshold=0.5) -> Dict[str, Any]:\n",
    "    pred = (proba >= threshold).astype(int)\n",
    "    roc = float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    pr = float(average_precision_score(y_true, proba)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    f1 = float(f1_score(y_true, pred))\n",
    "    cm = confusion_matrix(y_true, pred).tolist()\n",
    "    return {\"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"confusion_matrix\": cm}\n",
    "\n",
    "def best_f1_threshold(y_true, proba, thresholds=np.linspace(0.05, 0.95, 19)):\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in thresholds:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "def sklearn_sanitize_df(X):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        return X\n",
    "\n",
    "    X = X.copy()\n",
    "\n",
    "    # Convert NaT -> np.nan\n",
    "    X = X.replace({pd.NaT: np.nan})\n",
    "\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "\n",
    "        # pandas string or categorical -> object + np.nan\n",
    "        if pd.api.types.is_string_dtype(dt) or isinstance(dt, pd.CategoricalDtype):\n",
    "            X[c] = X[c].astype(\"object\")\n",
    "            X[c] = X[c].where(pd.notna(X[c]), np.nan)\n",
    "\n",
    "        # pandas nullable boolean -> float (0/1/nan)\n",
    "        elif str(dt) == \"boolean\":\n",
    "            X[c] = X[c].astype(\"float64\")\n",
    "\n",
    "        # pandas nullable integer (Int64, Int32...) -> float (so missing -> np.nan)\n",
    "        elif str(dt).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float64\")\n",
    "\n",
    "        # object columns might still contain pd.NA -> replace with np.nan\n",
    "        elif X[c].dtype == \"object\":\n",
    "            X[c] = X[c].where(pd.notna(X[c]), np.nan)\n",
    "\n",
    "    return X\n",
    "\n",
    "sanitize_tf = FunctionTransformer(sklearn_sanitize_df, feature_names_out=\"one-to-one\")"
   ],
   "id": "1aacd242493bd3d4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten Laden",
   "id": "1adca59ee8b67d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:12.227534Z",
     "start_time": "2025-12-29T07:49:11.124752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = Path(\"../data/datasets\")\n",
    "\n",
    "def _load_parquet(p: Path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {p}\")\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "files = {\n",
    "    \"X_track_pop\": DATA_DIR / \"X_track_pop.parquet\",\n",
    "    \"y_track_pop\": DATA_DIR / \"y_track_pop.parquet\",\n",
    "    \"X_album_pop\": DATA_DIR / \"X_album_pop.parquet\",\n",
    "    \"y_album_pop\": DATA_DIR / \"y_album_pop.parquet\",\n",
    "    \"X_track_hit\": DATA_DIR / \"X_track_hit.parquet\",\n",
    "    \"y_hit\": DATA_DIR / \"y_hit.parquet\",\n",
    "    \"X_track_explicit\": DATA_DIR / \"X_track_explicit.parquet\",\n",
    "    \"y_explicit\": DATA_DIR / \"y_explicit.parquet\",\n",
    "    \"X_track_mood\": DATA_DIR / \"X_track_mood.parquet\",\n",
    "    \"Y_mood\": DATA_DIR / \"Y_mood.parquet\",\n",
    "}\n",
    "\n",
    "X_track_pop = _load_parquet(files[\"X_track_pop\"])\n",
    "y_track_pop = _load_parquet(files[\"y_track_pop\"]).squeeze()\n",
    "\n",
    "X_album_pop = _load_parquet(files[\"X_album_pop\"])\n",
    "y_album_pop = _load_parquet(files[\"y_album_pop\"]).squeeze()\n",
    "\n",
    "X_track_hit = _load_parquet(files[\"X_track_hit\"])\n",
    "y_hit = _load_parquet(files[\"y_hit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_explicit = _load_parquet(files[\"X_track_explicit\"])\n",
    "y_explicit = _load_parquet(files[\"y_explicit\"]).squeeze().astype(int)\n",
    "\n",
    "X_track_mood = _load_parquet(files[\"X_track_mood\"])\n",
    "y_mood = _load_parquet(files[\"Y_mood\"])"
   ],
   "id": "fe46e569ed6f7e83",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:16.562522Z",
     "start_time": "2025-12-29T07:49:16.554551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X_track_pop:\", X_track_mood.shape)\n",
    "print(\"y_track_pop:\", y_mood.shape)\n",
    "print(\"X index unique:\", X_track_pop.index.is_unique)\n",
    "print(\"y index unique:\", y_track_pop.index.is_unique)\n",
    "print(\"Index equal:\", X_track_pop.index.equals(y_track_pop.index))\n"
   ],
   "id": "7c9e4a565e2e9e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_track_pop: (294616, 63)\n",
      "y_track_pop: (294616, 7)\n",
      "X index unique: True\n",
      "y index unique: True\n",
      "Index equal: True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline-Modelle laden",
   "id": "92edf052f9efe44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:21.900791Z",
     "start_time": "2025-12-29T07:49:20.740464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_paths = {\n",
    "    \"track_popularity\": MODELS_BASELINE_DIR / \"03_track_popularity_pipeline_xgb.joblib\",\n",
    "    \"album_popularity\": MODELS_BASELINE_DIR / \"03_album_popularity_pipeline_xgb.joblib\",\n",
    "    \"hit\": MODELS_BASELINE_DIR / \"03_hit_pipeline_xgb.joblib\",\n",
    "    \"explicit\": MODELS_BASELINE_DIR / \"03_explicit_pipeline_xgb.joblib\",\n",
    "    \"mood\": MODELS_BASELINE_DIR / \"03_mood_pipeline.joblib\",\n",
    "}\n",
    "\n",
    "baseline_models = {}\n",
    "for k, p in baseline_paths.items():\n",
    "    if p.exists():\n",
    "        baseline_models[k] = load(p)\n",
    "    else:\n",
    "        print(f\"Baseline nicht gefunden (übersprungen): {p}\")\n",
    "\n",
    "list(baseline_models.keys())\n"
   ],
   "id": "2666e1a630658fe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_popularity', 'album_popularity', 'hit', 'explicit', 'mood']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gemeinsame Splits (Baseline & Hypertuned müssen gleich evaluieren)",
   "id": "d1b7190b41b089a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:26.744769Z",
     "start_time": "2025-12-29T07:49:25.938347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = {}\n",
    "\n",
    "# Regression Splits (kein stratify)\n",
    "splits[\"track_pop\"] = train_test_split(\n",
    "    X_track_pop, y_track_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "splits[\"album_pop\"] = train_test_split(\n",
    "    X_album_pop, y_album_pop, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Classification Splits (stratifiziert)\n",
    "splits[\"hit\"] = train_test_split(\n",
    "    X_track_hit, y_hit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_hit\n",
    ")\n",
    "splits[\"explicit\"] = train_test_split(\n",
    "    X_track_explicit, y_explicit, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_explicit\n",
    ")\n",
    "\n",
    "# Mood optional\n",
    "splits[\"mood\"] = train_test_split(\n",
    "        X_track_mood, y_mood, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    ")\n"
   ],
   "id": "71799f0884756e07",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tuning-Funktion",
   "id": "133f49c1885a134c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T07:49:30.916531Z",
     "start_time": "2025-12-29T07:49:30.908187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tune_xgb_regression(Xtr, ytr, preprocessor):\n",
    "    base = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1  # wichtig: CV parallel, XGB single-thread\n",
    "    )\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"sanitize\", sanitize_tf),\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"model\", base)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": randint(800, 5000),\n",
    "        \"model__learning_rate\": loguniform(0.01, 0.15),\n",
    "        \"model__max_depth\": randint(3, 11),\n",
    "        \"model__min_child_weight\": loguniform(0.5, 20.0),\n",
    "        \"model__subsample\": uniform(0.6, 0.4),\n",
    "        \"model__colsample_bytree\": uniform(0.6, 0.4),\n",
    "        \"model__gamma\": loguniform(1e-8, 5.0),\n",
    "        \"model__reg_lambda\": loguniform(1e-3, 50.0),\n",
    "        \"model__reg_alpha\": loguniform(1e-8, 5.0),\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=TUNE_ITER_REG,                 # z.B. 20–40 für Qualität\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=CV_FOLDS,                          # 3 ist stabiler als 2\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,                            # CV parallel\n",
    "        error_score=\"raise\"\n",
    "    )\n",
    "    search.fit(Xtr, ytr)\n",
    "    return search\n",
    "\n",
    "\n",
    "def tune_xgb_classification(Xtr, ytr, preprocessor, scale_pos_weight: float):\n",
    "    base = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"aucpr\",\n",
    "        tree_method=\"hist\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    pipe = Pipeline(steps=[(\"sanitize\", sanitize_tf), (\"pre\", preprocessor), (\"model\", base)])\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": randint(800, 6000),\n",
    "        \"model__learning_rate\": loguniform(0.01, 0.15),\n",
    "        \"model__max_depth\": randint(3, 10),\n",
    "        \"model__min_child_weight\": loguniform(0.5, 20.0),\n",
    "        \"model__subsample\": uniform(0.6, 0.4),\n",
    "        \"model__colsample_bytree\": uniform(0.6, 0.4),\n",
    "\n",
    "        \"model__gamma\": loguniform(1e-8, 5.0),\n",
    "        \"model__reg_lambda\": loguniform(1e-3, 50.0),\n",
    "        \"model__reg_alpha\": loguniform(1e-8, 5.0),\n",
    "\n",
    "        # optional (oft hilft bei Imbalance)\n",
    "        \"model__max_delta_step\": randint(0, 10),\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=TUNE_ITER_CLS,\n",
    "        scoring=\"average_precision\",\n",
    "        cv=CV_FOLDS,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        error_score=\"raise\"\n",
    "    )\n",
    "    search.fit(Xtr, ytr)\n",
    "    return search\n",
    "\n"
   ],
   "id": "746484a1fd632633",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Track Popularity Tuning",
   "id": "bcfdbd56221c220a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T14:59:15.027757Z",
     "start_time": "2025-12-24T14:07:57.262834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"track_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_pop)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"track_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"track_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_track_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"track_popularity\"]"
   ],
   "id": "fdf3cb7380a9052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'MAE': 11.320437130854698,\n",
       "  'RMSE': 14.92832240076673,\n",
       "  'R2': 0.5362143878263578},\n",
       " 'cv_best_mae': 11.48122283841365}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Album Popularity Tuning",
   "id": "5f7d9f3811ae8f85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T15:22:57.051183Z",
     "start_time": "2025-12-24T14:59:15.256572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"album_pop\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_album_pop)\n",
    "\n",
    "search = tune_xgb_regression(Xtr, ytr, pre)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(Xte)\n",
    "metrics = regression_report(yte, pred)\n",
    "\n",
    "report[\"album_popularity\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_mae\": float(-search.best_score_)\n",
    "}\n",
    "best_params[\"album_popularity\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_album_popularity_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"album_popularity\"]\n"
   ],
   "id": "95e07df45a9f421f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'MAE': 12.239141901091108,\n",
       "  'RMSE': 15.590195381269522,\n",
       "  'R2': 0.4829835964278618},\n",
       " 'cv_best_mae': 12.329024047507666}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hit Tuning + Threshhold\n",
   "id": "1e036896b3a25ca4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T09:47:57.278814Z",
     "start_time": "2025-12-29T07:49:39.193278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"hit\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_hit)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"hit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"hit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_hit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"hit_prediction\"]\n"
   ],
   "id": "67fda4b58ececf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hypertuned': {'roc_auc': 0.8449704111799579,\n",
       "  'pr_auc': 0.5518084630783496,\n",
       "  'f1': 0.532349653476414,\n",
       "  'confusion_matrix': [[42512, 6840], [3619, 5953]],\n",
       "  'best_threshold': 0.6,\n",
       "  'best_threshold_f1': 0.532349653476414},\n",
       " 'cv_best_pr_auc': 0.5346853321808986,\n",
       " 'scale_pos_weight': 5.1556582830577975}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Explicit Tuning + Threshold",
   "id": "e68005067d391d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Xtr, Xte, ytr, yte = splits[\"explicit\"]\n",
    "pre, _, _ = build_preprocessor_tree(X_track_explicit)\n",
    "\n",
    "neg = int((ytr == 0).sum())\n",
    "pos = int((ytr == 1).sum())\n",
    "spw = neg / max(pos, 1)\n",
    "\n",
    "search = tune_xgb_classification(Xtr, ytr, pre, scale_pos_weight=spw)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "proba = best_model.predict_proba(Xte)[:, 1]\n",
    "thr, thr_f1 = best_f1_threshold(yte, proba)\n",
    "\n",
    "metrics = classification_report_binary(yte, proba, threshold=thr)\n",
    "metrics[\"best_threshold\"] = thr\n",
    "metrics[\"best_threshold_f1\"] = thr_f1\n",
    "\n",
    "report[\"explicit_prediction\"] = {\n",
    "    \"hypertuned\": metrics,\n",
    "    \"cv_best_pr_auc\": float(search.best_score_),\n",
    "    \"scale_pos_weight\": float(spw)\n",
    "}\n",
    "best_params[\"explicit_prediction\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_explicit_xgb_hypertuned.joblib\")\n",
    "\n",
    "report[\"explicit_prediction\"]\n"
   ],
   "id": "5b3c93a70e7af2e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mood Tuning",
   "id": "6e229830d3bfb21c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "Xtr, Xte, Ytr, Yte = splits[\"mood\"]\n",
    "pre, _, _ = build_preprocessor_linear(X_track_mood)\n",
    "\n",
    "base = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        max_iter=2000,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"sanitize\", sanitize_tf), (\"pre\", pre), (\"model\", base)])\n",
    "\n",
    "param_dist = {\n",
    "    \"model__estimator__C\": loguniform(1e-2, 30.0),\n",
    "    \"model__estimator__penalty\": [\"l2\", \"elasticnet\"],\n",
    "    \"model__estimator__l1_ratio\": uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    scoring=\"f1_micro\",\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "search.fit(Xtr, Ytr)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "Ypred = best_model.predict(Xte)\n",
    "\n",
    "micro_f1 = float(f1_score(Yte, Ypred, average=\"micro\"))\n",
    "macro_f1 = float(f1_score(Yte, Ypred, average=\"macro\"))\n",
    "\n",
    "report[\"mood_multilabel\"] = {\n",
    "    \"hypertuned\": {\"micro_f1\": micro_f1, \"macro_f1\": macro_f1},\n",
    "    \"cv_best_f1_micro\": float(search.best_score_)\n",
    "}\n",
    "best_params[\"mood_multilabel\"] = search.best_params_\n",
    "\n",
    "dump(best_model, MODELS_HYPER_DIR / \"04_mood_multilabel_logreg_hypertuned.joblib\")\n"
   ],
   "id": "1d831a9781adee52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reports Saving",
   "id": "64c200f35d048540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write hypertuning report\n",
    "with open(REPORT_PATH / \"04_hypertuning_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Write best parameters\n",
    "with open(BEST_PARAMS_PATH / \"04_hypertuning_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(\"Hypertuning fertig. Gespeichert unter:\")\n",
    "print(\" -\", MODELS_HYPER_DIR)\n",
    "print(\" -\", REPORT_PATH)\n",
    "print(\" -\", BEST_PARAMS_PATH)\n"
   ],
   "id": "1f02a85343f7a96d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
