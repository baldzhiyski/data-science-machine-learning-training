{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Recommender Systems Notebook",
   "id": "e1e559dff3285819"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup & Demo Data\n",
    "\n",
    "We implement some  common recommender algorithms used in production :\n",
    "\n",
    "- Popularity baseline\n",
    "- Content-based TF-IDF\n",
    "- Item-Item Co-visitation\n",
    "- Collaborative Filtering (kNN) user-based and item-based\n",
    "- Collaborative Filtering Matrix Factorization with Tensorflow\n",
    "- Two-Tower Retrieval with Tensorflow"
   ],
   "id": "7f71e62b91fa2b22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "9e1a725aadb7df97"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:33.582009Z",
     "start_time": "2025-12-30T14:04:33.563197Z"
    }
   },
   "source": [
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# For clean \"struct-like\" models (optional)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Typing clarity (optional but good practice)\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "\n",
    "# Useful for co-visitation counting\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Content-based TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from twisted.python.reflect import prefixedMethods\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Demo dataset generator",
   "id": "617b80a2368f0d08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:33.735554Z",
     "start_time": "2025-12-30T14:04:33.683205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_demo_data(\n",
    "        n_users: int = 30,\n",
    "        n_items: int = 60,\n",
    "        n_categories: int = 6,\n",
    "        ratings_per_user: int = 12,\n",
    "        session_len: int = 8,\n",
    "        seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a toy dataset that behaves like a real recommendation dataset.\n",
    "\n",
    "    Users:\n",
    "      - each user prefers one category\n",
    "\n",
    "    Items:\n",
    "      - each item belongs to one category\n",
    "      - each item has a text description (category-specific keywords)\n",
    "\n",
    "    Ratings:\n",
    "      - user gives higher ratings to items in their preferred category\n",
    "\n",
    "    Sessions:\n",
    "      - implicit sequences of interacted items (mostly from preferred category)\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # Assign each iteam a category ID\n",
    "    item_category = rng.randint(0, n_categories, size=n_items)\n",
    "\n",
    "    # Assign each user a preferred category\n",
    "    user_pref = rng.randint(0, n_categories, size=n_users)\n",
    "\n",
    "    # Words to generate item descriptions per category\n",
    "    category_words = {\n",
    "        0: [  # Action / Adventure\n",
    "            \"action\", \"fast\", \"adventure\", \"hero\", \"battle\", \"explosion\",\n",
    "            \"chase\", \"mission\", \"fight\", \"weapon\", \"danger\", \"rescue\"\n",
    "        ],\n",
    "        1: [  # Romance / Drama\n",
    "            \"romance\", \"love\", \"drama\", \"heart\", \"relationship\", \"emotion\",\n",
    "            \"passion\", \"kiss\", \"betrayal\", \"wedding\", \"tearful\", \"affection\"\n",
    "        ],\n",
    "        2: [  # Sci-Fi\n",
    "            \"scifi\", \"space\", \"future\", \"alien\", \"robot\", \"technology\",\n",
    "            \"galaxy\", \"time\", \"experiment\", \"spaceship\", \"cyber\", \"planet\"\n",
    "        ],\n",
    "        3: [  # Comedy\n",
    "            \"comedy\", \"funny\", \"joke\", \"laugh\", \"humor\", \"awkward\",\n",
    "            \"satire\", \"parody\", \"prank\", \"clumsy\", \"ridiculous\", \"smile\"\n",
    "        ],\n",
    "        4: [  # Horror\n",
    "            \"horror\", \"scary\", \"ghost\", \"dark\", \"monster\", \"fear\",\n",
    "            \"nightmare\", \"blood\", \"curse\", \"haunted\", \"evil\", \"scream\"\n",
    "        ],\n",
    "        5: [  # Documentary\n",
    "            \"documentary\", \"history\", \"facts\", \"nature\", \"real\",\n",
    "            \"science\", \"culture\", \"wildlife\", \"investigation\", \"education\",\n",
    "            \"truth\", \"archive\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Create item text and titles\n",
    "    item_text: Dict[int, str] = {}\n",
    "    item_title: Dict[int, str] = {}\n",
    "    for i in range(n_items):\n",
    "        category = int(item_category[i])\n",
    "        words = category_words[category]\n",
    "\n",
    "        desc = \" \".join(rng.choice(words, size=5, replace=False))\n",
    "\n",
    "        item_text[i] = desc\n",
    "        item_title[i] = f\"Item_{i:02d}_Category_{category:02d}\"\n",
    "\n",
    "    # Build ratings as a list of (user_id,item_id,rating)\n",
    "    ratings: List[Tuple[int, int, float]] = []\n",
    "    for u in range(n_users):\n",
    "        # Items in user's preferred category\n",
    "        preferred_items = np.where(item_category == user_pref[u])[0]\n",
    "\n",
    "        # Items Not in preferred category\n",
    "        other_items = np.where(item_category != user_pref[u])[0]\n",
    "\n",
    "        # Choose ~70% from preferred and ~30% from others\n",
    "        n_pref = int(ratings_per_user * 0.7)\n",
    "        n_other = ratings_per_user - n_pref\n",
    "\n",
    "        # Choose without replacement\n",
    "        chosen_pref = rng.choice(preferred_items, size=min(n_pref, len(preferred_items)), replace=False)\n",
    "        chosen_other = rng.choice(other_items, size=min(n_other, len(other_items)), replace=False)\n",
    "\n",
    "        chosen = np.concatenate((chosen_pref, chosen_other))\n",
    "        rng.shuffle(chosen)\n",
    "\n",
    "        for item_id in chosen:\n",
    "            # Base rating is higher if matches preference\n",
    "            base = 4.2 if item_category[item_id] == user_pref[u] else 2.8\n",
    "\n",
    "            # Add Gaussian noise and clip into [1...5]\n",
    "            r = np.clip(rng.normal(base, 0.6), 1.0, 5.0)\n",
    "\n",
    "            # Round to 0.1 to look more realistic\n",
    "            ratings.append((u, int(item_id), float(np.round(r, 1))))\n",
    "\n",
    "    # Build the sessions ( view/click sequences)\n",
    "\n",
    "    sessions: List[List[int]] = []\n",
    "\n",
    "    for u in range(n_users):\n",
    "        # sample from preferred category with replacement ( views  can repeat )\n",
    "        pref_items = np.where(item_category == user_pref[u])[0]\n",
    "        session = rng.choice(pref_items, size=session_len, replace=True)\n",
    "        sessions.append(session)\n",
    "\n",
    "    # Item metadata table\n",
    "    items_df = pd.DataFrame({\n",
    "        \"item_id\": np.arange(n_items),\n",
    "        \"title\": [item_title[i] for i in range(n_items)],\n",
    "        \"category\": item_category,\n",
    "        \"description\": [item_text[i] for i in range(n_items)],\n",
    "    })\n",
    "\n",
    "    # Ratings dataframe\n",
    "    ratings_df = pd.DataFrame(\n",
    "        ratings,\n",
    "        columns=[\"user_id\", \"item_id\", \"rating\"]\n",
    "    )\n",
    "\n",
    "    return ratings_df, sessions, items_df, item_text"
   ],
   "id": "534083186fcdf4f9",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:33.829016Z",
     "start_time": "2025-12-30T14:04:33.766081Z"
    }
   },
   "cell_type": "code",
   "source": "ratings_df, sessions, items_df, item_text = make_demo_data()",
   "id": "ae35c66dd7247a14",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:33.913458Z",
     "start_time": "2025-12-30T14:04:33.874742Z"
    }
   },
   "cell_type": "code",
   "source": "ratings_df.head(10)",
   "id": "a5b6e96d361143ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0       39     4.4\n",
       "1        0       32     5.0\n",
       "2        0       47     2.6\n",
       "3        0       24     2.4\n",
       "4        0       40     3.2\n",
       "5        0       59     4.9\n",
       "6        0        7     2.9\n",
       "7        0        0     4.2\n",
       "8        0       38     5.0\n",
       "9        0       50     3.6"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:34.160537Z",
     "start_time": "2025-12-30T14:04:34.133613Z"
    }
   },
   "cell_type": "code",
   "source": "items_df.head(10)",
   "id": "4063c28d6ccaa237",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   item_id                title  category  \\\n",
       "0        0  Item_00_Category_03         3   \n",
       "1        1  Item_01_Category_04         4   \n",
       "2        2  Item_02_Category_02         2   \n",
       "3        3  Item_03_Category_04         4   \n",
       "4        4  Item_04_Category_04         4   \n",
       "5        5  Item_05_Category_01         1   \n",
       "6        6  Item_06_Category_02         2   \n",
       "7        7  Item_07_Category_02         2   \n",
       "8        8  Item_08_Category_02         2   \n",
       "9        9  Item_09_Category_04         4   \n",
       "\n",
       "                                   description  \n",
       "0         smile joke awkward comedy ridiculous  \n",
       "1             fear scary haunted monster ghost  \n",
       "2  technology experiment cyber alien spaceship  \n",
       "3               scream haunted evil dark ghost  \n",
       "4                  blood dark scary curse evil  \n",
       "5      wedding relationship love emotion heart  \n",
       "6       experiment scifi robot space spaceship  \n",
       "7              cyber planet scifi galaxy space  \n",
       "8          technology time space galaxy future  \n",
       "9          haunted fear scream nightmare blood  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Item_00_Category_03</td>\n",
       "      <td>3</td>\n",
       "      <td>smile joke awkward comedy ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Item_01_Category_04</td>\n",
       "      <td>4</td>\n",
       "      <td>fear scary haunted monster ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Item_02_Category_02</td>\n",
       "      <td>2</td>\n",
       "      <td>technology experiment cyber alien spaceship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Item_03_Category_04</td>\n",
       "      <td>4</td>\n",
       "      <td>scream haunted evil dark ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Item_04_Category_04</td>\n",
       "      <td>4</td>\n",
       "      <td>blood dark scary curse evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Item_05_Category_01</td>\n",
       "      <td>1</td>\n",
       "      <td>wedding relationship love emotion heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Item_06_Category_02</td>\n",
       "      <td>2</td>\n",
       "      <td>experiment scifi robot space spaceship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Item_07_Category_02</td>\n",
       "      <td>2</td>\n",
       "      <td>cyber planet scifi galaxy space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Item_08_Category_02</td>\n",
       "      <td>2</td>\n",
       "      <td>technology time space galaxy future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Item_09_Category_04</td>\n",
       "      <td>4</td>\n",
       "      <td>haunted fear scream nightmare blood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "3a25002346b69bb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train/Test Split & Metrics\n",
    "\n",
    "- For each user keep 1 rating as a test , the rest remains for training"
   ],
   "id": "4977d314bb932c40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:34.369417Z",
     "start_time": "2025-12-30T14:04:34.336127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def leave_last_one_out_split(ratings: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each user, hold out 1 interaction for test.\n",
    "    \"\"\"\n",
    "    train_parts = []  # list of train chunks for each user\n",
    "    test_rows = []    # list of single held-out rows for each user\n",
    "\n",
    "    # Group ratings by user\n",
    "    for user_id, group in ratings.groupby(\"user_id\"):\n",
    "        # Shuffle this user's ratings so \"last one out\" isn't biased by item_id ordering\n",
    "        group = group.sample(frac=1.0, random_state=seed)\n",
    "\n",
    "        # Last row becomes test\n",
    "        test_rows.append(group.iloc[-1])\n",
    "\n",
    "        # All except last become train\n",
    "        train_parts.append(group.iloc[:-1])\n",
    "\n",
    "    train_df = pd.concat(train_parts).reset_index(drop=True)\n",
    "    test_df = pd.DataFrame(test_rows).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def precision_recall_at_k(recs: List[int], relevant: set, k: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision@k and recall@k.\n",
    "\n",
    "    - precision@k: fraction of recommended items (top k) that are relevant\n",
    "    - recall@k: fraction of relevant items that appear in top k\n",
    "    \"\"\"\n",
    "    top_k = recs[:k]                            # keep only top-k recommendations\n",
    "    hits = sum(1 for x in top_k if x in relevant)  # count matches with relevant set\n",
    "\n",
    "    precision = hits / k\n",
    "    recall = hits / max(1, len(relevant))\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    recommend_fn: Callable[[int, set, int], List[int]],\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    k: int = 10,\n",
    "    name: str = \"model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a recommender function using leave-last-one-out test.\n",
    "\n",
    "    recommend_fn signature: (user_id, seen_set, k) -> list[item_id]\n",
    "    \"\"\"\n",
    "    # Build helper dicts for speed:\n",
    "    # - seen items per user (from train)\n",
    "    seen_by_user = train_df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    # - test relevant item per user (from test)\n",
    "    relevant_by_user = test_df.groupby(\"user_id\")[\"item_id\"].apply(list).to_dict()\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for u, rel_items in relevant_by_user.items():\n",
    "        relevant_set = set(rel_items)             # relevant items (here only 1)\n",
    "        seen_set = seen_by_user.get(u, set())     # seen in training\n",
    "\n",
    "        recs = recommend_fn(u, seen_set, k)       # top-k recommendations\n",
    "        p, r = precision_recall_at_k(recs, relevant_set, k)\n",
    "\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "\n",
    "    print(f\"\\n{name} @ {k}\")\n",
    "    print(f\"Precision@{k}: {np.mean(precisions):.3f}\")\n",
    "    print(f\"Recall@{k}:    {np.mean(recalls):.3f}\")\n",
    "\n",
    "\n",
    "def show_recommendations(user_id: int, recs: List[int], title: str):\n",
    "    \"\"\"Prints recommended item titles and categories.\"\"\"\n",
    "    print(f\"\\n{title} (user={user_id})\")\n",
    "    display(items_df.set_index(\"item_id\").loc[recs][[\"title\", \"category\"]])\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b8dcf921c4c2ecdc",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:34.487507Z",
     "start_time": "2025-12-30T14:04:34.409327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data\n",
    "train_df, test_df = leave_last_one_out_split(ratings_df)\n",
    "\n",
    "# Useful global counts\n",
    "n_users = int(ratings_df[\"user_id\"].max() + 1)\n",
    "n_items = int(ratings_df[\"item_id\"].max() + 1)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))"
   ],
   "id": "cc4d9b03fa982fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 328 Test size: 30\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### User Id",
   "id": "7e8ba6181de4a9e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:34.663208Z",
     "start_time": "2025-12-30T14:04:34.651924Z"
    }
   },
   "cell_type": "code",
   "source": "u_demo = 2",
   "id": "8e1d8e725380f2fb",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Popularity Baseline Algorithm\n",
   "id": "ae10200bd50d6289"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:34.785134Z",
     "start_time": "2025-12-30T14:04:34.731203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PopularityRecommender:\n",
    "    \"\"\"\n",
    "    Recommends items based on global popularity (interaction count).\n",
    "    Very common baseline and fallback in production.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ranked_items : List[int] = []\n",
    "\n",
    "    def fit(self,train_df:pd.DataFrame):\n",
    "        # How much an item appears in train\n",
    "        counts = train_df['item_id'].value_counts()\n",
    "\n",
    "        self.ranked_items = [int(item_id) for item_id in counts.index]\n",
    "    def recommend(self,user_id:int , seen:set,k:int) -> List[int]:\n",
    "        recs = []\n",
    "\n",
    "        for item_id in self.ranked_items:\n",
    "            if item_id not in seen:\n",
    "                recs.append(item_id)\n",
    "            if len(recs) == k:\n",
    "                break\n",
    "        return recs\n",
    "\n",
    "pop = PopularityRecommender()\n",
    "pop.fit(train_df)\n",
    "\n",
    "seen_demo = set(train_df[train_df.user_id == u_demo].item_id)\n",
    "recs_demo = pop.recommend(u_demo, seen_demo, k=10)\n",
    "show_recommendations(u_demo, recs_demo, \"Popularity recommendations\")\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(lambda u, seen, k: pop.recommend(u, seen, k), train_df, test_df, k=10, name=\"Popularity\")"
   ],
   "id": "e1d22bc88290055f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Popularity recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "50       Item_50_Category_05         5\n",
       "17       Item_17_Category_05         5\n",
       "18       Item_18_Category_01         1\n",
       "16       Item_16_Category_05         5\n",
       "33       Item_33_Category_03         3\n",
       "56       Item_56_Category_01         1\n",
       "12       Item_12_Category_05         5\n",
       "5        Item_05_Category_01         1\n",
       "36       Item_36_Category_05         5\n",
       "47       Item_47_Category_03         3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Item_50_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Item_17_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Item_18_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Item_16_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Item_33_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Item_56_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Item_12_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Item_05_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Item_36_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Item_47_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Popularity @ 10\n",
      "Precision@10: 0.030\n",
      "Recall@10:    0.300\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Those numbers mean: with the **Most Popular** baseline, **only ~3% of the top-10 recommendations are correct** on average (≈0.3 “hits” per user), but it still manages to include each user’s **one held-out test item** in the top-10 for about **30% of users** (that’s what Recall@10 = 0.300 means in your leave-one-out setup).\n",
   "id": "a94f72e13f4cc504"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Content-Based TF-IDF Algorithm",
   "id": "a41bf6b5af5780c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:35.053920Z",
     "start_time": "2025-12-30T14:04:34.896093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Set\n",
    "\n",
    "\n",
    "class ContentTFIDFRecommender:\n",
    "    \"\"\"\n",
    "    Content-based TF-IDF Recommender (Text Similarity Recommender)\n",
    "\n",
    "    Idea:\n",
    "    - Each item has a text description (bag of words).\n",
    "    - We convert every item's text into a TF-IDF vector.\n",
    "      (A vector where each dimension corresponds to a word; the value says how important\n",
    "       that word is for that item compared to all other items.)\n",
    "    - For a user, we take the TF-IDF vectors of the items they already interacted with\n",
    "      (their \"history\") and average them → this becomes the user's \"profile vector\".\n",
    "      (This profile captures what words/topics the user seems to like.)\n",
    "    - We then compute cosine similarity between the user profile and ALL items.\n",
    "      Items with the highest similarity are the recommendations.\n",
    "    - We exclude items the user already saw/rated (seen set), so we don’t recommend them again.\n",
    "\n",
    "    Output:\n",
    "    - recommend(...) returns a list of item_ids (length k), sorted from most relevant to least.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_features: int = 5000):\n",
    "        # Limit how many words/features TF-IDF keeps (top max_features most useful words).\n",
    "        self.max_features = max_features\n",
    "\n",
    "        # Keeps item IDs in the exact same order as rows in the TF-IDF matrix X.\n",
    "        # Example: item_ids[0] corresponds to X[0], item_ids[1] -> X[1], etc.\n",
    "        self.item_ids: List[int] = []\n",
    "\n",
    "        # Turns text -> TF-IDF vectors.\n",
    "        # stop_words='english' removes common words like \"the\", \"and\", \"is\" etc.\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "\n",
    "        # TF-IDF matrix: shape = (num_items, num_features)\n",
    "        # Row = one item, Columns = words/features\n",
    "        # Usually a sparse matrix because most words are 0 for a given item.\n",
    "        self.X = None\n",
    "\n",
    "    def fit(self, item_text: Dict[int, str]) -> None:\n",
    "        \"\"\"\n",
    "        Train/prepare the model on item descriptions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_text : dict[item_id -> description]\n",
    "            Example: {0: \"space alien future robot\", 1: \"romance love heart drama\", ...}\n",
    "\n",
    "        What happens:\n",
    "        1) We fix an ordering of item IDs (sorted).\n",
    "        2) We build a list of item descriptions in that exact order.\n",
    "        3) We fit TF-IDF on all descriptions and store the resulting matrix in self.X.\n",
    "\n",
    "        After this:\n",
    "        - self.item_ids maps row index -> item_id\n",
    "        - self.X[row_index] is the TF-IDF vector for that item_id\n",
    "        \"\"\"\n",
    "        # Ensure consistent stable ordering so row indices are reproducible.\n",
    "        self.item_ids = sorted(item_text.keys())\n",
    "\n",
    "        # Create a list of texts in the same order as item_ids.\n",
    "        texts = [item_text[i] for i in self.item_ids]\n",
    "\n",
    "        # Learn vocabulary + transform text to TF-IDF matrix.\n",
    "        # X: (n_items x n_features)\n",
    "        self.X = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        user_id: int,\n",
    "        seen: Set[int],\n",
    "        k: int,\n",
    "        user_history: List[int]\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Recommend top-k items for a user using TF-IDF + cosine similarity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_id : int\n",
    "            Not used by the algorithm itself here (history already represents the user),\n",
    "            but kept for a consistent recommender interface.\n",
    "        seen : set[int]\n",
    "            Items already interacted with in training. We exclude these from output.\n",
    "        k : int\n",
    "            Number of recommendations to return.\n",
    "        user_history : list[int]\n",
    "            Items the user interacted with in the past (training interactions).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[int]\n",
    "            Top-k recommended item_ids (most similar first).\n",
    "\n",
    "        Flow (high level):\n",
    "        1) Convert user_history item IDs -> row indices in self.X\n",
    "        2) Build user profile vector = average of TF-IDF vectors of history items\n",
    "        3) Compute cosine similarity(user_profile, every_item_vector)\n",
    "        4) Set similarity of \"seen\" items to -1 (so they will never be recommended)\n",
    "        5) Take the top-k highest similarity items\n",
    "        \"\"\"\n",
    "        # -------------------------\n",
    "        # (0) Cold start: no history\n",
    "        # -------------------------\n",
    "        if not user_history:\n",
    "            # Without history, we can't build a user profile from content.\n",
    "            return []\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # (1) Convert item IDs -> TF-IDF row indices (in self.X)\n",
    "        # ---------------------------------------------------------\n",
    "        # self.item_ids is the mapping: row_index -> item_id\n",
    "        # We need the inverse mapping: item_id -> row_index, but here we use list.index(...)\n",
    "        # which is correct but slower (can be optimized later).\n",
    "        idxs = [self.item_ids.index(i) for i in user_history]\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # (2) Build user profile vector (average TF-IDF of history)\n",
    "        # ---------------------------------------------------------\n",
    "        # self.X[idxs] selects the rows for the user's history items:\n",
    "        # shape: (len(history), n_features)\n",
    "        #\n",
    "        # mean(axis=0) averages over history items to create one \"taste vector\":\n",
    "        # shape: (1, n_features)\n",
    "        user_vec = self.X[idxs].mean(axis=0)\n",
    "\n",
    "        # Convert potential np.matrix to a normal numpy array (sklearn dislikes np.matrix).\n",
    "        # Keep it 2D: (1, n_features) so cosine_similarity is happy.\n",
    "        user_vec = np.asarray(user_vec).reshape(1, -1)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # (3) Similarity between user profile and ALL items\n",
    "        # ---------------------------------------------------------\n",
    "        # cosine_similarity compares user_vec to each item vector in self.X:\n",
    "        # result shape: (1, n_items)\n",
    "        sims = cosine_similarity(user_vec, self.X).ravel()  # -> (n_items,)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # (4) Exclude already seen items\n",
    "        # ---------------------------------------------------------\n",
    "        # We set their similarity to -1 so they drop to the bottom.\n",
    "        # (Cosine similarity for TF-IDF is normally between 0 and 1.)\n",
    "        for it in seen:\n",
    "            if it in self.item_ids:\n",
    "                sims[self.item_ids.index(it)] = -1.0\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # (5) Choose the top-k highest similarity items\n",
    "        # ---------------------------------------------------------\n",
    "        # argsort sorts ascending, so we sort by -sims (descending).\n",
    "        top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "        # Convert TF-IDF row indices back to item IDs.\n",
    "        return [self.item_ids[i] for i in top_idx]\n",
    "\n",
    "\n",
    "# Train content model\n",
    "cb = ContentTFIDFRecommender(max_features=5000)\n",
    "cb.fit(item_text)\n",
    "\n",
    "# Demo\n",
    "history_demo = list(train_df[train_df.user_id == u_demo].item_id)\n",
    "recs_demo = cb.recommend(u_demo, seen_demo, k=10, user_history=history_demo)\n",
    "show_recommendations(u_demo, recs_demo, \"Content TF-IDF recommendations\")\n",
    "\n",
    "# Evaluate with fallback to popularity if needed\n",
    "def cb_recommend_fn(u, seen, k):\n",
    "    history = list(train_df[train_df.user_id == u].item_id)\n",
    "    recs = cb.recommend(u, seen, k, history)\n",
    "    return recs if recs else pop.recommend(u, seen, k)\n",
    "\n",
    "evaluate_model(cb_recommend_fn, train_df, test_df, k=10, name=\"Content TF-IDF\")"
   ],
   "id": "63150e690f2bbd0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content TF-IDF recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "29       Item_29_Category_02         2\n",
       "30       Item_30_Category_02         2\n",
       "6        Item_06_Category_02         2\n",
       "17       Item_17_Category_05         5\n",
       "0        Item_00_Category_03         3\n",
       "50       Item_50_Category_05         5\n",
       "40       Item_40_Category_00         0\n",
       "24       Item_24_Category_05         5\n",
       "12       Item_12_Category_05         5\n",
       "58       Item_58_Category_03         3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Item_29_Category_02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Item_30_Category_02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Item_06_Category_02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Item_17_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item_00_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Item_50_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Item_40_Category_00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Item_24_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Item_12_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Item_58_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content TF-IDF @ 10\n",
      "Precision@10: 0.080\n",
      "Recall@10:    0.800\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These results mean that the **Content TF-IDF recommender** places the user’s held-out test item in the **top-10 recommendations for about 80% of users** (Recall@10 = 0.800), and on average **0.8 of the 10 recommended items are actually relevant** (Precision@10 = 0.080), which is a **substantial improvement over the popularity baseline** and shows that matching item descriptions to a user’s past content effectively captures user preferences in this dataset.\n",
   "id": "8f9337c87d759a3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Item-Item Co-visitation\n",
    "\n",
    "- From sessions we count how often item A appears together with item B and we recommend neighbors of the last seen item"
   ],
   "id": "2a15722df616bcf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:35.166354Z",
     "start_time": "2025-12-30T14:04:35.093240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ItemItemCoVisitation:\n",
    "    \"\"\"\n",
    "    Item-Item Co-Visitation Recommender (Session Co-occurrence Graph)\n",
    "\n",
    "    What this model does:\n",
    "    - It learns which items tend to appear together in the SAME session.\n",
    "      Example: if item 12 and item 33 are often viewed in the same session,\n",
    "      then 33 becomes a strong \"neighbor\" recommendation for 12.\n",
    "\n",
    "    This is commonly used in real systems as:\n",
    "      \"Customers who viewed this also viewed...\"\n",
    "      \"Frequently bought together...\"\n",
    "\n",
    "    Output:\n",
    "    - A graph/dictionary:\n",
    "        graph[item_a] = [most common co-occurring items with item_a]\n",
    "      sorted by how frequently they co-occur in sessions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,top_k_per_item:int = 50):\n",
    "         # For each item, keep only the top-K most frequent neighbors\n",
    "        self.top_k_per_item = top_k_per_item\n",
    "\n",
    "        # The learned co-visitation graph:\n",
    "        self.graph : Dict[int,List[int]] = {}\n",
    "\n",
    "    def fit(self,sessions:List[List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Build co-visitation counts from session data.\n",
    "\n",
    "        sessions:\n",
    "            A list of sessions; each session is a list of item_ids\n",
    "            Example session: [10, 10, 3, 7]  (views can repeat)\n",
    "\n",
    "        Training logic:\n",
    "        1) For each session, take UNIQUE items (avoid counting duplicates in the same session).\n",
    "        2) For every pair (a,b) that appear together, increment counts[a][b].\n",
    "        3) For each item a, store its top_k_per_item neighbors by count.\n",
    "        \"\"\"\n",
    "        # counts[a] is a Counter holding co-occurrence counts with other items b\n",
    "        # counts[a][b] = number of sessions where both a and b appeared\n",
    "        counts = defaultdict(Counter)\n",
    "\n",
    "        for session in sessions:\n",
    "\n",
    "            # Remove duplicates\n",
    "            unique_items = list(dict.fromkeys(session))\n",
    "\n",
    "            # Count co-occurences for all pairs in this session\n",
    "\n",
    "            for a in unique_items:\n",
    "                for b in unique_items:\n",
    "                    if a != b:\n",
    "                        counts[a][b] += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the counts into a neighbour list graph\n",
    "\n",
    "        self.graph = {}\n",
    "\n",
    "        for a, ctr in counts.items():\n",
    "             self.graph[int(a)] = [int(b) for b, _ in ctr.most_common(self.top_k_per_item)]\n",
    "\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        user_id: int,\n",
    "        seen: set,\n",
    "        k: int,\n",
    "        last_item: Optional[int]\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Recommend items based on the user's last interacted item.\n",
    "\n",
    "        Intuition:\n",
    "        - If the user's last_item is X, recommend \"neighbors of X\" (items often seen with X).\n",
    "        - This is a SHORT-TERM / session-context recommender (not long-term user taste).\n",
    "\n",
    "        Inputs:\n",
    "        - last_item: the last item the user interacted with (context)\n",
    "        - seen: items already seen by the user in training; exclude them\n",
    "        - k: how many items to return\n",
    "\n",
    "        Returns:\n",
    "        - Up to k recommended item_ids\n",
    "        \"\"\"\n",
    "        # Without context no prediction possible\n",
    "        if last_item is None:\n",
    "            return []\n",
    "\n",
    "        # Get candidate neighbors from the graph\n",
    "        candidates = self.graph.get(int(last_item), [])\n",
    "\n",
    "        # Filter out items user already saw and return top-k\n",
    "        return [i for i in candidates if i not in seen][:k]\n",
    "\n",
    "\n",
    "covis = ItemItemCoVisitation(top_k_per_item=50)\n",
    "covis.fit(sessions)\n",
    "\n",
    "# Use the last training interaction as context (what they most recently viewed)\n",
    "last_item_demo = history_demo[-1] if history_demo else None\n",
    "\n",
    "recs_demo = covis.recommend(\n",
    "    user_id=u_demo,\n",
    "    seen=seen_demo,\n",
    "    k=10,\n",
    "    last_item=last_item_demo\n",
    ")\n",
    "\n",
    "show_recommendations(\n",
    "    u_demo,\n",
    "    recs_demo,\n",
    "    f\"Co-visitation recommendations (last_item={last_item_demo})\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Evaluate: co-visitation with popularity fallback\n",
    "# ------------------------------------------\n",
    "def covis_recommend_fn(u, seen, k):\n",
    "    \"\"\"\n",
    "    Wrapper that matches evaluate_model(...) signature.\n",
    "\n",
    "    - Build last_item from this user's train history.\n",
    "    - If co-visitation can't recommend (no history or empty neighbor list),\n",
    "      fall back to the popularity baseline so evaluation always returns k items.\n",
    "    \"\"\"\n",
    "    history = list(train_df[train_df.user_id == u].item_id)\n",
    "    last_item = history[-1] if history else None\n",
    "\n",
    "    recs = covis.recommend(u, seen, k, last_item)\n",
    "    return recs if recs else pop.recommend(u, seen, k)\n",
    "\n",
    "\n",
    "evaluate_model(covis_recommend_fn, train_df, test_df, k=10, name=\"Item-Item Co-visitation\")"
   ],
   "id": "d7815337f028c0d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Co-visitation recommendations (last_item=38) (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "15       Item_15_Category_03         3\n",
       "57       Item_57_Category_03         3\n",
       "32       Item_32_Category_03         3\n",
       "22       Item_22_Category_03         3\n",
       "26       Item_26_Category_03         3\n",
       "39       Item_39_Category_03         3\n",
       "58       Item_58_Category_03         3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Item_15_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Item_57_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Item_32_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Item_22_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Item_26_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Item_39_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Item_58_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item-Item Co-visitation @ 10\n",
      "Precision@10: 0.043\n",
      "Recall@10:    0.433\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Item-Item Co-visitation achieves high recall (43.3%) but low precision (4.3%) at @10, which is expected and desirable because it is designed as a recall-focused candidate generator rather than a final ranking model.\n",
   "id": "333c12cf0c4542d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "- User-based kNN CF\n",
    "- Item-based kNN CF"
   ],
   "id": "70839b4fb871b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:35.280278Z",
     "start_time": "2025-12-30T14:04:35.267023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_dense_rating_matrix(train_df:pd.DataFrame,n_users:int,n_items: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a dense user-item rating matrix R.\n",
    "\n",
    "    Goal:\n",
    "      Convert a \"long\" ratings table like:\n",
    "         user_id | item_id | rating\n",
    "      into a matrix:\n",
    "         R[user, item] = rating\n",
    "\n",
    "    Output:\n",
    "      R shape: (n_users, n_items)\n",
    "      - R[u, i] = rating if user u rated item i in TRAIN\n",
    "      - R[u, i] = NaN if user u never rated item i (unknown/missing)\n",
    "\n",
    "    Why NaN?\n",
    "      We want to distinguish:\n",
    "        - \"unknown rating\" (missing)   -> NaN\n",
    "        - \"real rating value\" (1..5)   -> float\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill everything with Nan initially\n",
    "    R = np.full((n_users, n_items), np.nan,dtype=float)\n",
    "\n",
    "    for row in train_df.itertuples(index=False):\n",
    "        u = int(row.user_id)\n",
    "        i = int(row.item_id)\n",
    "        r = float(row.rating)\n",
    "\n",
    "        R[u,i] = r\n",
    "\n",
    "    return R\n",
    "\n",
    "def nanmean_safe(x:np.ndarray) -> float:\n",
    "     \"\"\"\n",
    "    Compute mean while ignoring NaNs.\n",
    "\n",
    "    Why needed?\n",
    "      Users will have NaNs for items they never rated.\n",
    "      np.mean would produce NaN if any NaNs exist,\n",
    "      so we use np.nanmean.\n",
    "\n",
    "    Edge case:\n",
    "      If the entire vector is NaN (user has no ratings),\n",
    "      np.nanmean returns NaN. In that case return 0.0.\n",
    "    \"\"\"\n",
    "\n",
    "     m = np.nanmean(x)\n",
    "     return float(m) if not np.isnan(m) else 0.0"
   ],
   "id": "a041642d9f052c4c",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:35.307835Z",
     "start_time": "2025-12-30T14:04:35.295221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pearson_sim_nan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Pearson correlation similarity for rating vectors with NaNs.\n",
    "\n",
    "    Use case:\n",
    "      USER-based CF typically uses Pearson correlation:\n",
    "      - It compares rating patterns AFTER mean-centering.\n",
    "      - This handles users with different rating scales\n",
    "        (e.g., strict user vs generous user).\n",
    "\n",
    "    How it works:\n",
    "      1) Find overlap positions where BOTH users rated the same items.\n",
    "      2) Mean-center both users over only that overlap.\n",
    "      3) Compute correlation (dot / norms).\n",
    "\n",
    "    Returns:\n",
    "      similarity in [-1, 1]\n",
    "      0.0 if not enough overlap or zero variance\n",
    "    \"\"\"\n",
    "    # Only compare where both have real ratings\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)\n",
    "\n",
    "    # If fewer than 2 overlapping ratings, correlation isn't meaningful\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    x = a[mask]\n",
    "    y = b[mask]\n",
    "\n",
    "    # Mean-center within the overlap (Pearson requirement)\n",
    "    x = x - x.mean()\n",
    "    y = y - y.mean()\n",
    "\n",
    "    denom = np.sqrt((x * x).sum()) * np.sqrt((y * y).sum())\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return float((x * y).sum() / denom)\n",
    "\n",
    "def cosine_sim_nan(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cosine similarity for vectors with NaNs, using only overlap positions.\n",
    "\n",
    "    Use case:\n",
    "      ITEM-based CF often uses cosine similarity (especially for implicit feedback\n",
    "      or when you don't want mean-centering).\n",
    "\n",
    "    Returns:\n",
    "      similarity in [0, 1] typically for non-negative data,\n",
    "      but can be negative if values can be negative.\n",
    "      0.0 if not enough overlap or zero norm.\n",
    "    \"\"\"\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    x = a[mask]\n",
    "    y = b[mask]\n",
    "\n",
    "    denom = np.sqrt((x * x).sum()) * np.sqrt((y * y).sum())\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return float((x * y).sum() / denom)"
   ],
   "id": "8b15ac882a594794",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:35.343956Z",
     "start_time": "2025-12-30T14:04:35.321769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UserKNNCF:\n",
    "    \"\"\"\n",
    "    User-based kNN Collaborative Filtering (CF)\n",
    "\n",
    "    Core idea:\n",
    "      Users who rated items similarly in the past will rate new items similarly.\n",
    "\n",
    "    Predict rating(u, i) using neighbors:\n",
    "      1) Find other users v who rated item i\n",
    "      2) Compute similarity s(u, v) (Pearson correlation)\n",
    "      3) Combine their ratings with a weighted average\n",
    "\n",
    "    We use a mean-centered formula:\n",
    "      pred(u,i) = mean(u) + sum_v s(u,v) * (r(v,i) - mean(v)) / sum_v |s(u,v)|\n",
    "\n",
    "    Why mean-centering?\n",
    "      - Some users rate high overall (4-5), others rate low (2-3).\n",
    "      - Pearson + mean-centering focuses on *preference patterns* not scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,k_neighbors:int = 30):\n",
    "\n",
    "        self.k = k_neighbors\n",
    "\n",
    "        # Rating matrix stored after fit()\n",
    "        self.R: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, R: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Store the training rating matrix R.\n",
    "        No \"learning\" parameters here — kNN is mostly lazy evaluation.\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "\n",
    "    def predict_rating(self, user: int, item: int) -> float:\n",
    "        \"\"\"\n",
    "        Predict a rating for (user, item), even if user never rated it.\n",
    "\n",
    "        Steps:\n",
    "          A) If user already rated this item in TRAIN -> return that rating.\n",
    "          B) Otherwise:\n",
    "             - compute user's mean rating\n",
    "             - find neighbors who rated this item\n",
    "             - compute Pearson similarity to each neighbor\n",
    "             - aggregate neighbor contributions (mean-centered)\n",
    "        \"\"\"\n",
    "        R = self.R\n",
    "        assert R is not None, \"Call fit() before predict_rating().\"\n",
    "\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "\n",
    "        # A) Known rating shortcut (already rated in train)\n",
    "        if not np.isnan(R[user, item]):\n",
    "            return float(R[user, item])\n",
    "\n",
    "        # B1) Baseline: user's average rating\n",
    "        user_mean = nanmean_safe(R[user])\n",
    "\n",
    "        # B2) Collect candidate neighbors: users who rated this item\n",
    "        sims: List[Tuple[int, float]] = []\n",
    "\n",
    "        for v in range(R.shape[0]):\n",
    "            if v == user:\n",
    "                continue\n",
    "\n",
    "            # neighbor must have rated the target item\n",
    "            if np.isnan(R[v, item]):\n",
    "                continue\n",
    "\n",
    "            # similarity between user and neighbor v\n",
    "            s = pearson_sim_nan(R[user], R[v])\n",
    "\n",
    "            # keep only non-zero sims (0 means no useful overlap or no variance)\n",
    "            if s != 0.0:\n",
    "                sims.append((v, s))\n",
    "\n",
    "        # If nobody rated the item (or no similarity), fallback to user mean\n",
    "        if not sims:\n",
    "            return user_mean\n",
    "\n",
    "        # B3) Keep top-K neighbors by ABS(similarity)\n",
    "        # (both strong positive and strong negative correlations are \"strong\")\n",
    "        sims.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        sims = sims[: self.k]\n",
    "\n",
    "        # B4) Weighted mean-centered aggregation\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "\n",
    "        for v, s in sims:\n",
    "            v_mean = nanmean_safe(R[v])\n",
    "\n",
    "            # neighbor's \"deviation\" from their mean on this item\n",
    "            # if neighbor rated above their mean, that's a positive signal\n",
    "            num += s * (R[v, item] - v_mean)\n",
    "\n",
    "            # use abs(s) to keep denominator positive and stable\n",
    "            den += abs(s)\n",
    "\n",
    "        # B5) Final prediction\n",
    "        # If den is 0 , fallback to user mean\n",
    "        return float(user_mean + num / den) if den != 0 else user_mean\n",
    "\n",
    "    def recommend(self, user_id: int, seen: set, k: int, n_items: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Recommend top-k items for user_id by predicting ratings for all unseen items.\n",
    "\n",
    "        Steps:\n",
    "          1) For every item i the user has not seen:\n",
    "             score(i) = predicted_rating(user_id, i)\n",
    "          2) Sort items by score descending\n",
    "          3) Return top-k item IDs\n",
    "\n",
    "        Note:\n",
    "          This is O(n_items * n_users) per user in the worst case (slow for large data),\n",
    "          but fine for a toy dataset / learning exercise.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "\n",
    "        for i in range(n_items):\n",
    "            if i in seen:\n",
    "                continue\n",
    "\n",
    "            pred = self.predict_rating(user_id, i)\n",
    "            scores.append((i, pred))\n",
    "\n",
    "        # Sort by predicted rating high -> low\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return top-k item IDs\n",
    "        return [int(i) for i, _ in scores[:k]]\n"
   ],
   "id": "9160e683f7862348",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:36.540514Z",
     "start_time": "2025-12-30T14:04:35.365475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_train = build_dense_rating_matrix(train_df, n_users, n_items)\n",
    "\n",
    "user_knn = UserKNNCF(k_neighbors=30)\n",
    "user_knn.fit(R_train)\n",
    "\n",
    "# Demo recommendations for one user\n",
    "recs_demo = user_knn.recommend(user_id=u_demo, seen=seen_demo, k=10, n_items=n_items)\n",
    "show_recommendations(u_demo, recs_demo, \"User-based kNN CF recommendations\")\n",
    "\n",
    "# Evaluate (using your leave-one-out pipeline)\n",
    "evaluate_model(\n",
    "    recommend_fn=lambda u, seen, k: user_knn.recommend(u, seen, k, n_items),\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    k=10,\n",
    "    name=\"User-kNN CF\"\n",
    ")"
   ],
   "id": "f86c2aa36e1286ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-based kNN CF recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "25       Item_25_Category_04         4\n",
       "48       Item_48_Category_00         0\n",
       "32       Item_32_Category_03         3\n",
       "53       Item_53_Category_00         0\n",
       "6        Item_06_Category_02         2\n",
       "1        Item_01_Category_04         4\n",
       "56       Item_56_Category_01         1\n",
       "9        Item_09_Category_04         4\n",
       "0        Item_00_Category_03         3\n",
       "54       Item_54_Category_01         1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Item_25_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Item_48_Category_00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Item_32_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Item_53_Category_00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Item_06_Category_02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item_01_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Item_56_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Item_09_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item_00_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Item_54_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-kNN CF @ 10\n",
      "Precision@10: 0.037\n",
      "Recall@10:    0.367\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Item-based kNN CF",
   "id": "25b48a2dc7791015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:36.693621Z",
     "start_time": "2025-12-30T14:04:36.668676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ItemKNNCF:\n",
    "    \"\"\"\n",
    "    Item-based kNN Collaborative Filtering (CF)\n",
    "\n",
    "    Core idea:\n",
    "      Items that are rated similarly by many users are \"similar\".\n",
    "      If a user liked (rated high) items similar to item i, then the user will\n",
    "      likely also like item i.\n",
    "\n",
    "    Prediction strategy for rating(u, i):\n",
    "      1) Look at items j that the user u HAS rated.\n",
    "      2) Compute similarity(sim(i, j)) between the target item i and each item j\n",
    "         using their rating vectors across users (columns of R).\n",
    "      3) Weighted average of the user's ratings on those neighbors:\n",
    "            pred(u,i) = sum_j sim(i,j) * r(u,j) / sum_j |sim(i,j)|\n",
    "\n",
    "    Notes:\n",
    "      - We use cosine similarity on item vectors (common for item-based CF).\n",
    "      - We handle missing ratings (NaN) by computing cosine on overlap only (cosine_sim_nan).\n",
    "      - If we can't compute any neighbors, we fall back to the user's mean rating.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_neighbors: int = 50):\n",
    "        # Max number of similar items to use as neighbors\n",
    "        self.k = k_neighbors\n",
    "\n",
    "        # Rating matrix: shape (n_users, n_items)\n",
    "        # R[u, i] = rating or NaN if missing\n",
    "        self.R: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, R: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Store the training rating matrix.\n",
    "        Item-based kNN is also \"lazy\": it doesn't learn parameters,\n",
    "        it just uses R at prediction time.\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "\n",
    "    def predict_rating(self, user: int, item: int) -> float:\n",
    "        \"\"\"\n",
    "        Predict rating for a specific (user, item).\n",
    "\n",
    "        Steps:\n",
    "          A) If user already rated item in training -> return that rating.\n",
    "          B) Otherwise:\n",
    "             - Take the target item's rating vector across users: R[:, item]\n",
    "             - Find neighbor items j that this user rated (R[user, j] not NaN)\n",
    "             - Compute sim(target_item, item_j)\n",
    "             - Predict using weighted average of the user's ratings on those neighbors\n",
    "        \"\"\"\n",
    "        R = self.R\n",
    "        assert R is not None, \"Call fit() before predict_rating().\"\n",
    "\n",
    "        # Make sure indices are real ints (avoids NumPy indexing errors)\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "\n",
    "        # A) If already rated, no need to predict\n",
    "        if not np.isnan(R[user, item]):\n",
    "            return float(R[user, item])\n",
    "\n",
    "        # B0) Baseline fallback: user's average rating (ignoring NaNs)\n",
    "        user_mean = nanmean_safe(R[user])\n",
    "\n",
    "        # B1) Target item vector = ratings of this item by all users\n",
    "        # shape: (n_users,)\n",
    "        target_item_vec = R[:, item]\n",
    "\n",
    "        # Collect (neighbor_item_id, similarity) pairs\n",
    "        sims: List[Tuple[int, float]] = []\n",
    "\n",
    "        # Loop over all possible neighbor items j\n",
    "        for j in range(R.shape[1]):\n",
    "            if j == item:\n",
    "                continue\n",
    "\n",
    "            # Only consider neighbor items that the user has rated\n",
    "            # (otherwise user has no signal about item j)\n",
    "            if np.isnan(R[user, j]):\n",
    "                continue\n",
    "\n",
    "            # Similarity between items i and j using their rating patterns across users\n",
    "            # Uses only overlapping users who rated both items (handled inside cosine_sim_nan)\n",
    "            s = cosine_sim_nan(target_item_vec, R[:, j])\n",
    "\n",
    "            if s != 0.0:\n",
    "                sims.append((j, s))\n",
    "\n",
    "        # If no similar neighbor items found, fallback\n",
    "        if not sims:\n",
    "            return user_mean\n",
    "\n",
    "        # Keep top-K neighbors by strength of similarity\n",
    "        sims.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        sims = sims[: self.k]\n",
    "\n",
    "        # B2) Weighted average of user's ratings on neighbor items\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "\n",
    "        for j, s in sims:\n",
    "            # user’s rating on neighbor item j influences prediction,\n",
    "            # scaled by similarity between j and the target item\n",
    "            num += s * R[user, j]\n",
    "            den += abs(s)\n",
    "\n",
    "        # B3) Final prediction\n",
    "        # If den=0 for some edge case, fallback to user mean\n",
    "        return float(num / den) if den != 0 else user_mean\n",
    "\n",
    "    def recommend(self, user_id: int, seen: set, k: int, n_items: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Recommend top-k items for user_id.\n",
    "\n",
    "        Strategy:\n",
    "          - For each item i not in seen:\n",
    "              score(i) = predict_rating(user_id, i)\n",
    "          - Sort by score descending\n",
    "          - Return top-k item IDs\n",
    "        \"\"\"\n",
    "        user_id = int(user_id)\n",
    "        seen = set(int(x) for x in seen)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for i in range(int(n_items)):\n",
    "            if i in seen:\n",
    "                continue\n",
    "\n",
    "            pred = self.predict_rating(user_id, i)\n",
    "            scores.append((i, pred))\n",
    "\n",
    "        # Sort high-to-low predicted rating\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [int(i) for i, _ in scores[:k]]"
   ],
   "id": "839ec559628aa687",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:38.420949Z",
     "start_time": "2025-12-30T14:04:36.716446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Train and run the model\n",
    "# -------------------------\n",
    "\n",
    "R_train = build_dense_rating_matrix(train_df, n_users, n_items)\n",
    "\n",
    "item_knn = ItemKNNCF(k_neighbors=50)\n",
    "item_knn.fit(R_train)\n",
    "\n",
    "# Demo: recommend for one user\n",
    "recs_demo = item_knn.recommend(user_id=u_demo, seen=seen_demo, k=10, n_items=n_items)\n",
    "show_recommendations(u_demo, recs_demo, \"Item-based kNN CF recommendations\")\n",
    "\n",
    "# Evaluate with your leave-one-out pipeline\n",
    "evaluate_model(\n",
    "    recommend_fn=lambda u, seen, k: item_knn.recommend(u, seen, k, n_items),\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    k=10,\n",
    "    name=\"Item-kNN CF\"\n",
    ")\n"
   ],
   "id": "97ca094cde9334f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item-based kNN CF recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "15       Item_15_Category_03         3\n",
       "3        Item_03_Category_04         4\n",
       "14       Item_14_Category_01         1\n",
       "23       Item_23_Category_01         1\n",
       "31       Item_31_Category_01         1\n",
       "45       Item_45_Category_00         0\n",
       "46       Item_46_Category_01         1\n",
       "52       Item_52_Category_01         1\n",
       "55       Item_55_Category_04         4\n",
       "42       Item_42_Category_04         4"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Item_15_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item_03_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Item_14_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Item_23_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Item_31_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Item_45_Category_00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Item_46_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Item_52_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Item_55_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Item_42_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item-kNN CF @ 10\n",
      "Precision@10: 0.033\n",
      "Recall@10:    0.333\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collaborative Filtering in TensorFlow(Matrix Factorization)",
   "id": "5e090d1af63e9d5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:45.972744Z",
     "start_time": "2025-12-30T14:04:38.528458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Goal:\n",
    "#   We want to learn to predict ratings:\n",
    "#       rating(user, item) ≈ model(user, item)\n",
    "#\n",
    "#   Then for recommendations:\n",
    "#       \"score every item for this user\" -> pick top-K unseen items\n",
    "#\n",
    "# MF idea (very common in recommender systems):\n",
    "#   - Give every user a learnable vector (embedding)\n",
    "#   - Give every item a learnable vector (embedding)\n",
    "#   - If user vector and item vector match well (dot product is big),\n",
    "#     user is predicted to like the item.\n",
    "#\n",
    "# Also add \"biases\":\n",
    "#   - Some users rate high overall (user bias)\n",
    "#   - Some items are liked by everyone (item bias)\n",
    "#   - Plus a global average rating (mu)\n",
    "#\n",
    "# Prediction formula:\n",
    "#   r_hat(u, i) = mu + b_user[u] + b_item[i] + dot(user_vec[u], item_vec[i])\n",
    "# ============================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PART 1) Convert pandas DataFrame -> tf.data.Dataset\n",
    "# ------------------------------------------------------------\n",
    "def make_tf_rating_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int = 256,\n",
    "    shuffle: bool = True\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    TensorFlow training usually expects batches of tensors.\n",
    "\n",
    "    Your ratings are in a DataFrame like:\n",
    "        user_id | item_id | rating\n",
    "\n",
    "    This function converts it into a TensorFlow dataset that yields batches:\n",
    "        inputs  = (user_ids_batch, item_ids_batch)\n",
    "        labels  = ratings_batch\n",
    "\n",
    "    Why do we do this?\n",
    "    - TensorFlow trains fastest on batches (not one row at a time)\n",
    "    - tf.data lets TF stream data efficiently (shuffle, batch, prefetch)\n",
    "\n",
    "    Important:\n",
    "    - user_id and item_id must be integers for Embedding layers\n",
    "    - rating must be float (we train a regression model)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame columns to numpy arrays with correct dtypes:\n",
    "    # - Embedding layers need integer indices (int32 is standard)\n",
    "    user_ids = df[\"user_id\"].to_numpy().astype(np.int32)   # shape [N]\n",
    "    item_ids = df[\"item_id\"].to_numpy().astype(np.int32)   # shape [N]\n",
    "\n",
    "    # Ratings are continuous values -> float32\n",
    "    ratings  = df[\"rating\"].to_numpy().astype(np.float32)  # shape [N]\n",
    "\n",
    "    # Create dataset of individual examples:\n",
    "    # Each example looks like: ((user_id, item_id), rating)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((user_ids, item_ids), ratings))\n",
    "\n",
    "    # Shuffle training data so the model doesn’t see users/items in the same order every epoch\n",
    "    # (this helps training generalize and not learn ordering patterns)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df), seed=42)\n",
    "\n",
    "    # Batch = group many examples together:\n",
    "    # Instead of one (user,item)->rating, you get a batch of 256 examples at a time.\n",
    "    # Prefetch overlaps data preparation and model training (speed boost).\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PART 2) Define the Matrix Factorization model\n",
    "# ------------------------------------------------------------\n",
    "class TFMatrixFactorization(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A TensorFlow model that predicts ratings from (user_id, item_id).\n",
    "\n",
    "    Key TensorFlow concept:\n",
    "    - Embedding layer is basically a \"lookup table\":\n",
    "        Embedding(n_users, k) means:\n",
    "            you store a matrix of shape [n_users, k]\n",
    "            user_id=7 returns row 7 (a vector of length k)\n",
    "\n",
    "    So we learn:\n",
    "    - user_emb[user_id] -> user vector (preferences)\n",
    "    - item_emb[item_id] -> item vector (properties)\n",
    "    - user_bias[user_id] -> scalar\n",
    "    - item_bias[item_id] -> scalar\n",
    "    - mu -> scalar (global mean)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users: int, n_items: int, k: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        # This creates a learnable table:\n",
    "        # user_emb_matrix: [n_users, k]\n",
    "        # When you pass user_ids [B], you get vectors [B, k]\n",
    "        self.user_emb = tf.keras.layers.Embedding(input_dim=n_users, output_dim=k)\n",
    "\n",
    "        # Same for items:\n",
    "        # item_emb_matrix: [n_items, k]\n",
    "        self.item_emb = tf.keras.layers.Embedding(input_dim=n_items, output_dim=k)\n",
    "\n",
    "        # Bias tables (1 number per user/item):\n",
    "        self.user_bias = tf.keras.layers.Embedding(input_dim=n_users, output_dim=1)\n",
    "        self.item_bias = tf.keras.layers.Embedding(input_dim=n_items, output_dim=1)\n",
    "\n",
    "        # Global mean rating (one trainable scalar)\n",
    "        self.mu = tf.Variable(0.0, dtype=tf.float32, trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        The forward pass: given (user_ids, item_ids), output predicted ratings.\n",
    "\n",
    "        inputs:\n",
    "            (user_ids, item_ids)\n",
    "            user_ids: shape [B]\n",
    "            item_ids: shape [B]\n",
    "\n",
    "        output:\n",
    "            predicted ratings: shape [B]\n",
    "        \"\"\"\n",
    "        user_ids, item_ids = inputs\n",
    "\n",
    "        # 1) Look up the user vectors and item vectors\n",
    "        # u_vec: [B, k], i_vec: [B, k]\n",
    "        u_vec = self.user_emb(user_ids)\n",
    "        i_vec = self.item_emb(item_ids)\n",
    "\n",
    "        # 2) Dot product between user and item vectors\n",
    "        # elementwise multiply u_vec * i_vec -> [B, k]\n",
    "        # sum over k -> [B, 1]\n",
    "        dot = tf.reduce_sum(u_vec * i_vec, axis=1, keepdims=True)\n",
    "\n",
    "        # 3) Look up user and item biases -> [B, 1]\n",
    "        b_u = self.user_bias(user_ids)\n",
    "        b_i = self.item_bias(item_ids)\n",
    "\n",
    "        # 4) Combine everything into the final prediction\n",
    "        # pred: [B, 1]\n",
    "        pred = self.mu + b_u + b_i + dot\n",
    "\n",
    "        # Convert shape [B, 1] -> [B]\n",
    "        return tf.squeeze(pred, axis=1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PART 3) Build train/test datasets\n",
    "# ------------------------------------------------------------\n",
    "train_ds = make_tf_rating_dataset(train_df, batch_size=256, shuffle=True)\n",
    "test_ds  = make_tf_rating_dataset(test_df,  batch_size=256, shuffle=False)\n",
    "\n",
    "# Create the MF model\n",
    "tf_mf = TFMatrixFactorization(n_users=n_users, n_items=n_items, k=32)\n",
    "\n",
    "# Initialize global mean mu to average training rating\n",
    "# (This helps training start from a reasonable baseline)\n",
    "tf_mf.mu.assign(float(train_df[\"rating\"].mean()))\n",
    "\n",
    "# Optimizer: controls how model parameters are updated\n",
    "# Adam is a popular default optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Regularization: discourage huge embedding values (helps avoid overfitting)\n",
    "l2_reg = 1e-5\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PART 4) One training step (one batch update)\n",
    "# ------------------------------------------------------------\n",
    "@tf.function\n",
    "def train_step(batch_inputs, batch_ratings):\n",
    "    \"\"\"\n",
    "    This function does ONE learning update on ONE batch.\n",
    "\n",
    "    TensorFlow concepts:\n",
    "    - GradientTape tracks operations to compute gradients (derivatives).\n",
    "    - gradients tell us how to change parameters to reduce loss.\n",
    "    - optimizer.apply_gradients updates the weights.\n",
    "\n",
    "    Inputs:\n",
    "      batch_inputs  = (user_ids_batch, item_ids_batch), shape [B]\n",
    "      batch_ratings = true ratings, shape [B]\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: model predicts ratings for this batch\n",
    "        preds = tf_mf(batch_inputs)  # shape [B]\n",
    "\n",
    "        # Compute error (loss):\n",
    "        # MSE = mean((true - predicted)^2)\n",
    "        mse = tf.reduce_mean(tf.square(batch_ratings - preds))\n",
    "\n",
    "        # L2 penalty: sum of squared weights for all trainable variables\n",
    "        # This includes embeddings + biases + mu\n",
    "        l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf_mf.trainable_variables])\n",
    "\n",
    "        # Total loss: fit the data + keep weights small\n",
    "        loss = mse + l2_reg * l2\n",
    "\n",
    "    # Compute gradients of loss with respect to all model parameters\n",
    "    grads = tape.gradient(loss, tf_mf.trainable_variables)\n",
    "\n",
    "    # Apply gradients (update weights)\n",
    "    optimizer.apply_gradients(zip(grads, tf_mf.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PART 5) Training loop over epochs\n",
    "# ------------------------------------------------------------\n",
    "#\n",
    "# Epoch = one pass over the full training dataset.\n",
    "# Each epoch contains many batches.\n",
    "#\n",
    "for epoch in range(10):\n",
    "    batch_losses = []\n",
    "\n",
    "    # train_ds yields batches:\n",
    "    #   batch_inputs  = (user_ids_batch, item_ids_batch)\n",
    "    #   batch_ratings = ratings_batch\n",
    "    for (batch_inputs, batch_ratings) in train_ds:\n",
    "        loss_val = train_step(batch_inputs, batch_ratings)\n",
    "        batch_losses.append(float(loss_val.numpy()))\n",
    "\n",
    "    # Print average loss across all batches in this epoch\n",
    "    print(f\"TF-MF Epoch {epoch+1} | loss={np.mean(batch_losses):.4f}\")\n"
   ],
   "id": "63794803a60a2519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-MF Epoch 1 | loss=0.8039\n",
      "TF-MF Epoch 2 | loss=0.8318\n",
      "TF-MF Epoch 3 | loss=0.8050\n",
      "TF-MF Epoch 4 | loss=0.6996\n",
      "TF-MF Epoch 5 | loss=0.6760\n",
      "TF-MF Epoch 6 | loss=0.6340\n",
      "TF-MF Epoch 7 | loss=0.5480\n",
      "TF-MF Epoch 8 | loss=0.4863\n",
      "TF-MF Epoch 9 | loss=0.4314\n",
      "TF-MF Epoch 10 | loss=0.3583\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:04:47.433896Z",
     "start_time": "2025-12-30T14:04:46.015835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tf_mf_recommend(user_id: int, seen: set, k: int, n_items: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate top-k recommendations for one user using the trained TF MF model.\n",
    "\n",
    "    How MF recommend works:\n",
    "    - The MF model can predict a rating for ANY (user, item) pair:\n",
    "        tf_mf((user_ids, item_ids)) -> predicted_ratings\n",
    "    - To recommend, we score ALL items for the user and take the highest predictions\n",
    "      among the items the user has not already seen.\n",
    "\n",
    "    This is a brute-force recommender:\n",
    "    - It predicts scores for all n_items in one shot.\n",
    "    - Totally fine for a toy dataset (60 items).\n",
    "    - For real systems (millions of items), you'd use ANN / candidate generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure types are correct for TensorFlow embedding lookups\n",
    "    user_id = int(user_id)\n",
    "    seen = set(int(x) for x in seen)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # (1) Build candidate item IDs: [0, 1, 2, ..., n_items-1]\n",
    "    # ---------------------------------------------------------\n",
    "    item_ids = np.arange(n_items, dtype=np.int32)  # shape [n_items]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # (2) Build a user_id array of the same length\n",
    "    # ---------------------------------------------------------\n",
    "    # We want to predict scores for:\n",
    "    #   (user_id, item_0), (user_id, item_1), ..., (user_id, item_{n_items-1})\n",
    "    #\n",
    "    # So we create:\n",
    "    #   user_ids = [user_id, user_id, ..., user_id]  (length n_items)\n",
    "    user_ids = np.full(shape=(n_items,), fill_value=user_id, dtype=np.int32)  # shape [n_items]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # (3) Predict ratings for ALL items in one model call\n",
    "    # ---------------------------------------------------------\n",
    "    # tf_mf expects a tuple: (user_ids, item_ids)\n",
    "    # It returns predicted ratings (floats) for each pair\n",
    "    #\n",
    "    # preds shape = [n_items]\n",
    "    preds = tf_mf((user_ids, item_ids)).numpy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # (4) Remove items the user already saw\n",
    "    # ---------------------------------------------------------\n",
    "    # We do that by assigning a very low score so they never appear in top-k.\n",
    "    for it in seen:\n",
    "        # it is an item_id, so it is also the index in preds (because item_ids = 0..n_items-1)\n",
    "        preds[it] = -1e9\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # (5) Pick top-k items by predicted rating\n",
    "    # ---------------------------------------------------------\n",
    "    # np.argsort sorts ascending, so use -preds for descending.\n",
    "    top_items = np.argsort(-preds)[:k]\n",
    "\n",
    "    # Return item IDs as Python ints\n",
    "    return [int(i) for i in top_items]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Demo: recommendations for one user\n",
    "# -------------------------\n",
    "recs_demo = tf_mf_recommend(user_id=u_demo, seen=seen_demo, k=10, n_items=n_items)\n",
    "show_recommendations(u_demo, recs_demo, \"TensorFlow MF (Collaborative Filtering) recommendations\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate using your leave-one-out pipeline\n",
    "# -------------------------\n",
    "evaluate_model(\n",
    "    recommend_fn=lambda u, seen, k: tf_mf_recommend(u, seen, k, n_items),\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    k=10,\n",
    "    name=\"TF Matrix Factorization CF\"\n",
    ")\n"
   ],
   "id": "a0375e479ced03b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorFlow MF (Collaborative Filtering) recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "46       Item_46_Category_01         1\n",
       "51       Item_51_Category_01         1\n",
       "56       Item_56_Category_01         1\n",
       "23       Item_23_Category_01         1\n",
       "57       Item_57_Category_03         3\n",
       "25       Item_25_Category_04         4\n",
       "5        Item_05_Category_01         1\n",
       "31       Item_31_Category_01         1\n",
       "4        Item_04_Category_04         4\n",
       "1        Item_01_Category_04         4"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Item_46_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Item_51_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Item_56_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Item_23_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Item_57_Category_03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Item_25_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Item_05_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Item_31_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item_04_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item_01_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF Matrix Factorization CF @ 10\n",
      "Precision@10: 0.047\n",
      "Recall@10:    0.467\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Two Tower Retrieval",
   "id": "7b8ab3284d8ffe2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:05:05.160794Z",
     "start_time": "2025-12-30T14:04:47.664894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Two-Tower is a RETRIEVAL model (not a rating predictor).\n",
    "# - It learns embeddings so that \"positive\" user-item pairs have high similarity.\n",
    "# - It is typically trained on IMPLICIT feedback (click/view/buy), not explicit ratings.\n",
    "#\n",
    "# Architecture:\n",
    "#   user tower: user_id -> user embedding vector u\n",
    "#   item tower: item_id -> item embedding vector v\n",
    "#   score(u,i) = dot(u, v)\n",
    "#\n",
    "# Training objective:\n",
    "# - Binary classification with negative sampling:\n",
    "#     label=1 for observed (user,item)\n",
    "#     label=0 for randomly sampled (user,non-interacted-item)\n",
    "# - Use sigmoid cross entropy on logits = dot(u,v)\n",
    "#\n",
    "# Why L2 normalize?\n",
    "# - After normalization, dot(u,v) becomes cosine similarity.\n",
    "# - Keeps scores bounded and stabilizes training.\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TwoTower(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Two-Tower retrieval model.\n",
    "\n",
    "    This is used in production recommender systems for candidate generation:\n",
    "    - retrieve top-N items for a user from a huge catalog quickly\n",
    "\n",
    "    Conceptually:\n",
    "      - We learn a user embedding space and an item embedding space.\n",
    "      - We train them so that true interactions (positives) are close.\n",
    "      - Non-interactions (negatives) are far.\n",
    "\n",
    "    Implementation here:\n",
    "      - Both towers are simple Embedding layers (no extra features).\n",
    "      - In real-world setups, towers often include:\n",
    "          * user/item features (age, country, category, text embeddings, etc.)\n",
    "          * deep MLP layers\n",
    "          * context features (time, device, query, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users: int, n_items: int, dim: int = 32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup tables:\n",
    "        # user_emb: [n_users, dim], item_emb: [n_items, dim]\n",
    "        self.user_emb = tf.keras.layers.Embedding(n_users, dim)\n",
    "        self.item_emb = tf.keras.layers.Embedding(n_items, dim)\n",
    "\n",
    "    def call(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        Forward pass returns a similarity score (logit) for each (user,item) pair.\n",
    "\n",
    "        Inputs:\n",
    "          user_ids: shape [B]  (batch of users)\n",
    "          item_ids: shape [B]  (batch of items aligned with users)\n",
    "\n",
    "        Output:\n",
    "          logits/scores: shape [B]\n",
    "          (These are unbounded real numbers; we treat them as logits for sigmoid.)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Lookup embeddings:\n",
    "        # u: [B, dim], v: [B, dim]\n",
    "        u = self.user_emb(user_ids)\n",
    "        v = self.item_emb(item_ids)\n",
    "\n",
    "        # 2) Normalize:\n",
    "        # After normalization:\n",
    "        #   dot(u,v) == cosine_similarity(u,v)\n",
    "        # This avoids embedding norms becoming a \"cheat\" to inflate dot products.\n",
    "        u = tf.nn.l2_normalize(u, axis=-1)\n",
    "        v = tf.nn.l2_normalize(v, axis=-1)\n",
    "\n",
    "        # 3) Dot product (per row): sum(u * v) over dim -> [B]\n",
    "        # For each pair in the batch, we get one similarity score.\n",
    "        return tf.reduce_sum(u * v, axis=-1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: Convert explicit ratings -> implicit positives\n",
    "# ------------------------------------------------------------\n",
    "def build_implicit_training_data(train_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert explicit ratings into implicit positive interactions.\n",
    "\n",
    "    In implicit retrieval, we don't train on \"rating value\".\n",
    "    We only train on:\n",
    "      - positive = user interacted with item (exists in train)\n",
    "      - negative = user did not interact with item (sampled)\n",
    "\n",
    "    Here we treat ANY (user,item) in train_df as a positive interaction.\n",
    "\n",
    "    Output:\n",
    "      pos_pairs: ndarray shape [N_pos, 2]\n",
    "        pos_pairs[:,0] = user_id\n",
    "        pos_pairs[:,1] = item_id\n",
    "    \"\"\"\n",
    "    pos = train_df[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    return pos.to_numpy().astype(np.int32)\n",
    "\n",
    "\n",
    "pos_pairs = build_implicit_training_data(train_df)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: Build dataset with negative sampling\n",
    "# ------------------------------------------------------------\n",
    "def build_two_tower_dataset(\n",
    "    pos_pairs: np.ndarray,\n",
    "    n_items: int,\n",
    "    neg_per_pos: int = 3,\n",
    "    batch_size: int = 256,\n",
    "    seed: int = 42\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Build tf.data dataset for two-tower training using negative sampling.\n",
    "\n",
    "    For each positive pair (u, pos_item):\n",
    "      - yield (u, pos_item, label=1)\n",
    "      - sample neg_per_pos random items as negatives:\n",
    "          yield (u, neg_item, label=0)\n",
    "\n",
    "    Why negative sampling?\n",
    "      In implicit data we usually only observe positives (clicks/views).\n",
    "      We must invent negatives by sampling items the user did not interact with.\n",
    "\n",
    "    Important note:\n",
    "      Random negatives are \"easy negatives\".\n",
    "      In production, you'd often use \"hard negatives\":\n",
    "        items from same category / popular items / items retrieved by a baseline model,\n",
    "      to improve ranking quality.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    users = pos_pairs[:, 0]\n",
    "    pos_items = pos_pairs[:, 1]\n",
    "\n",
    "    def gen():\n",
    "        for u, pi in zip(users, pos_items):\n",
    "            # Positive interaction example\n",
    "            yield int(u), int(pi), 1.0\n",
    "\n",
    "            # Negative interactions: same user, random items not equal to the positive item\n",
    "            for _ in range(neg_per_pos):\n",
    "                ni = int(rng.integers(0, n_items))\n",
    "                while ni == int(pi):\n",
    "                    ni = int(rng.integers(0, n_items))\n",
    "                yield int(u), ni, 0.0\n",
    "\n",
    "    # Build TF dataset from Python generator\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),   # user_id\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),   # item_id\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float32), # label\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Shuffle + batch + prefetch\n",
    "    return ds.shuffle(5000, seed=seed).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "two_tower_ds = build_two_tower_dataset(pos_pairs, n_items=n_items, neg_per_pos=3)\n",
    "\n",
    "# Create model + optimizer\n",
    "tt = TwoTower(n_users=n_users, n_items=n_items, dim=32)\n",
    "tt_opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training step: binary classification on (u,i)\n",
    "# ------------------------------------------------------------\n",
    "@tf.function\n",
    "def two_tower_train_step(user_ids, item_ids, labels):\n",
    "    \"\"\"\n",
    "    One optimization step.\n",
    "\n",
    "    We treat the dot-product scores as logits for sigmoid:\n",
    "      p(positive | u,i) = sigmoid(score(u,i))\n",
    "\n",
    "    Loss:\n",
    "      sigmoid_cross_entropy(labels, logits)\n",
    "\n",
    "    If label=1:\n",
    "      we want logits to be large positive (sigmoid close to 1)\n",
    "    If label=0:\n",
    "      we want logits to be large negative (sigmoid close to 0)\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = tt(user_ids, item_ids)  # shape [B]\n",
    "\n",
    "        # Per-example loss (vector [B]) -> mean scalar\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "    grads = tape.gradient(loss, tt.trainable_variables)\n",
    "    tt_opt.apply_gradients(zip(grads, tt.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Train a few epochs (toy example)\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for user_ids, item_ids, labels in two_tower_ds:\n",
    "        loss_val = two_tower_train_step(user_ids, item_ids, labels)\n",
    "        losses.append(float(loss_val.numpy()))\n",
    "    print(f\"Two-Tower Epoch {epoch+1} | loss={np.mean(losses):.4f}\")"
   ],
   "id": "37f15a9985bac9ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Tower Epoch 1 | loss=0.6911\n",
      "Two-Tower Epoch 2 | loss=0.6430\n",
      "Two-Tower Epoch 3 | loss=0.6131\n",
      "Two-Tower Epoch 4 | loss=0.5805\n",
      "Two-Tower Epoch 5 | loss=0.5675\n",
      "Two-Tower Epoch 6 | loss=0.5558\n",
      "Two-Tower Epoch 7 | loss=0.5404\n",
      "Two-Tower Epoch 8 | loss=0.5620\n",
      "Two-Tower Epoch 9 | loss=0.5523\n",
      "Two-Tower Epoch 10 | loss=0.5364\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T14:05:05.418810Z",
     "start_time": "2025-12-30T14:05:05.188441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Recommendation with Two-Tower:\n",
    "# - For a given user u:\n",
    "#     score(u,i) = dot(u_vec, i_vec) for all items i\n",
    "# - Take top-K highest scores (excluding seen)\n",
    "#\n",
    "# This is brute force scoring (O(n_items)).\n",
    "# Real production retrieval:\n",
    "# - precompute item embeddings and index them with ANN (FAISS/ScaNN)\n",
    "# - query the index with user embedding to retrieve top-N fast\n",
    "# =========================\n",
    "\n",
    "def two_tower_recommend(user_id: int, seen: set, k: int, n_items: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Recommend top-k items for a user using the trained two-tower model.\n",
    "\n",
    "    Steps:\n",
    "      1) compute user embedding u_vec\n",
    "      2) compute all item embeddings i_vec (for all items)\n",
    "      3) compute scores = dot(u_vec, i_vec) for each item\n",
    "      4) remove already seen items\n",
    "      5) return top-k\n",
    "\n",
    "    Note:\n",
    "      This assumes item IDs are 0..n_items-1 so we can index scores by item_id.\n",
    "    \"\"\"\n",
    "    user_id = int(user_id)\n",
    "    seen = set(int(x) for x in seen)\n",
    "\n",
    "    # 1) User embedding: [1, dim]\n",
    "    u = tf.constant([user_id], dtype=tf.int32)\n",
    "    u_vec = tt.user_emb(u)\n",
    "    u_vec = tf.nn.l2_normalize(u_vec, axis=-1)\n",
    "\n",
    "    # 2) All item embeddings: [n_items, dim]\n",
    "    item_ids = tf.range(n_items, dtype=tf.int32)\n",
    "    i_vec = tt.item_emb(item_ids)\n",
    "    i_vec = tf.nn.l2_normalize(i_vec, axis=-1)\n",
    "\n",
    "    # 3) Compute scores for all items:\n",
    "    #    scores[i] = dot(u_vec, i_vec[i])\n",
    "    # u_vec: [1, dim]\n",
    "    # i_vec^T: [dim, n_items]\n",
    "    # result: [1, n_items] -> squeeze -> [n_items]\n",
    "    scores = tf.squeeze(tf.matmul(u_vec, i_vec, transpose_b=True)).numpy()\n",
    "\n",
    "    # 4) Exclude seen items by forcing score very low\n",
    "    for it in seen:\n",
    "        scores[it] = -1e9\n",
    "\n",
    "    # 5) Top-K by score descending\n",
    "    top = np.argsort(-scores)[:k]\n",
    "    return [int(i) for i in top]\n",
    "\n",
    "\n",
    "# Demo\n",
    "recs_demo = two_tower_recommend(u_demo, seen_demo, k=10, n_items=n_items)\n",
    "show_recommendations(u_demo, recs_demo, \"Two-Tower recommendations\")\n",
    "\n",
    "# Evaluate using your pipeline\n",
    "evaluate_model(\n",
    "    lambda u, seen, k: two_tower_recommend(u, seen, k, n_items),\n",
    "    train_df,\n",
    "    test_df,\n",
    "    k=10,\n",
    "    name=\"Two-Tower Retrieval (TF)\"\n",
    ")"
   ],
   "id": "2a88254dd27f99dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Two-Tower recommendations (user=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       title  category\n",
       "item_id                               \n",
       "50       Item_50_Category_05         5\n",
       "36       Item_36_Category_05         5\n",
       "16       Item_16_Category_05         5\n",
       "56       Item_56_Category_01         1\n",
       "54       Item_54_Category_01         1\n",
       "17       Item_17_Category_05         5\n",
       "24       Item_24_Category_05         5\n",
       "5        Item_05_Category_01         1\n",
       "25       Item_25_Category_04         4\n",
       "1        Item_01_Category_04         4"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Item_50_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Item_36_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Item_16_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Item_56_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Item_54_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Item_17_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Item_24_Category_05</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Item_05_Category_01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Item_25_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item_01_Category_04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Two-Tower Retrieval (TF) @ 10\n",
      "Precision@10: 0.050\n",
      "Recall@10:    0.500\n"
     ]
    }
   ],
   "execution_count": 114
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
